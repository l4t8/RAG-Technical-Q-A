question_number,pipeline,correct,predicted,ground_truth,quote,source,latency,run_timestamp
1,Baseline (Zero-shot),False,ERROR,C,No quote provided,,17.38694143295288,2025-12-02 16:08:38
2,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.267144680023192,2025-12-02 16:08:38
3,Baseline (Zero-shot),False,ERROR,B,No quote provided,,10.708547592163086,2025-12-02 16:08:38
4,Baseline (Zero-shot),True,D,D,No quote provided,,4.6562416553497314,2025-12-02 16:08:38
5,Baseline (Zero-shot),False,ERROR,C,No quote provided,,7.404452085494995,2025-12-02 16:08:38
6,Baseline (Zero-shot),False,D,C,No quote provided,,12.390499353408812,2025-12-02 16:08:38
7,Baseline (Zero-shot),True,C,C,No quote provided,,4.914943218231201,2025-12-02 16:08:38
8,Baseline (Zero-shot),True,B,B,No quote provided,,7.065745830535889,2025-12-02 16:08:38
9,Baseline (Zero-shot),True,C,C,No quote provided,,7.067361354827881,2025-12-02 16:08:38
10,Baseline (Zero-shot),True,C,C,No quote provided,,10.407362461090088,2025-12-02 16:08:38
11,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.797832727432253,2025-12-02 16:08:38
12,Baseline (Zero-shot),False,ERROR,C,No quote provided,,22.10896778106689,2025-12-02 16:08:38
13,Baseline (Zero-shot),False,ERROR,C,No quote provided,,259.28093576431274,2025-12-02 16:08:38
14,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.44430923461914,2025-12-02 16:08:38
15,Baseline (Zero-shot),False,ERROR,C,No quote provided,,15.05284070968628,2025-12-02 16:08:38
16,Baseline (Zero-shot),True,C,C,No quote provided,,5.779683589935303,2025-12-02 16:08:38
17,Baseline (Zero-shot),False,ERROR,C,No quote provided,,15.418951034545898,2025-12-02 16:08:38
18,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.828935146331789,2025-12-02 16:08:38
19,Baseline (Zero-shot),False,A,C,No quote provided,,6.65905237197876,2025-12-02 16:08:38
20,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.31696915626526,2025-12-02 16:08:38
21,Baseline (Zero-shot),True,C,C,No quote provided,,7.8487372398376465,2025-12-02 16:08:38
22,Baseline (Zero-shot),False,ERROR,C,No quote provided,,66.49571394920349,2025-12-02 16:08:38
23,Baseline (Zero-shot),True,C,C,No quote provided,,7.677035570144653,2025-12-02 16:08:38
24,Baseline (Zero-shot),False,ERROR,C,No quote provided,,8.607865571975708,2025-12-02 16:08:38
25,Baseline (Zero-shot),False,A,C,No quote provided,,9.727467060089111,2025-12-02 16:08:38
26,Baseline (Zero-shot),False,B,C,No quote provided,,3.90659761428833,2025-12-02 16:08:38
27,Baseline (Zero-shot),False,ERROR,C,No quote provided,,28.139307737350464,2025-12-02 16:08:38
28,Baseline (Zero-shot),False,ERROR,C,No quote provided,,50.38258099555969,2025-12-02 16:08:38
29,Baseline (Zero-shot),True,C,C,No quote provided,,3.9941868782043457,2025-12-02 16:08:38
30,Baseline (Zero-shot),False,A,C,No quote provided,,5.528787136077881,2025-12-02 16:08:38
31,Baseline (Zero-shot),False,ERROR,C,No quote provided,,14.131281614303589,2025-12-02 16:08:38
32,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.751900911331177,2025-12-02 16:08:38
33,Baseline (Zero-shot),False,A,C,No quote provided,,3.684698820114136,2025-12-02 16:08:38
34,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.52555513381958,2025-12-02 16:08:38
35,Baseline (Zero-shot),True,C,C,No quote provided,,5.52888035774231,2025-12-02 16:08:38
36,Baseline (Zero-shot),False,ERROR,C,No quote provided,,12.400662183761597,2025-12-02 16:08:38
37,Baseline (Zero-shot),False,A,C,No quote provided,,4.186343193054199,2025-12-02 16:08:38
38,Baseline (Zero-shot),False,ERROR,C,No quote provided,,21.50435185432434,2025-12-02 16:08:38
39,Baseline (Zero-shot),False,ERROR,C,No quote provided,,254.3637323379517,2025-12-02 16:08:38
40,Baseline (Zero-shot),False,B,C,No quote provided,,14.131260871887209,2025-12-02 16:08:38
41,Baseline (Zero-shot),True,C,C,No quote provided,,6.143826961517334,2025-12-02 16:08:38
42,Baseline (Zero-shot),True,B,B,No quote provided,,4.6075544357299805,2025-12-02 16:08:38
43,Baseline (Zero-shot),True,C,C,No quote provided,,6.007459402084351,2025-12-02 16:08:38
44,Baseline (Zero-shot),False,A,C,No quote provided,,8.00029706954956,2025-12-02 16:08:38
45,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.875770807266235,2025-12-02 16:08:38
46,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.830308198928831,2025-12-02 16:08:38
47,Baseline (Zero-shot),False,ERROR,B,No quote provided,,12.902543067932127,2025-12-02 16:08:38
48,Baseline (Zero-shot),True,B,B,No quote provided,,5.222078561782837,2025-12-02 16:08:38
49,Baseline (Zero-shot),False,ERROR,C,No quote provided,,21.1969997882843,2025-12-02 16:08:38
50,Baseline (Zero-shot),False,A,C,No quote provided,,10.444539308547974,2025-12-02 16:08:38
51,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.139298677444458,2025-12-02 16:08:38
52,Baseline (Zero-shot),True,C,C,No quote provided,,6.019439935684204,2025-12-02 16:08:38
53,Baseline (Zero-shot),True,C,C,No quote provided,,4.840469121932983,2025-12-02 16:08:38
54,Baseline (Zero-shot),True,C,C,No quote provided,,8.897587060928345,2025-12-02 16:08:38
55,Baseline (Zero-shot),False,ERROR,C,No quote provided,,24.787273168563843,2025-12-02 16:08:38
56,Baseline (Zero-shot),True,C,C,No quote provided,,4.406660795211792,2025-12-02 16:08:38
57,Baseline (Zero-shot),False,ERROR,C,No quote provided,,11.904621362686155,2025-12-02 16:08:38
58,Baseline (Zero-shot),False,A,C,No quote provided,,9.437885761260986,2025-12-02 16:08:38
59,Baseline (Zero-shot),False,A,C,No quote provided,,6.584682941436768,2025-12-02 16:08:38
60,Baseline (Zero-shot),True,C,C,No quote provided,,6.793975591659546,2025-12-02 16:08:38
61,Baseline (Zero-shot),True,C,C,No quote provided,,3.8532986640930176,2025-12-02 16:08:38
62,Baseline (Zero-shot),False,ERROR,C,No quote provided,,14.337415933609009,2025-12-02 16:08:38
63,Baseline (Zero-shot),False,ERROR,C,No quote provided,,17.032433032989502,2025-12-02 16:08:38
64,Baseline (Zero-shot),False,ERROR,C,No quote provided,,19.035902976989743,2025-12-02 16:08:38
65,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.445059776306152,2025-12-02 16:08:38
66,Baseline (Zero-shot),False,ERROR,C,No quote provided,,20.275167226791385,2025-12-02 16:08:38
67,Baseline (Zero-shot),True,C,C,No quote provided,,11.9807231426239,2025-12-02 16:08:38
68,Baseline (Zero-shot),True,C,C,No quote provided,,3.993319034576416,2025-12-02 16:08:38
69,Baseline (Zero-shot),True,C,C,No quote provided,,6.216581344604492,2025-12-02 16:08:38
70,Baseline (Zero-shot),False,ERROR,C,No quote provided,,18.36518836021424,2025-12-02 16:08:38
1,BM25 RAG,True,C,C,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.",2509.01092v2.pdf page 10,9.756217002868652,2025-12-02 16:16:26
2,BM25 RAG,False,ERROR,C,No quote provided,,8.661659717559814,2025-12-02 16:16:26
3,BM25 RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, page 9",4.624433994293213,2025-12-02 16:16:26
4,BM25 RAG,True,D,D,"Our model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the sameLLaMA model without extending its effective positional encoding, maintains robust performance","2509.01092v2.pdf, Page: 7",3.060482978820801,2025-12-02 16:16:26
5,BM25 RAG,True,C,C,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.","2509.01092v2.pdf, Page: 1",9.83765697479248,2025-12-02 16:16:26
6,BM25 RAG,True,C,C,"Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",6.149009943008423,2025-12-02 16:16:26
7,BM25 RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",4.28603982925415,2025-12-02 16:16:26
8,BM25 RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",5.529476642608643,2025-12-02 16:16:26
9,BM25 RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",5.529305458068848,2025-12-02 16:16:26
10,BM25 RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",2.9828360080718994,2025-12-02 16:16:26
11,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","2509.01092v2.pdf, Page: 0",4.697164535522461,2025-12-02 16:16:26
12,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",4.303472518920898,2025-12-02 16:16:26
13,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.966557741165161,2025-12-02 16:16:26
14,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, page 3",3.175469398498535,2025-12-02 16:16:26
15,BM25 RAG,True,C,C,"In this task, wefreeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss","2509.01092v2.pdf, Page: 3",7.371518611907959,2025-12-02 16:16:26
16,BM25 RAG,True,C,C,"As the chunk lengthk increases, the number of possible token combinations expands exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge.","2509.01092v2.pdf, Page: 3",18.70886945724488,2025-12-02 16:16:26
17,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",9.256389141082764,2025-12-02 16:16:26
18,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",3.676219940185547,2025-12-02 16:16:26
19,BM25 RAG,True,C,C,"With a weak retriever setting, at 10 passages,REFRAGimproves performance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.0745689868927,2025-12-02 16:16:26
20,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications. With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",3.0689377784729004,2025-12-02 16:16:26
21,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.3791723251342773,2025-12-02 16:16:26
22,BM25 RAG,True,C,C,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",2.121798038482666,2025-12-02 16:16:26
23,BM25 RAG,True,C,C,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",3.996469020843506,2025-12-02 16:16:26
24,BM25 RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the","2509.01092v2.pdf, Page: 3",3.7131290435791016,2025-12-02 16:16:26
25,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",3.3827548027038574,2025-12-02 16:16:26
26,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)...","2509.01092v2.pdf, page 2",4.859056949615479,2025-12-02 16:16:26
27,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",2.817665338516236,2025-12-02 16:16:26
28,BM25 RAG,True,C,C,"For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",2.1484534740448,2025-12-02 16:16:26
29,BM25 RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",3.05408787727356,2025-12-02 16:16:26
30,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",5.24117112159729,2025-12-02 16:16:26
31,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.456237316131592,2025-12-02 16:16:26
32,BM25 RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, page 15",2.7645459175109863,2025-12-02 16:16:26
33,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)","2509.01092v2.pdf, page 2",4.607474327087402,2025-12-02 16:16:26
34,BM25 RAG,False,A,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from s/k chunk embedding directly).","2509.01092v2.pdf, Page: 5",27.341137170791622,2025-12-02 16:16:26
35,BM25 RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over","2509.01092v2.pdf, Page: 3",3.3794198036193848,2025-12-02 16:16:26
36,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",2509.01092v2.pdf page 3,3.993269681930542,2025-12-02 16:16:26
37,BM25 RAG,True,C,C,"With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",1.845695972442627,2025-12-02 16:16:26
38,BM25 RAG,False,A,C,No direct quote is available in the provided context that describes the scale and composition of the continual pre-training corpus.,,20.88751459121704,2025-12-02 16:16:26
39,BM25 RAG,True,C,C,Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.,"2509.01092v2.pdf, Page: 7",3.519681215286255,2025-12-02 16:16:26
40,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",2.930429697036743,2025-12-02 16:16:26
41,BM25 RAG,True,C,C,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9",8.456843614578247,2025-12-02 16:16:26
42,BM25 RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",2509.01092v2.pdf page 3,4.138075590133667,2025-12-02 16:16:26
43,BM25 RAG,False,ERROR,C,No quote provided,,14.48560094833374,2025-12-02 16:16:26
44,BM25 RAG,False,ERROR,C,No quote provided,,22.68779802322388,2025-12-02 16:16:26
45,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,2509.01092v2.pdf: page 5,3.2778215408325195,2025-12-02 16:16:26
46,BM25 RAG,True,C,C,"By exploiting this attention sparsity structure, we demonstrate a30.85× the time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity.","2509.01092v2.pdf, Page: 0",7.197681665420532,2025-12-02 16:16:26
47,BM25 RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).","2509.01092v2.pdf, Page: 0",1.5030436515808103,2025-12-02 16:16:26
48,BM25 RAG,True,B,B,Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,"2509.01092v2.pdf, Page: 21",4.610375642776489,2025-12-02 16:16:26
49,BM25 RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",2509.01092v2.pdf: 7,3.684324979782105,2025-12-02 16:16:26
50,BM25 RAG,True,C,C,"the overall input to the decoder will be reduced by a factor of ≃k. This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence.","2509.01092v2.pdf, page 2",10.753897905349731,2025-12-02 16:16:26
51,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",4.245680570602417,2025-12-02 16:16:26
52,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, page 1",2.8190691471099854,2025-12-02 16:16:26
53,BM25 RAG,True,C,C,"This chunk embedding is then projected with a projection layerϕ to match the size of the token embedding of the decoder model, ecnk i = ϕ(ci).","2509.01092v2.pdf, Page: 1",2.455756902694702,2025-12-02 16:16:26
54,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.","2509.01092v2.pdf, Page: 2",1.842499017715454,2025-12-02 16:16:26
55,BM25 RAG,True,C,C,"In this task, wefreeze the decoder model and only train the encoder and projection layer.","2509.01092v2.pdf, Page: 3",2.457995653152466,2025-12-02 16:16:26
56,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"2509.01092v2.pdf, Page: 5",5.529051303863525,2025-12-02 16:16:26
57,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,2.457498073577881,2025-12-02 16:16:26
58,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,2509.01092v2.pdf page 5,3.379411697387696,2025-12-02 16:16:26
59,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",2.1497962474823,2025-12-02 16:16:26
60,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",2.7675981521606445,2025-12-02 16:16:26
61,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, page 1",4.155661344528198,2025-12-02 16:16:26
62,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both metrics.","2509.01092v2.pdf, page 2",1.984066247940064,2025-12-02 16:16:26
63,BM25 RAG,True,C,C,The effective compression ratek 1−p+kp decreases when,"2509.01092v2.pdf, Page: 4",5.223769187927246,2025-12-02 16:16:26
64,BM25 RAG,False,B,C,"During CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in predicting the nexto tokens xs+1:s+o.","2509.01092v2.pdf, Page: 3",24.575815200805664,2025-12-02 16:16:26
65,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"2509.01092v2.pdf, Page: 5",19.355340003967285,2025-12-02 16:16:26
66,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",2.87630558013916,2025-12-02 16:16:26
67,BM25 RAG,False,ERROR,C,No quote provided,,32.00105905532837,2025-12-02 16:16:26
68,BM25 RAG,False,ERROR,C,No quote provided,,13.351359605789185,2025-12-02 16:16:26
69,BM25 RAG,False,A,C,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages.,"2509.01092v2.pdf, Page: 7",21.81088924407959,2025-12-02 16:16:26
70,BM25 RAG,True,C,C,"For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",2.4603965282440186,2025-12-02 16:16:26
1,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.756526231765747,2025-12-02 16:27:01
2,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",8.60121750831604,2025-12-02 16:27:01
3,Dense RAG,False,D,B,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",35.63692045211792,2025-12-02 16:27:01
4,Dense RAG,True,D,D,"However, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG16 and REFRAG32 both outperform the LLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency). The same result occurs in long context scenarios.","2509.01092v2.pdf, Page: 7",13.3587327003479,2025-12-02 16:27:01
5,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",21.66316390037537,2025-12-02 16:27:01
6,Dense RAG,False,D,C,These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.,"2509.01092v2.pdf, Page: 6",24.00275230407715,2025-12-02 16:27:01
7,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",9.172898530960085,2025-12-02 16:27:01
8,Dense RAG,False,ERROR,B,No quote provided,,10.44354796409607,2025-12-02 16:27:01
9,Dense RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",4.608464479446411,2025-12-02 16:27:01
10,Dense RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",3.3786652088165283,2025-12-02 16:27:01
11,Dense RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","2509.01092v2.pdf, page 0",2.7218663692474365,2025-12-02 16:27:01
12,Dense RAG,False,A,C,"attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements.","filename: 2509.01092v2.pdf, page 9",7.415619611740112,2025-12-02 16:27:01
13,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, page 2",4.709967851638794,2025-12-02 16:27:01
14,Dense RAG,True,C,C,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, Page: 3",9.420964479446411,2025-12-02 16:27:01
15,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","2509.01092v2.pdf, Page: 3",3.072166919708252,2025-12-02 16:27:01
16,Dense RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3",23.961710929870605,2025-12-02 16:27:01
17,Dense RAG,True,C,C,"Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths. ... chunk embeddings, yet still surpasses the performance ofREFRAG8.","2509.01092v2.pdf: 5, 6",9.178810834884644,2025-12-02 16:27:01
18,Dense RAG,False,B,C,"However, CEPE is worse than original LLaMA in TTIT since it require the additional computation of KV cache projection in the inference time.","2509.01092v2.pdf, Page: 15",32.395628452301025,2025-12-02 16:27:01
19,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, page 7",4.197874069213867,2025-12-02 16:27:01
20,Dense RAG,True,C,C,"However, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG16 and REFRAG32 both outperform the LLaMAFT model despite having 2× and 4 × fewer tokens in the decoder (i.e., lower latency).","2509.01092v2.pdf, Page: 7",12.595723628997805,2025-12-02 16:27:01
21,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.993099451065064,2025-12-02 16:27:01
22,Dense RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.4578235149383545,2025-12-02 16:27:01
23,Dense RAG,True,C,C,These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.,"2509.01092v2.pdf, Page: 6",12.590068817138672,2025-12-02 16:27:01
24,Dense RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","2509.01092v2.pdf, Page: 3",5.8433918952941895,2025-12-02 16:27:01
25,Dense RAG,True,C,C,RL-trained chunk expansion policy Reward = - Log(Perplexity),"2509.01092v2.pdf, Page: 17",3.991287708282471,2025-12-02 16:27:01
26,Dense RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).","2509.01092v2.pdf, Page: 2",4.1821606159210205,2025-12-02 16:27:01
27,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, page 7",2.8853988647460938,2025-12-02 16:27:01
28,Dense RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",18.12288784980774,2025-12-02 16:27:01
29,Dense RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",2509.01092v2.pdf: page 6,2.45743727684021,2025-12-02 16:27:01
30,Dense RAG,True,C,C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context. Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at","2509.01092v2.pdf, page 1",6.6575188636779785,2025-12-02 16:27:01
31,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.22231125831604,2025-12-02 16:27:01
32,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.","2509.01092v2.pdf, Page: 2",1.8883001804351809,2025-12-02 16:27:01
33,Dense RAG,True,C,C,"Under the context length of16384(i.e., mid-to-long context),REFRAG achieves16 .53× acceleration in TTFT with cache and8.59× without cache.","2509.01092v2.pdf, Page: 15",3.0549721717834477,2025-12-02 16:27:01
34,Dense RAG,True,C,C,Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported as average of Arxiv and Book domain. LLaMA-Full Context1.397 0.734 0.203 0.021 LLaMA-No Context3.483 2.981 2.249 1.590,"2509.01092v2.pdf, Page: 25",25.06362557411194,2025-12-02 16:27:01
35,Dense RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over","2509.01092v2.pdf, Page: 3",3.505829334259033,2025-12-02 16:27:01
36,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",29.18455982208252,2025-12-02 16:27:01
37,Dense RAG,True,C,C,"With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",1.5356571674346924,2025-12-02 16:27:01
38,Dense RAG,False,NONE OF THE ABOVE,C,The information regarding the scale and composition of the continual pre-training corpus is not present in the provided context.,,3.37953519821167,2025-12-02 16:27:01
39,Dense RAG,True,C,C,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.,2509.01092v2.pdf page 7,4.097335338592529,2025-12-02 16:27:01
40,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.274568796157837,2025-12-02 16:27:01
41,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-tations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",2.683037757873535,2025-12-02 16:27:01
42,Dense RAG,True,B,B,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",18.821153163909912,2025-12-02 16:27:01
43,Dense RAG,True,C,C,REFRAGRL uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4",6.144157886505127,2025-12-02 16:27:01
44,Dense RAG,False,D,C,"Notably,REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4",6.143711090087891,2025-12-02 16:27:01
45,Dense RAG,True,C,C,"This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.","2509.01092v2.pdf, Page: 6",6.452802419662476,2025-12-02 16:27:01
46,Dense RAG,False,D,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",11.98313808441162,2025-12-02 16:27:01
47,Dense RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).","2509.01092v2.pdf, Page: 0",1.5316579341888428,2025-12-02 16:27:01
48,Dense RAG,True,B,B,"Table 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",3.993669033050537,2025-12-02 16:27:01
49,Dense RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","2509.01092v2.pdf, Page: 7",3.0738911628723145,2025-12-02 16:27:01
50,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.455625295639038,2025-12-02 16:27:01
51,Dense RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1",4.011880874633789,2025-12-02 16:27:01
52,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",4.899102449417114,2025-12-02 16:27:01
53,Dense RAG,False,D,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) Itshortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computedchunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computationcomplexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at","2509.01092v2.pdf, Page: 1",34.09686207771301,2025-12-02 16:27:01
54,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.","2509.01092v2.pdf, Page: 2",3.071789264678955,2025-12-02 16:27:01
55,Dense RAG,True,C,C,"In this task, wefreeze the decoder model and only train the encoder and projection layer.","2509.01092v2.pdf, Page: 3",2.1499929428100586,2025-12-02 16:27:01
56,Dense RAG,True,C,C,REFRAGw/o curriculum 3.719 ... REFRAGwith curriculum 0.669,"2509.01092v2.pdf, Page: 25",11.980985164642334,2025-12-02 16:27:01
57,Dense RAG,True,C,C,"We compare the perplexity of xs+1:s+o using different selection policy under differentp. The perplexity-based selection is an heuristic based selection which compresses chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can therefore be compressed with minimal information loss. Ideally, this approach should outperform random selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves superior performance across varying compression ratesp.","2509.01092v2.pdf, Page: 4",22.42567992210388,2025-12-02 16:27:01
58,Dense RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",3.071579933166504,2025-12-02 16:27:01
59,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",2.7647268772125244,2025-12-02 16:27:01
60,Dense RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",2.766885995864868,2025-12-02 16:27:01
61,Dense RAG,True,C,C,"In real applications (e.g., RAG), the context is the dominating part of the input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k .","2509.01092v2.pdf, page 2",6.758489847183228,2025-12-02 16:27:01
62,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",3.0699446201324463,2025-12-02 16:27:01
63,Dense RAG,True,C,C,"The effective compression ratek 1−p+kp decreases when fewer chunks are compressed (i.e.,p increases).","2509.01092v2.pdf, Page: 4",2.150042057037353,2025-12-02 16:27:01
64,Dense RAG,False,ERROR,C,No quote provided,,17.819667100906372,2025-12-02 16:27:01
65,Dense RAG,True,C,C,"REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-tations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",13.51494526863098,2025-12-02 16:27:01
66,Dense RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",2.265734434127808,2025-12-02 16:27:01
67,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",39.820454597473145,2025-12-02 16:27:01
68,Dense RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",3.3797075748443604,2025-12-02 16:27:01
69,Dense RAG,False,D,C,"Models trained with our recipe achieve performance comparable to the Full Context setting (i.e., without context compression). Moreover, increasing the context length continues to benefit our model, as evidenced by lower perplexity for a context length of4096compared to2048.
5 Contextual Learning Applications
In this section, we investigate fine-tuning the model obtained from the pre-training stage to address various
downstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.","2509.01092v2.pdf, page 6",3.993086338043213,2025-12-02 16:27:01
70,Dense RAG,True,C,C,"For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",3.993450164794922,2025-12-02 16:27:01
1,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.3524258136749268,2025-12-02 16:33:44
2,Hybrid RAG,False,A,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task. Reconstruction task is essential for the model to learn the continual pre-training task.","2509.01092v2.pdf, Page: 5",8.928820133209229,2025-12-02 16:33:44
3,Hybrid RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",5.790865182876587,2025-12-02 16:33:44
4,Hybrid RAG,True,D,D,"Our model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the sameLLaMA model without extending its effective positional encoding, maintains robust performance","2509.01092v2.pdf, Page: 7",4.041974782943726,2025-12-02 16:33:44
5,Hybrid RAG,True,C,C,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.","2509.01092v2.pdf, Page: 1",8.905990839004517,2025-12-02 16:33:44
6,Hybrid RAG,True,C,C,"Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",7.066152811050415,2025-12-02 16:33:44
7,Hybrid RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",4.914550065994263,2025-12-02 16:33:44
8,Hybrid RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",6.143902540206909,2025-12-02 16:33:44
9,Hybrid RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",5.498393297195435,2025-12-02 16:33:44
10,Hybrid RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","filename: 2509.01092v2.pdf, page 6",3.102915048599243,2025-12-02 16:33:44
11,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","2509.01092v2.pdf, Page: 0",5.222309827804565,2025-12-02 16:33:44
12,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.9957077503204346,2025-12-02 16:33:44
13,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",4.29840874671936,2025-12-02 16:33:44
14,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, Page: 3",13.211026430130005,2025-12-02 16:33:44
15,Hybrid RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","2509.01092v2.pdf, Page: 3",3.377650737762451,2025-12-02 16:33:44
16,Hybrid RAG,True,C,C,"As the chunk lengthk increases, the number of possible token combinations expands exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens fromLchunk embeddings further compounds the difficulty of the task. ... For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",2509.01092v2.pdf page 3,5.482954025268555,2025-12-02 16:33:44
17,Hybrid RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",7.8091089725494385,2025-12-02 16:33:44
18,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",3.6038503646850586,2025-12-02 16:33:44
19,Hybrid RAG,True,C,C,"With a weak retriever setting, at 10 passages,REFRAGimproves performance by0.71%and accelerates TTFT by5.26× compared to LLaMA. At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.0752570629119873,2025-12-02 16:33:44
20,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications. With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",6.471766471862793,2025-12-02 16:33:44
21,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.9408986568450928,2025-12-02 16:33:44
22,Hybrid RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",3.2949037551879883,2025-12-02 16:33:44
23,Hybrid RAG,True,C,C,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",3.798799991607666,2025-12-02 16:33:44
24,Hybrid RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","2509.01092v2.pdf, Page: 3",5.529301166534424,2025-12-02 16:33:44
25,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",2.457974433898926,2025-12-02 16:33:44
26,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)",2509.01092v2.pdf: page 2,4.607604265213013,2025-12-02 16:33:44
27,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",5.224316358566284,2025-12-02 16:33:44
28,Hybrid RAG,True,C,C,"For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",2.1199092864990234,2025-12-02 16:33:44
29,Hybrid RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",2.7945175170898438,2025-12-02 16:33:44
30,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",5.425254583358765,2025-12-02 16:33:44
31,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.252683162689209,2025-12-02 16:33:44
32,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.4659459590911865,2025-12-02 16:33:44
33,Hybrid RAG,True,C,C,"Under the context length of16384(i.e., mid-to-long context),REFRAG achieves16 .53× acceleration in TTFT with cache and8.59× without cache.","2509.01092v2.pdf, Page: 15",3.3707990646362305,2025-12-02 16:33:44
34,Hybrid RAG,False,A,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task.","2509.01092v2.pdf, Page: 5",18.73906946182251,2025-12-02 16:33:44
35,Hybrid RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",2509.01092v2.pdf page 3,4.302494525909424,2025-12-02 16:33:44
36,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s","2509.01092v2.pdf, Page: 3",8.523854970932007,2025-12-02 16:33:44
37,Hybrid RAG,True,C,C,"With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",1.6116619110107422,2025-12-02 16:33:44
38,Hybrid RAG,False,NONE OF THE ABOVE,C,N/A,N/A,3.6863584518432617,2025-12-02 16:33:44
39,Hybrid RAG,True,C,C,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.,"2509.01092v2.pdf, Page: 7",3.993922233581543,2025-12-02 16:33:44
40,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.378770112991333,2025-12-02 16:33:44
41,Hybrid RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-tations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",3.5742318630218506,2025-12-02 16:33:44
42,Hybrid RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the","2509.01092v2.pdf, Page: 3",3.4913246631622314,2025-12-02 16:33:44
43,Hybrid RAG,False,ERROR,C,No quote provided,None,13.823642015457153,2025-12-02 16:33:44
44,Hybrid RAG,False,ERROR,C,No quote provided,None,15.974501132965088,2025-12-02 16:33:44
45,Hybrid RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",3.071958303451538,2025-12-02 16:33:44
46,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity.","2509.01092v2.pdf, Page: 0",7.67956018447876,2025-12-02 16:33:44
47,Hybrid RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).","2509.01092v2.pdf, Page: 0",3.932652473449707,2025-12-02 16:33:44
48,Hybrid RAG,True,B,B,"Table 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",5.283657550811768,2025-12-02 16:33:44
49,Hybrid RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","2509.01092v2.pdf, Page: 7",3.9930601119995117,2025-12-02 16:33:44
50,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",1.8433525562286377,2025-12-02 16:33:44
51,Hybrid RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1",3.0717153549194336,2025-12-02 16:33:44
52,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.379230499267578,2025-12-02 16:33:44
53,Hybrid RAG,True,C,C,"This chunk embedding is then projected with a projection layerϕ to match the size of the token embedding of the decoder model, ecnk i = ϕ(ci).","2509.01092v2.pdf, Page: 1",2.3961682319641113,2025-12-02 16:33:44
54,Hybrid RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.2110555171966553,2025-12-02 16:33:44
55,Hybrid RAG,True,C,C,"In this task, wefreeze the decoder model and only train the encoder and projection layer.","2509.01092v2.pdf, Page: 3",2.0483782291412354,2025-12-02 16:33:44
56,Hybrid RAG,False,ERROR,C,No quote provided,None,8.703842163085938,2025-12-02 16:33:44
57,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",1.8425087928771973,2025-12-02 16:33:44
58,Hybrid RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, page 5",3.9935224056243896,2025-12-02 16:33:44
59,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.215711832046509,2025-12-02 16:33:44
60,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",2.621674060821533,2025-12-02 16:33:44
61,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.6893787384033203,2025-12-02 16:33:44
62,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, page 2",2.7641894817352295,2025-12-02 16:33:44
63,Hybrid RAG,True,C,C,"The effective compression ratek 1−p+kp decreases when fewer chunks are compressed (i.e.,p increases).","2509.01092v2.pdf, Page: 4",1.5634644031524658,2025-12-02 16:33:44
64,Hybrid RAG,False,B,C,"To align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction tasks for continual pre-training (CPT).","2509.01092v2.pdf, page 3",10.1067054271698,2025-12-02 16:33:44
65,Hybrid RAG,True,C,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task.","2509.01092v2.pdf, Page: 5",7.987203598022461,2025-12-02 16:33:44
66,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",2.948936939239502,2025-12-02 16:33:44
67,Hybrid RAG,False,ERROR,C,No quote provided,None,34.223100900650024,2025-12-02 16:33:44
68,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",3.3784005641937256,2025-12-02 16:33:44
69,Hybrid RAG,False,A,C,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages.,"2509.01092v2.pdf, Page: 7",31.026968479156494,2025-12-02 16:33:44
70,Hybrid RAG,True,C,C,"For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",3.071641683578491,2025-12-02 16:33:44
