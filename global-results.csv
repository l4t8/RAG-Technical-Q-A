question_number,pipeline,correct,predicted,ground_truth,quote,source,latency,run_timestamp
1,Baseline (Zero-shot),True,C,C,No quote provided,,8.124951124191284,2025-11-30 10:49:53
2,Baseline (Zero-shot),True,C,C,No quote provided,,11.022520780563354,2025-11-30 10:49:53
3,Baseline (Zero-shot),False,ERROR,B,No quote provided,,12.152956008911133,2025-11-30 10:49:53
4,Baseline (Zero-shot),True,D,D,No quote provided,,5.393776893615723,2025-11-30 10:49:53
5,Baseline (Zero-shot),True,C,C,No quote provided,,6.155365467071533,2025-11-30 10:49:53
6,Baseline (Zero-shot),False,D,C,No quote provided,,11.969016790390016,2025-11-30 10:49:53
7,Baseline (Zero-shot),True,C,C,No quote provided,,7.372412204742432,2025-11-30 10:49:53
8,Baseline (Zero-shot),True,B,B,No quote provided,,6.45120644569397,2025-11-30 10:49:53
9,Baseline (Zero-shot),False,B,C,No quote provided,,6.1436402797698975,2025-11-30 10:49:53
10,Baseline (Zero-shot),True,C,C,No quote provided,,9.830241441726685,2025-11-30 10:49:53
11,Baseline (Zero-shot),False,ERROR,C,No quote provided,,11.673871755599976,2025-11-30 10:49:53
12,Baseline (Zero-shot),False,ERROR,C,No quote provided,,15.35968542098999,2025-11-30 10:49:53
13,Baseline (Zero-shot),False,ERROR,C,No quote provided,,238.69352746009827,2025-11-30 10:49:53
14,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.523080587387083,2025-11-30 10:49:53
15,Baseline (Zero-shot),False,ERROR,C,No quote provided,,15.052305459976196,2025-11-30 10:49:53
16,Baseline (Zero-shot),True,C,C,No quote provided,,5.221926212310791,2025-11-30 10:49:53
17,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.69699215888977,2025-11-30 10:49:53
18,Baseline (Zero-shot),False,ERROR,C,No quote provided,,7.94920802116394,2025-11-30 10:49:53
19,Baseline (Zero-shot),False,A,C,No quote provided,,6.543812036514282,2025-11-30 10:49:53
20,Baseline (Zero-shot),False,ERROR,C,No quote provided,,8.601467370986938,2025-11-30 10:49:53
21,Baseline (Zero-shot),True,C,C,No quote provided,,7.254870891571045,2025-11-30 10:49:53
22,Baseline (Zero-shot),False,ERROR,C,No quote provided,,56.33492803573608,2025-11-30 10:49:53
23,Baseline (Zero-shot),True,C,C,No quote provided,,7.3724071979522705,2025-11-30 10:49:53
24,Baseline (Zero-shot),True,C,C,No quote provided,,5.222280502319336,2025-11-30 10:49:53
25,Baseline (Zero-shot),False,A,C,No quote provided,,7.725822448730469,2025-11-30 10:49:53
26,Baseline (Zero-shot),False,B,C,No quote provided,,2.7703027725219727,2025-11-30 10:49:53
27,Baseline (Zero-shot),False,A,C,No quote provided,,9.778599500656128,2025-11-30 10:49:53
28,Baseline (Zero-shot),False,ERROR,C,No quote provided,,47.30968379974365,2025-11-30 10:49:53
29,Baseline (Zero-shot),True,C,C,No quote provided,,4.606568098068237,2025-11-30 10:49:53
30,Baseline (Zero-shot),False,A,C,No quote provided,,5.837047100067139,2025-11-30 10:49:53
31,Baseline (Zero-shot),False,ERROR,C,No quote provided,,10.137154817581177,2025-11-30 10:49:53
32,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.523005962371826,2025-11-30 10:49:53
33,Baseline (Zero-shot),False,A,C,No quote provided,,3.5835235118865967,2025-11-30 10:49:53
34,Baseline (Zero-shot),False,ERROR,C,No quote provided,,8.397025108337402,2025-11-30 10:49:53
35,Baseline (Zero-shot),True,C,C,No quote provided,,4.914840936660767,2025-11-30 10:49:53
36,Baseline (Zero-shot),False,ERROR,C,No quote provided,,11.24106502532959,2025-11-30 10:49:53
37,Baseline (Zero-shot),False,A,C,No quote provided,,3.8115429878234863,2025-11-30 10:49:53
38,Baseline (Zero-shot),False,ERROR,C,No quote provided,,21.81106400489807,2025-11-30 10:49:53
39,Baseline (Zero-shot),False,ERROR,C,No quote provided,,24.575762510299683,2025-11-30 10:49:53
40,Baseline (Zero-shot),False,B,C,No quote provided,,13.516470670700071,2025-11-30 10:49:53
41,Baseline (Zero-shot),True,C,C,No quote provided,,6.450959920883179,2025-11-30 10:49:53
42,Baseline (Zero-shot),True,B,B,No quote provided,,3.993476867675781,2025-11-30 10:49:53
43,Baseline (Zero-shot),True,C,C,No quote provided,,5.529512882232666,2025-11-30 10:49:53
44,Baseline (Zero-shot),False,A,C,No quote provided,,7.37250018119812,2025-11-30 10:49:53
45,Baseline (Zero-shot),False,ERROR,C,No quote provided,,8.601684093475342,2025-11-30 10:49:53
46,Baseline (Zero-shot),False,ERROR,C,No quote provided,,9.216196298599243,2025-11-30 10:49:53
47,Baseline (Zero-shot),False,A,B,No quote provided,,5.836193084716797,2025-11-30 10:49:53
48,Baseline (Zero-shot),True,B,B,No quote provided,,4.479202032089233,2025-11-30 10:49:53
49,Baseline (Zero-shot),False,ERROR,C,No quote provided,,19.175062894821167,2025-11-30 10:49:53
50,Baseline (Zero-shot),False,ERROR,C,No quote provided,,16.857008695602417,2025-11-30 10:49:53
1,BM25 RAG,True,C,C,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.",2509.01092v2.pdf page 10,9.553980588912964,2025-11-30 10:54:37
2,BM25 RAG,False,ERROR,C,No quote provided,,7.987125635147095,2025-11-30 10:54:37
3,BM25 RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",2509.01092v2.pdf page 9,6.450937509536743,2025-11-30 10:54:37
4,BM25 RAG,True,D,D,"Our model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the sameLLaMA model without extending its effective positional encoding, maintains robust performance","2509.01092v2.pdf, Page: 7",2.7645838260650635,2025-11-30 10:54:37
5,BM25 RAG,True,C,C,"We argue that specialized techniques exploiting the unique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational overhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1) Inefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.","2509.01092v2.pdf, Page: 1",6.144155502319336,2025-11-30 10:54:37
6,BM25 RAG,True,C,C,"Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",5.10636568069458,2025-11-30 10:54:37
7,BM25 RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",6.566458702087402,2025-11-30 10:54:37
8,BM25 RAG,True,B,B,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",4.9148032665252686,2025-11-30 10:54:37
9,BM25 RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",5.222552537918091,2025-11-30 10:54:37
10,BM25 RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",2.457096099853516,2025-11-30 10:54:37
11,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","2509.01092v2.pdf, Page: 0",4.607908487319946,2025-11-30 10:54:37
12,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, page 1",2.457152843475342,2025-11-30 10:54:37
13,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.4724369049072266,2025-11-30 10:54:37
14,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, Page: 3",3.0568723678588867,2025-11-30 10:54:37
15,BM25 RAG,True,C,C,"Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the","2509.01092v2.pdf, Page: 3",4.91654896736145,2025-11-30 10:54:37
16,BM25 RAG,True,C,C,"As the chunk lengthk increases, the number of possible token combinations expands exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge.","2509.01092v2.pdf, Page: 3",17.508476972579956,2025-11-30 10:54:37
17,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",8.601657629013062,2025-11-30 10:54:37
18,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",2.91416335105896,2025-11-30 10:54:37
19,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",4.150930404663086,2025-11-30 10:54:37
20,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, page 4",2.767183780670166,2025-11-30 10:54:37
21,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.0692031383514404,2025-11-30 10:54:37
22,BM25 RAG,True,C,C,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",1.9211585521698,2025-11-30 10:54:37
23,BM25 RAG,True,C,C,"REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",3.606475353240967,2025-11-30 10:54:37
24,BM25 RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the","2509.01092v2.pdf, Page: 3",3.687748908996582,2025-11-30 10:54:37
25,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",2.45733642578125,2025-11-30 10:54:37
26,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)...","2509.01092v2.pdf, page 2",4.300543308258057,2025-11-30 10:54:37
27,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",2.2906785011291504,2025-11-30 10:54:37
28,BM25 RAG,True,C,C,"For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",2.0119946002960205,2025-11-30 10:54:37
29,BM25 RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",2.455005884170532,2025-11-30 10:54:37
30,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",4.607803583145142,2025-11-30 10:54:37
31,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.15018367767334,2025-11-30 10:54:37
32,BM25 RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, page 15",2.1501944065093994,2025-11-30 10:54:37
33,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)","2509.01092v2.pdf, page 2",4.607982635498047,2025-11-30 10:54:37
34,BM25 RAG,False,ERROR,C,No quote provided,,20.582061290740967,2025-11-30 10:54:37
35,BM25 RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over","2509.01092v2.pdf, Page: 3",3.071911334991455,2025-11-30 10:54:37
36,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",2509.01092v2.pdf page 3,3.993467569351196,2025-11-30 10:54:37
37,BM25 RAG,True,C,C,"With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",1.842723369598389,2025-11-30 10:54:37
38,BM25 RAG,False,A,C,No direct quote is available in the provided context that describes the scale and composition of the continual pre-training corpus.,,18.73942255973816,2025-11-30 10:54:37
39,BM25 RAG,True,C,C,We use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question. WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words.,"2509.01092v2.pdf, Page: 7",6.757752418518066,2025-11-30 10:54:37
40,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",2.764927387237549,2025-11-30 10:54:37
41,BM25 RAG,True,C,C,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9",8.601050615310669,2025-11-30 10:54:37
42,BM25 RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",2509.01092v2.pdf page 3,3.3793556690216064,2025-11-30 10:54:37
43,BM25 RAG,False,ERROR,C,No quote provided,,14.130847692489624,2025-11-30 10:54:37
44,BM25 RAG,False,ERROR,C,No quote provided,,19.27711009979248,2025-11-30 10:54:37
45,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",3.45501446723938,2025-11-30 10:54:37
46,BM25 RAG,True,C,C,"By exploiting this attention sparsity structure, we demonstrate a30.85× the time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity.","2509.01092v2.pdf, Page: 0",6.8721582889556885,2025-11-30 10:54:37
47,BM25 RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).","2509.01092v2.pdf, Page: 0",1.7285950183868408,2025-11-30 10:54:37
48,BM25 RAG,True,B,B,Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,"2509.01092v2.pdf, Page: 21",4.864626169204712,2025-11-30 10:54:37
49,BM25 RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",2509.01092v2.pdf: 7,3.429847955703736,2025-11-30 10:54:37
50,BM25 RAG,True,C,C,"the overall input to the decoder will be reduced by a factor of ≃k. This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence.","2509.01092v2.pdf, page 2",10.444714546203612,2025-11-30 10:54:37
1,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.4513275623321533,2025-11-30 11:01:33
2,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",7.679840564727783,2025-11-30 11:01:33
3,Dense RAG,False,D,B,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",32.15349054336548,2025-11-30 11:01:33
4,Dense RAG,True,D,D,"However, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG16 and REFRAG32 both outperform the LLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency). The same result occurs in long context scenarios.","2509.01092v2.pdf, Page: 7",13.004525423049929,2025-11-30 11:01:33
5,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",20.274879932403564,2025-11-30 11:01:33
6,Dense RAG,False,D,C,These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.,"2509.01092v2.pdf, Page: 6",17.202758312225342,2025-11-30 11:01:33
7,Dense RAG,True,C,C,"We sequentially pickT′ chunk indicesl = {lj}T′ j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′ j=1) = {E1, . . . , EL}, with Ei =e cnk i if i /∈ {lj}T′ j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′ j=1",2509.01092v2.pdf page 15,5.222073793411255,2025-11-30 11:01:33
8,Dense RAG,False,ERROR,B,No quote provided,,8.909060716629028,2025-11-30 11:01:33
9,Dense RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",2509.01092v2.pdf: page 7,4.637522459030151,2025-11-30 11:01:33
10,Dense RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",2.7366857528686523,2025-11-30 11:01:33
11,Dense RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","2509.01092v2.pdf, page 0",4.912946701049805,2025-11-30 11:01:33
12,Dense RAG,True,C,C,"For all chunks, by default, we compress them to a single token, while for crucial chunks, we expand them.","2509.01092v2.pdf, Page: 17",11.673667907714844,2025-11-30 11:01:33
13,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, page 2",3.380457878112793,2025-11-30 11:01:33
14,Dense RAG,True,C,C,A light-weight RL policy decide few chunks to expand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder. ... Reward = - Log(Perplexity) ... thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.,"2509.01092v2.pdf, Page: 2; 2509.01092v2.pdf, Page: 17; 2509.01092v2.pdf, Page: 3",5.220449686050415,2025-11-30 11:01:33
15,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","2509.01092v2.pdf, Page: 3",2.457439661026001,2025-11-30 11:01:33
16,Dense RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3",22.11843991279602,2025-11-30 11:01:33
17,Dense RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",16.588177919387817,2025-11-30 11:01:33
18,Dense RAG,False,B,C,"However, CEPE is worse than original LLaMA in TTIT since it require the additional computation of KV cache projection in the inference time.","2509.01092v2.pdf, Page: 15",32.5398895740509,2025-11-30 11:01:33
19,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, page 7",3.818894147872925,2025-11-30 11:01:33
20,Dense RAG,True,C,C,"However, due to the compression, we are able to have more context information and hence achieve better performance. Surprisingly, REFRAG16 and REFRAG32 both outperform the LLaMAFT model despite having 2× and 4× fewer tokens in the decoder (i.e., lower latency).","2509.01092v2.pdf, Page: 7",5.505451917648315,2025-11-30 11:01:33
21,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.414152145385742,2025-11-30 11:01:33
22,Dense RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.167286157608032,2025-11-30 11:01:33
23,Dense RAG,True,C,C,compressing chunks in the original token space using the RL policy.,"2509.01092v2.pdf, Page: 4",15.216393947601318,2025-11-30 11:01:33
24,Dense RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","2509.01092v2.pdf, Page: 3",3.242470979690552,2025-11-30 11:01:33
25,Dense RAG,True,C,C,"RL-trained chunk expansion policy
Reward = - Log(Perplexity)","2509.01092v2.pdf, Page: 17",3.177918672561645,2025-11-30 11:01:33
26,Dense RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)...","2509.01092v2.pdf, Page: 2",5.205416202545166,2025-11-30 11:01:33
27,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, page 7",2.4960825443267822,2025-11-30 11:01:33
28,Dense RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",16.70712161064148,2025-11-30 11:01:33
29,Dense RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",2.113134860992432,2025-11-30 11:01:33
30,Dense RAG,True,C,C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; ... REFRAG significantly reduces latency, TTFT, and memory usage during decoding, all without requiring modifications to the LLM architecture or introducing new decoder parameters.","2509.01092v2.pdf, page 1",6.693034887313843,2025-11-30 11:01:33
31,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",1.8932342529296875,2025-11-30 11:01:33
32,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.","2509.01092v2.pdf, Page: 2",2.098278045654297,2025-11-30 11:01:33
33,Dense RAG,True,C,C,"Under the context length of16384(i.e., mid-to-long context),REFRAG achieves16 .53× acceleration in TTFT with cache and8.59× without cache.","2509.01092v2.pdf, Page: 15",3.8047049045562735,2025-11-30 11:01:33
34,Dense RAG,False,ERROR,C,No quote provided,,21.747132301330566,2025-11-30 11:01:33
35,Dense RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over","2509.01092v2.pdf, Page: 3",3.04783034324646,2025-11-30 11:01:33
36,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",27.001619815826416,2025-11-30 11:01:33
37,Dense RAG,True,C,C,"With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",1.5456736087799072,2025-11-30 11:01:33
38,Dense RAG,False,NONE OF THE ABOVE,C,The information regarding the scale and composition of the continual pre-training corpus is not present in the provided context.,,3.6809377670288086,2025-11-30 11:01:33
39,Dense RAG,True,C,C,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.,2509.01092v2.pdf page 7,4.910589694976807,2025-11-30 11:01:33
40,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.399460792541504,2025-11-30 11:01:33
41,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-tations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",2.629251003265381,2025-11-30 11:01:33
42,Dense RAG,True,B,B,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",17.31766986846924,2025-11-30 11:01:33
43,Dense RAG,True,C,C,REFRAGRL uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4",5.003134727478027,2025-11-30 11:01:33
44,Dense RAG,False,D,C,"Notably, REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4",5.134072303771973,2025-11-30 11:01:33
45,Dense RAG,True,C,C,"This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.","2509.01092v2.pdf, Page: 6",5.837104558944702,2025-11-30 11:01:33
46,Dense RAG,False,D,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",12.149543762207031,2025-11-30 11:01:33
47,Dense RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).","2509.01092v2.pdf, Page: 0",1.9808554649353027,2025-11-30 11:01:33
48,Dense RAG,True,B,B,"Table 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",4.007472276687622,2025-11-30 11:01:33
49,Dense RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","2509.01092v2.pdf, Page: 7",2.956299066543579,2025-11-30 11:01:33
50,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.251978635787964,2025-11-30 11:01:33
1,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",2.6195759773254395,2025-11-30 11:06:04
2,Hybrid RAG,False,A,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task. Reconstruction task is essential for the model to learn the continual pre-training task.",2509.01092v2.pdf: 5,8.740822792053223,2025-11-30 11:06:04
3,Hybrid RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",4.275879621505737,2025-11-30 11:06:04
4,Hybrid RAG,True,D,D,"Our model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the sameLLaMA model without extending its effective positional encoding, maintains robust performance","2509.01092v2.pdf, Page: 7",3.7109382152557373,2025-11-30 11:06:04
5,Hybrid RAG,True,C,C,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.","2509.01092v2.pdf, Page: 1",8.29480767250061,2025-11-30 11:06:04
6,Hybrid RAG,True,C,C,"Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",6.143738031387329,2025-11-30 11:06:04
7,Hybrid RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",4.296944856643677,2025-11-30 11:06:04
8,Hybrid RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",6.453892230987549,2025-11-30 11:06:04
9,Hybrid RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",5.0518670082092285,2025-11-30 11:06:04
10,Hybrid RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","filename: 2509.01092v2.pdf, page 6",2.9161603450775146,2025-11-30 11:06:04
11,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.","2509.01092v2.pdf, Page: 0",4.187382459640503,2025-11-30 11:06:04
12,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.6284701824188232,2025-11-30 11:06:04
13,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",3.8789286613464355,2025-11-30 11:06:04
14,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, Page: 3",11.366766929626465,2025-11-30 11:06:04
15,Hybrid RAG,True,C,C,"In this task, wefreeze the decoder model and only train the encoder and projection layer. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.","2509.01092v2.pdf, Page: 3",2.4340732097625732,2025-11-30 11:06:04
16,Hybrid RAG,True,C,C,"As the chunk lengthk increases, the number of possible token combinations expands exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens fromLchunk embeddings further compounds the difficulty of the task. ... For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",2509.01092v2.pdf page 3,4.937188386917114,2025-11-30 11:06:04
17,Hybrid RAG,True,C,C,"Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths. ... REFRAG16 with RL-based selective compression, even without recomputing chunk embeddings, yet still surpasses the performance ofREFRAG8.","2509.01092v2.pdf page 5, 2509.01092v2.pdf page 6",3.771270990371704,2025-11-30 11:06:04
18,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",5.377793788909912,2025-11-30 11:06:04
19,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.4457175731658936,2025-11-30 11:06:04
20,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications. With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",5.527619361877441,2025-11-30 11:06:04
21,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",4.607568740844727,2025-11-30 11:06:04
22,Hybrid RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.4579858779907227,2025-11-30 11:06:04
23,Hybrid RAG,True,C,C,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",2.946272373199463,2025-11-30 11:06:04
24,Hybrid RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","2509.01092v2.pdf, Page: 3",3.504634141921997,2025-11-30 11:06:04
25,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",2.1504478454589844,2025-11-30 11:06:04
26,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)",2509.01092v2.pdf: page 2,4.607142686843872,2025-11-30 11:06:04
27,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.583852767944336,2025-11-30 11:06:04
28,Hybrid RAG,True,C,C,"For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",1.8434333801269531,2025-11-30 11:06:04
29,Hybrid RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",2.2389488220214844,2025-11-30 11:06:04
30,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",3.699885368347168,2025-11-30 11:06:04
31,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",3.6956629753112793,2025-11-30 11:06:04
32,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",2.7572531700134277,2025-11-30 11:06:04
33,Hybrid RAG,True,C,C,"Under the context length of16384(i.e., mid-to-long context),REFRAG achieves16 .53× acceleration in TTFT with cache and8.59× without cache.","2509.01092v2.pdf, Page: 15",3.056879758834839,2025-11-30 11:06:04
34,Hybrid RAG,False,A,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction ofx1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task.","2509.01092v2.pdf, Page: 5",17.53019690513611,2025-11-30 11:06:04
35,Hybrid RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",2509.01092v2.pdf page 3,3.9858968257904053,2025-11-30 11:06:04
36,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",12.597234010696411,2025-11-30 11:06:04
37,Hybrid RAG,True,C,C,"With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",1.53359055519104,2025-11-30 11:06:04
38,Hybrid RAG,False,A,C,Information regarding the scale and composition of the continual pre-training corpus is not provided in the context.,N/A,31.95108413696289,2025-11-30 11:06:04
39,Hybrid RAG,True,C,C,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.,"2509.01092v2.pdf, Page: 7",2.8914127349853516,2025-11-30 11:06:04
40,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",3.253209114074707,2025-11-30 11:06:04
41,Hybrid RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-tations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",3.375455141067505,2025-11-30 11:06:04
42,Hybrid RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the","2509.01092v2.pdf, Page: 3",3.379262924194336,2025-11-30 11:06:04
43,Hybrid RAG,False,ERROR,C,No quote provided,None,13.209749698638916,2025-11-30 11:06:04
44,Hybrid RAG,False,ERROR,C,No quote provided,None,15.156865119934082,2025-11-30 11:06:04
45,Hybrid RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",2.659874439239502,2025-11-30 11:06:04
46,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity.","2509.01092v2.pdf, Page: 0",6.801831483840942,2025-11-30 11:06:04
47,Hybrid RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).","2509.01092v2.pdf, page 0",1.416224718093872,2025-11-30 11:06:04
48,Hybrid RAG,True,B,B,"Table 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",4.685478210449219,2025-11-30 11:06:04
49,Hybrid RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","2509.01092v2.pdf, Page: 7",2.944093942642212,2025-11-30 11:06:04
50,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",1.9693737030029297,2025-11-30 11:06:04
