question_number,pipeline,correct,quote,source,paper_reference,fuzz_score,source_text_for_judge,faithfulness_score,faithfulness_reasoning
1.0,BM25 RAG,True,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts, REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.","2509.01092v2.pdf, Page: 10","REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",46.58634538152611,"7 Conclusion
In this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.
By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG
compresses, senses, and expands context representations to significantly reduce both memory usage and
inference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,
including RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG
achieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in
perplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-
based systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG
provides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive
applications.
8 Acknowledgements",0.99,"The quote is an exact sentence extracted from the source text. The only difference is the addition of a space between 'contexts,' and 'REFRAG' in the quote, which was missing in the source text (i.e., 'contexts,REFRAG'). This is a minor formatting adjustment and does not change the content or meaning of the text."
2.0,BM25 RAG,False,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly).","2509.01092v2.pdf, Page: 5","The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",48.93617021276596,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",0.0,"The provided quote is not present anywhere in the source text. While the source text discusses 'reconstruction task' and 'curriculum learning', the specific sentence 'performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly)' is entirely absent. Therefore, it is a hallucinated or unrelated extraction."
3.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text."
4.0,BM25 RAG,True,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.",filename: 2509.01092v2.pdf page 7,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",100.0,"2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we
retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.
Result analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.
Our model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three
datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of
LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting
in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on
the sameLLaMA model without extending its effective positional encoding, maintains robust performance
3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. No changes or omissions were made."
5.0,BM25 RAG,True,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.",filename: 2509.01092v2.pdf page 1,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",47.96747967479674,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",0.8,"The quote accurately synthesizes information from the source text. The phrase 'being uninformative and reused across multiple inferences' is a verbatim extraction from the beginning of the source. However, the preceding part of the quote ('RAG contexts often contain sparse information, with many retrieved passages') is a paraphrase and synthesis of other concepts mentioned in the source (e.g., 'Sparse Attention', 'most context chunks'). While the information conveyed is fully supported and highly accurate, the entire quote is not a verbatim or near-verbatim extraction of a single, complete sentence from the source, as it combines and rephrases elements."
6.0,BM25 RAG,True,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.",filename: 2509.01092v2.pdf page 4,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",46.01769911504425,"chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection
policy under differentp. The perplexity-based selection is an heuristic based selection which compresses
chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured
by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can
therefore be compressed with minimal information loss. Ideally, this approach should outperform random
selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves
superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while",0.2,"The quote begins with the phrase 'The reconstruction task, while' which is present at the very end of the source text. However, the remainder of the quote ('intuitive, is particularly challenging when multiple chunks must be reconstructed.') is not found in the provided source text. The source text ends abruptly after 'The reconstruction task, while'."
7.0,BM25 RAG,True,"...a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1","This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",97.75280898876404,"C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain
uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at
arbitrary positions, which is essential for the subsequent RL policy learning.
We sequentially pickT′ chunk indicesl = {lj}T′
j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′
j=1) =
{E1, . . . , EL}, with Ei =e cnk
i if i /∈ {lj}T′
j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′
j=1
16",0.1,"The quote describes the function of a 'lightweight reinforcement learning (RL) policy' that determines when to use 'full chunk token input' versus 'low-cost, approximate chunk embeddings'. While the source text mentions 'RL policy learning' and describes a mechanism for handling 'compressed' (approximate) and 'uncompressed' (full token) chunks, the specific sentence or a near-verbatim version of the quote is not present. The quote appears to be an interpretation or summary of what such a policy would do, rather than a direct extraction from the provided text."
8.0,BM25 RAG,True,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",82.08092485549133,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,"The quote accurately conveys the information from the source text regarding acceleration for longer context lengths. The source states: 'For longer context length, acceleration reaches up tok2× for both metrics.' The quote rephrases this as: 'With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.' While not verbatim, the changes are minor (e.g., 'For' to 'With', 'length' to 'lengths', 'reaches' to 'we are able to achieve') and the explicit mention of 'TTFT and throughput' is an accurate clarification of 'both metrics' given the context of the preceding sentences in the source text. The core meaning and numerical value (k2x acceleration) are perfectly preserved and fully supported."
9.0,BM25 RAG,True,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",filename: 2509.01092v2.pdf page 7,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",98.96907216494846,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
10.0,BM25 RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",100.0,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The provided quote, 'In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance,' is not present anywhere in the source text. The source text mentions 'compression' and 'compression rate of 8' but does not discuss a compression rate of 64 or the associated performance implications described in the quote."
11.0,BM25 RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",100.0,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the provided source text."
12.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source."
13.0,BM25 RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",96.35854341736696,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the source text perfectly without any alterations or omissions."
14.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form... The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...",filename: 2509.01092v2.pdf page 3,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",98.65319865319864,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",0.95,"The first sentence of the quote ('A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form') is an exact, verbatim extraction from the source text. The ellipsis correctly indicates an omission of a sentence. The subsequent text ('The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s') is also verbatim from the source. However, the quote adds the phrase 'autoregressive property' at the very end, which is not present in the provided source text. Despite this small addition, the vast majority of the quote is directly from the source."
15.0,BM25 RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",100.0,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",0.95,The quote is nearly an exact sentence from the source text. The only difference is a minor spacing issue: the source text has 'wefreeze' while the quote has 'we freeze'. This is a near-verbatim extraction.
16.0,BM25 RAG,True,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3","As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",90.64748201438849,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted from the source text. The line break in the source text ('over\ntime') does not alter the content of the sentence."
17.0,BM25 RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"Source Reference: 2509.01092v2.pdf, Page: 5","Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.0,"The quote is not supported by the source text. Several key elements are incorrect or hallucinated:
1.  **'Table 13'**: The source text mentions 'Table 1' and 'Table 2', but 'Table 13' is not present.
2.  **'REFRAG16 ... consistently outperforms REFRAG8'**: The source text states, 'Notably, REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings'. It groups REFRAG8 and REFRAG16 together as both performing well against *other baselines*, but it does not state that REFRAG16 consistently outperforms REFRAG8. In fact, it later mentions 'both REFRAG8 and REFRAG16 maintain superior performance at longer contexts', implying similar strong performance.
3.  **'across different datasets and context lengths'**: While the source discusses performance across different settings and longer contexts for REFRAG8 and REFRAG16 generally, this specific phrase is used to support the unsupported claim of REFRAG16 outperforming REFRAG8."
18.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","Source Reference: 2509.01092v2.pdf, Page: 9","CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",99.62406015037594,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction from the provided source text. Every word and punctuation mark matches perfectly."
19.0,BM25 RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.0,"The quote contains specific numerical details ('8 passages for REFRAG vs. 1 for LLaMA', '1.93% average gain') that are not present anywhere in the provided Source Text. While the Source Text discusses REFRAG outperforming LLaMA under equal latency and mentions '16 tasks', it does not provide the specific passage counts or the percentage gain stated in the quote. Therefore, the quote is not supported by the source text and appears to be hallucinated."
20.0,BM25 RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",46.29213483146067,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text. The only difference is a space between 'a' and '4k' in the quote, which is a minor formatting detail and does not alter the content or meaning."
21.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
22.0,BM25 RAG,True,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",83.04093567251462,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,"The quote accurately conveys the information from the source text regarding acceleration for longer context lengths. The source states: 'For longer context length, acceleration reaches up tok2× for both metrics.' The quote rephrases 'For longer context length' to 'With longer context lengths', changes 'acceleration reaches' to 'we are able to achieve up to k²× acceleration', and interprets 'for both metrics' as 'in both TTFT and throughput' (which is accurate given the preceding sentence defines TTFT and throughput as the metrics). While not a verbatim extraction, the core factual claim is identical and fully supported by the source text, making it a highly faithful and near-verbatim representation of the information."
23.0,BM25 RAG,True,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications.",filename: 2509.01092v2.pdf page 1,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",98.29787234042551,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",0.45,"The quote begins verbatim with 'Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at' which is found at the very end of the provided source text. However, the source text ends there, and the remainder of the quote, 'arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications.', is not present in the given source text. Therefore, a significant portion of the quote is not supported."
24.0,BM25 RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",99.0950226244344,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'The main objectives are to align...' perfectly."
25.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3","Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches perfectly without any alterations or omissions."
26.0,BM25 RAG,True,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache...",filename: 2509.01092v2.pdf page 2,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",98.59154929577466,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.8,"The quote accurately extracts the key numerical values (context length 16384, 16.53x TTFT acceleration with cache, 8.59x without cache) and the subject (REFRAG) from the source text. It also correctly references Figure 2 and the 'mid-to-long context'. However, there are several deviations from a verbatim or near-verbatim extraction: 1. The introductory phrase 'Empirically, as shown in figure 2, with a context length of' is a significant rephrasing and condensation of the original text 'Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of the acceleration ofREFRAGcompared with CEPE... Under the context length of'. 2. The quote adds 'with k = 16' after 'REFRAG'. While the source text mentions 'k× acceleration' in a general sense earlier, it does not explicitly state 'k = 16' in conjunction with the 16.53x and 8.59x acceleration figures for REFRAG in the provided segment. This is an unsupported addition of a specific detail within the provided source text. 3. There is a minor rephrasing of 'acceleration in TTFT' to 'TTFT acceleration'. Due to the significant rephrasing of the introductory part and, more critically, the insertion of a specific detail ('with k = 16') that is not explicitly present in the provided source text for this specific context, the quote is not considered near-verbatim."
27.0,BM25 RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",95.6896551724138,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.05,"The quote contains specific numerical claims ('8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement', 'across 16 RAG tasks') that are not present anywhere in the provided source text. While the source text generally discusses REFRAG outperforming LLaMA under the same latency, it does not provide these specific figures or conditions. Therefore, the quote is largely unsupported by the given source."
28.0,BM25 RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim sentence extracted from the source text. There are only negligible whitespace differences (e.g., 'yet REFRAG8' vs 'yetREFRAG8') which do not alter the content or meaning."
29.0,BM25 RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",100.0,"chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the
effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression
rate without compromising performance.
REFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG
under different compression rates in the continual pre-training task. We observe a performance regression as
the compression rate increases; however, even at a compression rate of32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.
Different combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B",0.99,"The quote is an almost verbatim extraction from the source text. The only difference is the presence of a space between 'of' and '64' in the quote ('of 64'), whereas the source text has no space ('of64'). This is a minor formatting difference and does not change the meaning or content of the extracted text."
30.0,BM25 RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4","The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",100.0,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",0.95,The quote is nearly an exact sentence from the source text. The only difference is the addition of a space between 'a' and '4k' in the quote ('a 4k') compared to the source ('a4k'). This is a minor formatting change and does not alter the meaning or content.
31.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
32.0,BM25 RAG,True,"Table 6 shows that with short context length s we are able to achieve k× acceleration in TTFT and up to k× acceleration in throughput. With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",79.82456140350877,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is very close to the source text. The main differences are minor formatting changes: 'k×' in the source sometimes appears without a space before the '×' (e.g., 'achievek×', 'uptok×', 'uptok2×'), while the quote consistently uses a space or renders it as 'k×' and 'k²×'. Specifically, 'k2×' in the source is rendered as 'k²×' in the quote, which is a typographical improvement but not an exact verbatim match. The meaning is fully preserved."
33.0,BM25 RAG,True,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).","2509.01092v2.pdf, Page: 2","With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",97.51773049645392,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction of a sentence from the provided source text. No changes, additions, or omissions were found."
34.0,BM25 RAG,True,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","2509.01092v2.pdf, Page: 3","In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",56.57370517928286,"on empirical evaluation is in section A.
3 Methodology
To align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction
tasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number
of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.
To further enhance performance, we introduce selective compression via RL. After aligning the encoder and
decoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream
tasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.
During CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in
predicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for
next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any",1.0,"The quote is an exact, verbatim extraction of a sentence found within the 'Source Text'. It starts with 'This task encourages...' and ends with '...downstream applications.'"
35.0,BM25 RAG,True,"For the reconstruction task, training begins with reconstructing a single chunk... Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",89.36170212765957,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",0.95,The quote accurately extracts two distinct phrases from the source text that are consecutive in the original. The ellipsis '...' correctly indicates the omission of intermediate text between these two phrases. The extracted parts themselves are verbatim.
36.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","Source Reference: 2509.01092v2.pdf, Page: 3","A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",98.1818181818182,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",0.85,"The first sentence of the quote, 'A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.', is an exact, verbatim extraction from the source text. However, the second sentence of the quote, 'The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.', deviates from the source text. While it matches the source up to 'preserving the decoder’s', it adds the phrase 'autoregressive property.' which is not present in the provided source text. This addition means the second sentence is not a verbatim or near-verbatim extraction."
37.0,BM25 RAG,True,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4","With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",97.79735682819384,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text. The only difference is the presence of spaces around the numbers '16' and '9.3%' in the quote, which are absent in the source text (e.g., 'of16' vs 'of 16', 'a9.3%' vs 'a 9.3%'). This is a minor formatting variation and does not alter the content or meaning, making it a verbatim extraction."
38.0,BM25 RAG,False,No quote provided,,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
39.0,BM25 RAG,True,We follow the work of Linetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,93.5064935064935,"accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in
table 7.
Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl
dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We
use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.
(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.
Result analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,
varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.
This is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is
the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We",0.95,"The quote is a near-verbatim extraction. It accurately captures the content from the source text. There is a minor difference in spacing ('Wefollow' in the source vs. 'We follow' in the quote) and the quote uses an ellipsis to indicate the truncation of the last sentence, which is a standard practice for near-verbatim extractions."
40.0,BM25 RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",94.82758620689656,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote is not present in the provided source text. It contains specific numerical claims (e.g., '8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement across 16 RAG tasks') that are not mentioned or supported anywhere in the source text. While the source discusses REFRAG, LLaMA, latency, and performance, it does not contain this particular statement."
41.0,BM25 RAG,True,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9","REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",48.52941176470589,"the KV cache to reduce memory requirements for long-context applications. However, this approach only
decreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the
model from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,
generating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive
nature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding
latency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings
for prediction, similar to our method. However, their sequential compression process results in high latency
when the summary vector is not cached, and their approach only supports applications where the summary
token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our",1.0,"The quote is an exact, verbatim extraction from the source text."
42.0,BM25 RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",99.0950226244344,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the text starting from 'The main objectives are to align...' to '...embeddings into the' without any changes or omissions."
43.0,BM25 RAG,True,We employ RL to train a policy that optimally determines which segments to compress.,"2509.01092v2.pdf, Page: 15","Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",51.19047619047619,"C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain
uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at
arbitrary positions, which is essential for the subsequent RL policy learning.
We sequentially pickT′ chunk indicesl = {lj}T′
j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′
j=1) =
{E1, . . . , EL}, with Ei =e cnk
i if i /∈ {lj}T′
j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′
j=1
16",0.85,"The quote is not a verbatim or near-verbatim extraction from the source text. However, the core concepts expressed in the quote are strongly supported and accurately synthesized from the provided text. The source mentions 'subsequent RL policy learning,' which directly supports the idea of employing RL to train a policy. Additionally, the text describes the process of selecting 'chunks to remain uncompressed,' which implies a determination of which segments are compressed. While not a direct copy-paste, the statement accurately reflects the implied purpose and mechanism described in the source."
44.0,BM25 RAG,False,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9","In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",47.77777777777777,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the provided source text."
45.0,BM25 RAG,True,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,98.65771812080536,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
46.0,BM25 RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0","Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",50.12919896640827,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the provided source text."
47.0,BM25 RAG,True,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",100.0,"The data transfer latency to high-bandwidth memory is2n+4dlb(s+o)
m .
Throughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given
by:
Throughput= bo
TTFT+DL
where DL is the data latency.
Before After
KV cache memory4dlb(s+o) 4dlb
",0.0,"The quote is not present in the provided source text. The source text mentions TTFT and TTIT with formulas and in a comparison table, but it does not state that TTFT latency increases quadratically or that TTIT latency grows linearly with prompt length, nor does it contain the citation '(Liu et al., 2025)'."
48.0,BM25 RAG,True,Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,filename: 2509.01092v2.pdf page 21,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",66.12903225806451,"Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the
success of reconstruction task.
Ablation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and
without continuing from reconstruction task.
22",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
49.0,BM25 RAG,True,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","Source Reference: 2509.01092v2.pdf, Page: 7","The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim extraction of two consecutive sentences from the provided source text. There are no alterations, omissions, or additions."
50.0,BM25 RAG,False,"attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements. It is complementary to our approach and can be integrated to further improve latency.","2509.01092v2.pdf, Page: 9","It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",52.84552845528455,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the first two sentences of the provided source text precisely."
51.0,BM25 RAG,True,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1","3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",100.0,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches the source text perfectly."
52.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction of a sentence (or a significant clause acting as a sentence) from the source text. It appears in the source as part of an enumerated list: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
53.0,BM25 RAG,True,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk
i = ϕ(ci).","2509.01092v2.pdf, Page: 1","The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",98.6013986013986,"foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu
et al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into
L := s
k number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then
processes all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding
is then projected with a projection layerϕ to match the size of the token embedding of the decoder model,
ecnk
i = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token
embeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk
1 , . . . ,ecnk
L })wheree i is the
2",0.95,"The quote is a near-verbatim extraction from the source text. The only difference is the presence of a space between 'layer' and 'ϕ' in the quote, which is absent in the source text ('layerϕ'). This is a minor formatting difference and does not alter the meaning or content of the extracted sentence."
54.0,BM25 RAG,True,"For longer context length, acceleration reaches up to k²× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",98.66666666666669,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is a very close paraphrase of the sentence 'With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.' from the source text. While not an exact verbatim copy (e.g., 'For longer context length' instead of 'With longer context lengths', 'acceleration reaches up to' instead of 'we are able to achieve up to', and 'for both metrics' summarizing 'in both TTFT and throughput'), it accurately captures the full meaning and all key information of the original sentence."
55.0,BM25 RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",100.0,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",0.95,The quote is nearly an exact sentence from the source text. The only difference is a minor spacing issue: the source text has 'wefreeze' while the quote has 'we freeze'. All words and their order are identical.
56.0,BM25 RAG,True,The results indicate that curriculum learning is essential for the success of the reconstruction task.,filename: 2509.01092v2.pdf page 5,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",70.34482758620689,"Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the
success of reconstruction task.
Ablation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and
without continuing from reconstruction task.
22",0.65,"The quote accurately conveys the core meaning of the first sentence in the source text. However, it is a paraphrase rather than a verbatim or near-verbatim extraction. The phrase 'Table 11 shows the' is replaced with 'The results indicate that', and 'necessity of' is replaced with 'essential for'. While the semantic content is highly faithful, the significant rephrasing and lack of direct word extraction prevent a higher score for 'verbatim (or near-verbatim) extraction'."
57.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches perfectly without any alterations or omissions."
58.0,BM25 RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5","This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.1,"The quote contains several significant inaccuracies and fabrications not present in the source text. Firstly, 'Table 13' is mentioned in the quote, but the source text only refers to 'Table 1' and 'Table 2'. Secondly, the quote states that 'REFRAG16 ... consistently outperforms REFRAG8', which is not supported by the source. The source text consistently groups 'REFRAG8 and REFRAG16' together, stating they 'consistently outperform other baselines' and 'maintain superior performance at longer contexts', but never claims REFRAG16 outperforms REFRAG8. Lastly, 'across different datasets' is not explicitly mentioned in the source, although 'across nearly all settings' and 'at longer contexts' are present. The claim about 'RL-based selective compression' is mentioned for 'REFRAGRL', not directly for 'REFRAG16' in the context of outperforming REFRAG8. Due to these major discrepancies, the quote is largely fabricated and not a faithful extraction."
59.0,BM25 RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",94.82758620689656,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote contains specific numerical claims ('8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement', '16 RAG tasks') that are not present anywhere in the provided Source Text. While the source mentions REFRAG outperforming LLaMA under the same latency, it does not quantify this performance with the numbers stated in the quote. Therefore, the quote is unsupported by the source text."
60.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text. It starts with 'However, CEPE is limited...' and ends with '...or summarization.', matching the source precisely."
61.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted from the source text. It appears as the third advantage listed: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
62.0,BM25 RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",95.73459715639812,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the source text perfectly without any alterations or omissions."
63.0,BM25 RAG,True,"The effective compression rate k
1−p+kp decreases when",filename: 2509.01092v2.pdf page 4,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",100.0,"chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection
policy under differentp. The perplexity-based selection is an heuristic based selection which compresses
chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured
by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can
therefore be compressed with minimal information loss. Ideally, this approach should outperform random
selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves
superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while",0.95,The quote is nearly verbatim from the source text. The only difference is a space between 'rate' and 'k' in the quote ('rate k') which is absent in the source text ('ratek'). This is a minor formatting difference that does not alter the meaning or content.
65.0,BM25 RAG,True,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"2509.01092v2.pdf, Page: 5","Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",51.93370165745856,"Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the
success of reconstruction task.
Ablation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and
without continuing from reconstruction task.
22",0.75,"The quote accurately conveys the core meaning from the source text regarding the importance of curriculum learning for the reconstruction task. However, it is a paraphrase rather than a verbatim or near-verbatim extraction. The original phrasing 'Table 11 shows the necessity of curriculum learning to the success of reconstruction task' has been rephrased to 'The results indicate that curriculum learning is essential for the success of the reconstruction task,' involving changes in the introductory phrase ('Table 11 shows' to 'The results indicate that') and the specific wording and structure ('necessity of X to Y' to 'X is essential for Y'). While semantically faithful, it does not meet the 'verbatim (or near-verbatim) extraction' criterion for a higher score."
66.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9","CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a sentence found within the provided source text."
67.0,BM25 RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.2,"The source text discusses REFRAG, LLaMA, equal latency, and performance over 16 tasks, which provides context for the quote. However, the specific numerical details such as '8 passages for REFRAG vs. 1 for LLaMA' and the '1.22% average improvement' are not present in the provided source text. Therefore, the quote is not a verbatim or near-verbatim extraction from the given text."
68.0,BM25 RAG,False,"REFRAG: Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o.","Source Reference: 2509.01092v2.pdf, Page: 17","Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",44.66666666666666,"feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote
our model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression
using our RL policy.
B.2 Additional Details on Hyperparameters and Experimental Settings for CPT
Hyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the
encoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the
18",0.25,"The quote contains significant information not present in the source text. Specifically, the phrases 'REFRAG: Our approach is illustrated in figure 1.' and 'We use RoBERTa-large (Liu et al., 2019) as the encoder,' are entirely absent from the provided source. Only the concluding phrase 'feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o.' is found verbatim in the source text. The majority of the quote is not supported, and the context of the supported phrase is altered by the preceding unsupported text, making it far from a verbatim or near-verbatim extraction."
69.0,BM25 RAG,False,No quote provided,,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
70.0,BM25 RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim extraction from the source text. The only differences are minor formatting (spacing around 'LLaMA256' and 'REFRAG8') which do not alter the content or meaning."
1.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted from the source text. It matches 'It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.' perfectly."
2.0,Dense RAG,False,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5","The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",61.36363636363637,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
3.0,Dense RAG,False,"Both higher than CEPE (i.e., 2.01× and 1.04× acceleration respectively) while having better model performance (see table 1).","2509.01092v2.pdf, Page: 15","CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",39.516129032258064,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.65,"The quote is not a verbatim or near-verbatim extraction from the source text. While it accurately conveys the general claims, it significantly rephrases and simplifies parts of the original sentence. Specifically: 
1. 'surpassing' is changed to 'higher than'.
2. 'i.e.' and 'acceleration' are added to the parenthetical explanation of CEPE's performance.
3. The specific quantitative detail 'achieving 9.3% performance (measured by log-perplexity) compared to CEPE' is generalized to 'having better model performance'. This is a substantial semantic change, losing specific information.
4. '(table 1)' is changed to '(see table 1)'.
These alterations prevent the quote from being considered a near-verbatim extraction."
4.0,Dense RAG,True,This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.,"2509.01092v2.pdf, Page: 7","This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",48.96551724137931,"accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in
table 7.
Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl
dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We
use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.
(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.
Result analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,
varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.
This is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is
the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We",0.0,The provided quote is not found anywhere in the source text. It appears to be completely unrelated or hallucinated with respect to the given source.
5.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",46.66666666666666,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'It reduces attention computation complexity...' perfectly."
7.0,Dense RAG,True,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2","This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",44.99999999999999,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",0.0,"The provided quote, 'Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.', is not present anywhere in the 'Source Text'. While the source text discusses 'RL-based selective compression', the specific sentence quoted is entirely absent."
8.0,Dense RAG,True,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",83.04093567251462,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.85,"The quote accurately conveys the core information regarding k²× acceleration for longer context lengths in TTFT and throughput. However, it is not a verbatim or near-verbatim extraction. It rephrases the original sentence from the source text: 'For longer context length, acceleration reaches up tok2× for both metrics.' The quote changes include: 'For' to 'With', 'context length' to 'context lengths', 'acceleration reaches' to 'we are able to achieve', and 'for both metrics' to 'in both TTFT and throughput'. While the meaning is fully supported and accurate based on the source text (as 'metrics' refers to TTFT and throughput defined earlier), these changes in wording and sentence structure are more significant than what would typically be considered 'near-verbatim'."
9.0,Dense RAG,True,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7","Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",98.96907216494846,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
10.0,Dense RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",100.0,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The provided quote, 'In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.', is not found anywhere in the source text. The source text mentions 'compression rate of 8' but not '64' or the specific phrase from the quote."
11.0,Dense RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0","These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",100.0,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the provided source text."
12.0,Dense RAG,False,"attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements.",filename: 2509.01092v2.pdf page 9,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",52.8,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction from the beginning of the source text. The line break in the source text after 'address' does not alter the textual content or sequence of words."
13.0,Dense RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",96.33802816901408,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,"The quote is nearly verbatim from the source text. The only difference is the rendering of 'k²×' in the quote versus 'k2×' in the source text. While the meaning is identical (k squared times), the character representation is slightly different, preventing a perfect 1.0 score."
14.0,Dense RAG,True,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, Page: 3","Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",83.61204013377926,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s
autoregressive property and enabling flexible placement of compression. Further discussion on sequential
selection is provided in section A.1.
4",1.0,"The quote is an exact, verbatim extraction of the first sentence from the provided source text. It perfectly matches the content from 'chunk embeddings...' up to '...placement of compression.'"
15.0,Dense RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer. [...] The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",98.5645933014354,"minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.
The reconstruction task was specifically chosen to encourage the model to rely on context memory rather
than its parametric memory during training. Once the encoder is aligned with the decoder through this
reconstruction task, we initiate CPT byunfreezing the decoder.
Curriculum learning.The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk lengthk increases, the number of possible token combinations expands
exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity
within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.",0.5,"The second sentence of the quote, 'The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.', is an exact, verbatim extraction from the source text. However, the first sentence, 'In this task, we freeze the decoder model and only train the encoder and projection layer.', is not present in the source text, nor is it a near-verbatim paraphrase. While the source text implies that the decoder was frozen during the reconstruction task ('Once the encoder is aligned with the decoder through this reconstruction task, we initiate CPT byunfreezing the decoder.'), the exact phrasing of the first sentence of the quote is not found."
16.0,Dense RAG,True,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",90.64748201438849,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'To continuously adjust task difficulty...' perfectly."
17.0,Dense RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.05,"The quote contains multiple significant inaccuracies and unsupported claims. 
1. The source text mentions 'Table 1' and 'Table 2', but 'Table 13' is not present in the source text, making it a hallucination.
2. The source states 'REFRAG8 and REFRAG16 consistently outperform other baselines'. The quote claims 'REFRAG16 ... consistently outperforms REFRAG8', which is a direct misrepresentation of the source's statement. The source groups them together as performing well, not one outperforming the other.
3. While 'RL-based selective compression' is mentioned in relation to 'REFRAGRL', the quote specifically attributes it to 'REFRAG16' in a way that implies REFRAG16 is *the* model using it to outperform REFRAG8, which is not explicitly stated.
4. 'across different datasets' is not mentioned in the source text; it refers to 'across nearly all settings' and 'at longer contexts'.
Due to these major discrepancies and factual errors, the quote is largely unsupported and misleading."
18.0,Dense RAG,True,"our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4","CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",44.14414414414415,"the KV cache to reduce memory requirements for long-context applications. However, this approach only
decreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the
model from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,
generating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive
nature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding
latency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings
for prediction, similar to our method. However, their sequential compression process results in high latency
when the summary vector is not cached, and their approach only supports applications where the summary
token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our",0.0,"The quote is not present in the provided source text. The source text ends with 'In contrast, our', and the quote begins with 'our approach enables...', indicating it is a continuation that is missing from the given source."
19.0,Dense RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.2,"The source text discusses the comparison between REFRAG and LLaMA under 'equivalent latency constraints' and mentions 'performance averaged over all 16 tasks'. It also states that REFRAG 'consistently outperform LLaMA' under these conditions. However, the specific numerical details provided in the quote, such as '(8 passages for REFRAG vs. 1 for LLaMA)' and 'achieves a 1.93% average gain', are not present in the provided source text. While the general context is mentioned, the core specific data points of the quote are not supported verbatim or near-verbatim by the source."
20.0,Dense RAG,True,The input context is chunked and processed by the light-weight encoder to produce chunk embeddings... These chunk embeddings along with the token embeddings of the question input are fed to the decoder.,"2509.01092v2.pdf, Page: 2","By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",45.97701149425287,"Donald Trump is 
the President of 
the United States. 
He assumed office 
on January 20, 
2025, making
him the 47th 
President of the 
United States.
 Encoder  Encoder  Encoder
Context Text
Decoder-only Foundation ModelSequence
Precomputable
Light-weight
EncoderWho is the President of USA?
Decoder Tokenizer & 
Embedding
Decoder Input Text
Token Embedding
Chunk 
Embedding
Light-weight RL-trained chunk expansion policy
Vector DBQuery 
Encoder
Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to
produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to
expand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.
token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the
input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This",0.95,"The quote is a near-verbatim extraction from the source text. The first part, 'The input context is chunked and processed by the light-weight encoder to produce chunk embeddings', is found verbatim. The second part, 'These chunk embeddings along with the token embeddings of the question input are fed to the decoder', is also found verbatim. The ellipsis '...' correctly indicates that some text was omitted between these two phrases in the original source text."
21.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
22.0,Dense RAG,True,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",97.2972972972973,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is a near-verbatim extraction from the source text. The source states: 'With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.' The quote rephrases 'With longer context lengths' to 'For longer context length', 'we are able to achieve' to 'acceleration reaches', and 'in both TTFT and throughput' to 'for both metrics'. Despite these minor rephrasing, the core information and numerical value (k2x acceleration for both metrics under longer context) are accurately preserved and fully supported by the source."
23.0,Dense RAG,True,These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.,"2509.01092v2.pdf, Page: 6","Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",48.38709677419355,"chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the
effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression
rate without compromising performance.
REFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG
under different compression rates in the continual pre-training task. We observe a performance regression as
the compression rate increases; however, even at a compression rate of32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.
Different combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
24.0,Dense RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",99.8158379373849,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",0.7,"The quote begins as a verbatim extraction from the source text. However, the source text ends abruptly with 'into the', while the quote continues and completes the sentence with additional information: 'decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.' This latter part of the quote is not present in the provided source text, making the quote not fully supported or verbatim."
25.0,Dense RAG,True,Reward = - Log(Perplexity),"2509.01092v2.pdf, Page: 17","Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",57.692307692307686,"embeddings {ci}L
i=1 as transformer input and do not recompute logitssi after each selection, as state changes
have minimal impact and this improves training speed.
We use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to
minimize contamination across different segment prediction task. Specifically, for eachx we randomly select
Gnumber of lengthT ′ action sequences{l (i)}G
i=1 . We have the following objective:
Jθ = 1
G
PG
i=1 E x∼P(X),
{l(i)}G
i=1∼πθ([L]|x)
1
T′
PT′
t=1 min

πθ(l(i)
t |x,{l(i)
j }t−1
j=1)
πθold(l(i)
t |x,{l(i)
j }t−1
j=1)A(i)
t ,clip

πθ(l(i)
t |x,{l(i)
j }t−1
j=1)
πθold(l(i)
t |x,{l(i)
j }t−1
j=1),1−ϵ,1 +ϵ

A(i)
t

(1)
where ϵ is the clipping hyperparameter in PPO (Schulman et al., 2017) for stable training,θ is the current
policy andθold is the policy fro the previous iteration,At is the advantage function. We define our advantage
function using the negative log-perplexity on theotokens xs+1:s+o:
ri =r
",0.35,"The source text states: 'We define our advantage function using the negative log-perplexity on theotokens xs+1:s+o: ri =r'. The quote, however, states: 'Reward = - Log(Perplexity)'. The source explicitly defines the *advantage function* using negative log-perplexity, not the *reward*. While reward and advantage function are related in reinforcement learning, they are distinct concepts. The quote is neither a verbatim nor a near-verbatim extraction and misattributes the definition from the advantage function to the reward."
26.0,Dense RAG,True,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...","2509.01092v2.pdf, Page: 2","Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",98.59943977591035,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.85,"The quote accurately extracts most of the numerical data and conditions regarding REFRAG's acceleration (context length, TTFT acceleration with/without cache). However, there are two main deviations from a verbatim or near-verbatim extraction: 
1. The beginning 'Empirically, as shown in figure 2,' is a rephrasing of the source's introduction to Figure 2 ('Figure 2 shows the empirical measurement...').
2. The quote adds 'with k = 16' which is not explicitly stated in the provided source text in direct relation to the '16.53× acceleration'. While 'k' is mentioned as a variable earlier in the source, the specific value '16' is an addition of information not directly supported by the provided text snippet for this specific empirical result. This addition prevents it from being considered near-verbatim."
27.0,Dense RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",95.6896551724138,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.25,"The source text states that 'REFRAG... outperform significantly under the same latency,' which aligns with the general premise of the quote regarding REFRAG's performance at equal latency. However, all the specific numerical details in the quote, such as '(8 passages for REFRAG vs. 1 for LLaMA)', 'a 1.22% average improvement', and 'across 16 RAG tasks', are not present anywhere in the provided source text. Therefore, the quote is not a verbatim or near-verbatim extraction and introduces significant new information not supported by the source."
28.0,Dense RAG,True,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4","For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",45.51282051282052,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
29.0,Dense RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",100.0,"chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the
effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression
rate without compromising performance.
REFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG
under different compression rates in the continual pre-training task. We observe a performance regression as
the compression rate increases; however, even at a compression rate of32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.
Different combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B",0.99,"The quote is an almost verbatim extraction from the source text. The only minor difference is the presence of a space between 'of' and '64' in the quote, which is absent in the source text ('of64'). This is a negligible formatting difference that does not alter the content or meaning."
30.0,Dense RAG,True,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1","The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",45.76271186440678,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted from the source text. It begins with 'Instead of using tokens...' and ends with '...directly into the decoder.'"
31.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted from the source text. It corresponds to the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
32.0,Dense RAG,True,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",75.37993920972644,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'Table 6 shows that...' and ending with '...in both TTFT and throughput.' perfectly."
33.0,Dense RAG,True,"Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves 16.53× acceleration in TTFT with cache and 8.59× without cache.",filename: 2509.01092v2.pdf page 15,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",84.80565371024736,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is a near-verbatim extraction from the source text. All the words and numerical values are identical. The only differences are minor variations in spacing and line breaks (e.g., 'of16384(' in source vs 'of 16384 (' in quote, 'REFRAG\nachieves' vs 'REFRAG achieves', '16 .53×' vs '16.53×', 'and8.59×' vs 'and 8.59×'). These are common formatting differences that do not alter the meaning or content."
34.0,Dense RAG,False,No quote provided,,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
35.0,Dense RAG,True,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",93.02325581395348,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",1.0,"The quote is an exact, verbatim extraction from the source text. Every character matches perfectly."
36.0,Dense RAG,True,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2","A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",46.97986577181208,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",0.05,"The quote is not present in the provided source text. While the source text discusses 'RL-based selective compression' and its performance, the specific sentence 'Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.' is entirely absent. The quote appears to be a hallucination or an extraction from a different part of the document, as it introduces new information (RL policy training, deferring discussion to section 2) not found in the given text."
37.0,Dense RAG,True,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",97.79735682819384,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text. The only difference is the addition of spaces around the numbers '16' and '9.3%', which is a trivial formatting change and does not alter the content or meaning."
38.0,Dense RAG,False,"Not applicable, as no supporting evidence was found in the context.",Not applicable.,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,46.26865671641791,"3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use
the stricter version and hence the reported numbers are lower in general.
8",0.0,"The quote 'Not applicable, as no supporting evidence was found in the context.' is completely unrelated to the provided source text. It is not present in the source text in any form, verbatim or near-verbatim. The quote appears to be a meta-commentary rather than an extraction from the source."
39.0,Dense RAG,True,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,95.54140127388536,"accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in
table 7.
Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl
dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We
use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.
(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.
Result analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,
varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.
This is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is
the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We",1.0,"The quote is a verbatim extraction from the source text. It starts exactly where the sentence begins after 'Retrieverandretrievalcorpus.' and continues word-for-word until the ellipsis, which correctly indicates the sentence continues in the source."
40.0,Dense RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.1,"The quote contains specific numerical details ('8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement') that are not present anywhere in the provided source text. While the source text discusses REFRAG and LLaMA, 'equal latency', and '16 tasks', it does not provide the specific results or passage counts mentioned in the quote. Therefore, the quote is not an accurate, verbatim, or near-verbatim extraction; it introduces new, unsupported information."
41.0,Dense RAG,True,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1","REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",98.989898989899,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
42.0,Dense RAG,True,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5","Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",63.41463414634146,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
43.0,Dense RAG,True,REFRAGRL uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4","Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",71.11111111111111,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim extraction of the first sentence from the provided source text."
44.0,Dense RAG,False,"Notably, REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4","In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",46.103896103896105,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
45.0,Dense RAG,True,"This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the performance of REFRAG8.",filename: 2509.01092v2.pdf page 6,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,52.151898734177216,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.05,"The quote is almost entirely fabricated. It introduces new information not present in the source text, such as 'This finding is particularly surprising', 'REFRAG16 achieves a compression rate of 8', 'without recomputing chunk embeddings', and the specific claim that 'REFRAG16 ... surpasses the performance of REFRAG8'. The source text actually groups REFRAG8 and REFRAG16 together as both performing well ('REFRAG8 and REFRAG16 consistently outperform other baselines', 'both REFRAG8 and REFRAG16 maintain superior performance'). There is no mention of REFRAG16 surpassing REFRAG8, nor the specific compression rate or recomputing chunk embeddings details for REFRAG16."
46.0,Dense RAG,False,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0","Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",45.58823529411765,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",0.75,"The quote accurately summarizes two key points from the source text: 1) the context in RAG comes from retrieved chunks ('retrieval process in RAG has already pre-processed the chunks of the contexts'), and 2) a significant portion of these chunks are irrelevant ('most context chunks during decoding are unrelated'). However, the quote itself is not a verbatim or near-verbatim extraction of a single sentence from the source text; it synthesizes information from different parts of the problem description."
47.0,Dense RAG,True,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",100.0,"The data transfer latency to high-bandwidth memory is2n+4dlb(s+o)
m .
Throughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given
by:
Throughput= bo
TTFT+DL
where DL is the data latency.
Before After
KV cache memory4dlb(s+o) 4dlb
",0.0,"The quote is not present in the provided source text. The source text mentions TTFT and TTIT in a table comparing values, but it does not describe their latency growth as quadratically or linearly with prompt length, nor does it include the citation (Liu et al., 2025). The quote is entirely unsupported by the given source."
48.0,Dense RAG,True,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19","To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",46.59090909090909,"Stage 1 Stage 3 Stage 5 Stage 7 Stage 9
Training Stage
0
50
100
Percentage
Context
1 × k
2 × k
4 × k
8 × k
16 × k
32 × k
64 × k
128 × k
256 × k
Figure 6The data mixture in curriculum learning during the training.
Factor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation
1×81333 445 148 49 16 6 2 1 0 2000
2×8333 298 267 238 213 191 171 153 137 2000
4×883 102 126 156 193 238 293 362 447 2000
8×820 35 61 106 185 324 565 985 1719 4000
16×85 11 23 48 103 220 468 997 2125 4000
32×81 3 7 19 50 133 353 939 2496 4000
64×81 3 9 25 73 212 618 1802 5259 8000
128×81 3 9 25 73 212 618 1802 5259 8000
256×81 3 9 25 73 212 618 1802 5259 8000
Table 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we
have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).",1.0,"The quote is an exact, verbatim extraction from the source text. The only minor difference is the removal of a line break present in the source text ('we\nhave' vs 'we have'), which is a standard text normalization and does not alter the content or meaning."
49.0,Dense RAG,True,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",filename: 2509.01092v2.pdf page 7,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim extraction of two consecutive sentences from the provided source text. There are no alterations or omissions."
50.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source."
51.0,Dense RAG,True,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",filename: 2509.01092v2.pdf page 1,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",100.0,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches the sentence starting with 'Due to diversity...' under point '3)Unusually Structured and Sparse Attention'."
52.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with '3) It reduces attention computation complexity...' perfectly."
53.0,Dense RAG,True,"REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: ... 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",46.774193548387096,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is a verbatim extraction from the source text. The '...' accurately indicates the omission of the first two advantages from the list, while the surrounding text is an exact match."
54.0,Dense RAG,True,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",83.04093567251462,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,"The quote accurately conveys the information from the source text regarding acceleration for longer context lengths. However, it is not a verbatim extraction. Minor differences include: 'For' changed to 'With', 'context length' changed to 'context lengths', 'acceleration reaches' changed to 'we are able to achieve', 'tok2×' corrected to 'k²×' (assuming a typo in the source), and 'for both metrics' clarified to 'in both TTFT and throughput'. Despite these changes, the core meaning and numerical facts are perfectly preserved, and the introduction of 'we' is consistent with the overall context of the paper (e.g., 'our method achieves' in the preceding sentence)."
55.0,Dense RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",100.0,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It can be found in the paragraph describing the 'Reconstruction task'."
56.0,Dense RAG,True,"Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported
as average of Arxiv and Book domain.
...
REFRAGw/o curriculum 3.719 3.098 2.272 1.599
REFRAGwith curriculum 0.669 0.451 0.230 0.135","2509.01092v2.pdf, Page: 25","Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",41.05691056910569,"Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported
as average of Arxiv and Book domain.
P16 P32 P128 P2048↓
LLaMA-Full Context1.397 0.734 0.203 0.021
LLaMA-No Context3.483 2.981 2.249 1.590
REFRAGw/o curriculum 3.719 3.098 2.272 1.599
REFRAGwith curriculum 0.669 0.451 0.230 0.135
Table 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.
Log-Perplexity is reported as average of Arxiv and Book domain.
P16 P32 P128 P2048↓
LLaMA-Full Context1.448 1.458 1.464 1.449
LLaMA-No Context3.483 2.981 2.249 1.590
REFRAGw/o reconstruction 3.272 2.789 2.119 1.544
REFRAGwith reconstruction 2.017 1.837 1.632 1.453
Table 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and
selective compression (i.e.,REFRAG16+RL).
Arxiv Book PG19 ProofPile
Compression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓
Context Length=2048",0.95,"The quote accurately extracts the beginning and end of 'Table 11' from the source text. The '...' correctly indicates an omission of intermediate lines ('P16 P32 P128 P2048↓', 'LLaMA-Full Context...', 'LLaMA-No Context...') from the table. The extracted parts are verbatim, and the omission is properly marked, making it a near-verbatim extraction."
57.0,Dense RAG,False,We compare the perplexity of xs+1:s+o using different selection policy under different p.,"2509.01092v2.pdf, Page: 4","Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",47.19101123595506,"selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate
1.02
1.03
1.04
1.05
1.06
1.07
Log-perplexity
Arxiv
4 8 16
Compression rate
1.82
1.83
1.84
1.85
1.86
Log-perplexity
Book
4 8 16
Compression rate
1.91
1.92
1.93
1.94
1.95
Log-perplexity
PG19
4 8 16
Compression rate
0.87
0.88
0.89
0.90
0.91
0.92
Log-perplexity
ProofPile
RL Perplexity-desc Perplexity-asc Random
Figure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages
of chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),
Perplexity-asc(heuristic: higher perplexity), andRandom(random selection).
6",0.35,"The quote is a summary or paraphrase of information found in the source text, particularly within the caption of Figure 3. It is not a verbatim or near-verbatim extraction of a single sentence or phrase. While the core concepts (comparing perplexity, xs+1:s+o, selection policies, and varying conditions) are derivable from the source, the phrasing and sentence structure of the quote are not directly present. For instance, 'the perplexity of xs+1:s+o' combines elements from 'Log-Perplexity on xs+1:s+o' and 'We compare', 'using different selection policy' rephrases 'three selection methods', and 'under different p.' is a significant abbreviation and rephrasing of 'under varying compression rates by selectively compressing different percentages of chunks.' Therefore, it does not meet the criteria of a verbatim or near-verbatim extraction."
58.0,Dense RAG,True,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",98.65771812080536,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'Table 13 demonstrates...' perfectly."
59.0,Dense RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",94.82758620689656,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote contains specific numerical data and comparisons (e.g., '8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement', '16 RAG tasks') that are not present anywhere in the provided source text. The source text discusses REFRAG and LLaMA performance and latency in general terms but does not provide these specific figures or the exact statement."
60.0,Dense RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the provided source text."
61.0,Dense RAG,True,"the context is the dominating part of the input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k .","2509.01092v2.pdf, Page: 2","This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",46.37681159420289,"Donald Trump is 
the President of 
the United States. 
He assumed office 
on January 20, 
2025, making
him the 47th 
President of the 
United States.
 Encoder  Encoder  Encoder
Context Text
Decoder-only Foundation ModelSequence
Precomputable
Light-weight
EncoderWho is the President of USA?
Decoder Tokenizer & 
Embedding
Decoder Input Text
Token Embedding
Chunk 
Embedding
Light-weight RL-trained chunk expansion policy
Vector DBQuery 
Encoder
Figure 1The main design ofREFRAG. The input context is chunked and processed by the light-weight encoder to
produce chunk embeddings, which are precomputable for efficient reuse. A light-weight RL policy decide few chunks to
expand. These chunk embeddings along with the token embeddings of the question input are fed to the decoder.
token embedding for tokenxi. In real applications (e.g., RAG), the context is the dominating part of the
input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This",1.0,"The quote is an exact, verbatim extraction from the source text. It can be found in the last paragraph of the source text, starting after 'In real applications (e.g., RAG),' and ending before 'This'."
62.0,Dense RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",95.73459715639812,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction of two consecutive sentences from the source text."
63.0,Dense RAG,True,"The effective compression rate k / (1−p+kp) decreases when fewer chunks are compressed (i.e.,p increases).",filename: 2509.01092v2.pdf page 4,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",95.23809523809524,"chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection
policy under differentp. The perplexity-based selection is an heuristic based selection which compresses
chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured
by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can
therefore be compressed with minimal information loss. Ideally, this approach should outperform random
selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves
superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while",0.95,"The quote accurately extracts the sentence. The only differences are the explicit addition of a division symbol '/' and parentheses '()' to clarify a mathematical expression that was implicitly represented by a line break and a potential missing space in the source text (`ratek` vs `rate k`). These changes preserve the exact mathematical and semantic meaning, making it a near-verbatim extraction."
64.0,Dense RAG,False, ,,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",0.0,,0.0,"The source text is an empty string, containing no characters. The quote, however, contains a single space character. Therefore, the quote is not present in the source text and cannot be considered an extraction from it."
65.0,Dense RAG,True,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1","Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",47.53363228699552,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact extraction from the source text. The minor differences ('REFRAGleverages' vs 'REFRAG leverages' and 'represen-tations' vs 'representations') are due to formatting (missing space and hyphenation for line break) in the source text, not a change in content or wording."
66.0,Dense RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text. It starts with 'However, CEPE is limited...' and ends with '...or summarization.', matching the source precisely."
67.0,Dense RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.1,"The source text discusses the comparison between REFRAG and LLaMA under 'equal latency' and mentions '16 tasks', stating that 'REFRAG consistently outperform LLaMA'. However, the quote introduces specific numerical details: '(8 passages for REFRAG vs. 1 for LLaMA)' and 'attains a 1.22% average improvement'. These crucial quantitative details are not present anywhere in the provided source text. Therefore, the quote is not a verbatim or near-verbatim extraction and contains significant specific information not supported by the source."
68.0,Dense RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",100.0,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text, with only a minor formatting difference (a space added between 'a' and '4k' in 'a4k context window'). This does not change the meaning or content, making it a verbatim extraction."
69.0,Dense RAG,False,No quote provided,,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
70.0,Dense RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.95,"The quote is a near-verbatim extraction of a sentence from the source text. The only differences are minor spacing variations around 'LLaMA256' and 'REFRAG8' (e.g., 'reference,LLaMA256' in source vs. 'reference, LLaMA256' in quote; 'inREFRAG8' vs. 'in REFRAG8'). The content, words, and punctuation are otherwise identical."
1.0,BM25 RAG,True,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts, REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.",filename: 2509.01092v2.pdf page 10,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",46.58634538152611,"7 Conclusion
In this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.
By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG
compresses, senses, and expands context representations to significantly reduce both memory usage and
inference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,
including RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG
achieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in
perplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-
based systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG
provides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive
applications.
8 Acknowledgements",0.95,"The quote is almost an exact sentence from the source text. The only difference is a minor formatting change: the source text has 'contexts,REFRAG' (no space after the comma), while the quote has 'contexts, REFRAG' (a space after the comma). This is a very minor deviation and does not alter the meaning or content of the quote."
2.0,BM25 RAG,False,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly).","2509.01092v2.pdf, Page: 5","The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",48.93617021276596,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",0.0,"The quote is not present in the provided source text. While the source text discusses 'reconstruction task' and 'curriculum learning', the specific phrase 'performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly)' is entirely absent. It appears to be a hallucination or an extraction from a different, unprovided text."
3.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim sentence extracted directly from the provided source text."
4.0,BM25 RAG,True,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.",filename: 2509.01092v2.pdf page 7,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",100.0,"2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we
retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.
Result analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.
Our model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three
datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of
LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting
in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on
the sameLLaMA model without extending its effective positional encoding, maintains robust performance
3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'This improvement is attributable...' and ending with '...subsequent questions.' perfectly."
5.0,BM25 RAG,True,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.",filename: 2509.01092v2.pdf page 1,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",47.96747967479674,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",0.6,"The latter part of the quote, 'being uninformative and reused across multiple inferences,' is a verbatim match from the beginning of the source text. However, the initial part of the quote, 'RAG contexts often contain sparse information, with many retrieved passages,' is not a verbatim or near-verbatim extraction. While the source text discusses 'Sparse Attention' and 'unrelated' context chunks (supporting the idea of sparse information) and implies 'retrieved passages' are uninformative, the specific phrasing used in the first part of the quote is a synthesis and rephrasing of these concepts, rather than a direct extraction. Therefore, the quote is a combination of a verbatim phrase and a supported but rephrased summary, failing to meet the 'verbatim (or near-verbatim) extraction' criteria for its entirety."
6.0,BM25 RAG,True,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4","As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",46.01769911504425,"chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection
policy under differentp. The perplexity-based selection is an heuristic based selection which compresses
chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured
by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can
therefore be compressed with minimal information loss. Ideally, this approach should outperform random
selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves
superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while",0.05,"The quote starts with 'The reconstruction task, while' which is present at the very end of the source text. However, the rest of the quote, 'intuitive, is particularly challenging when multiple chunks must be reconstructed.', is not present in the provided source text. The source text cuts off abruptly after 'while', indicating that the majority of the quote is not supported by the given text."
7.0,BM25 RAG,True,"...a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1","This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",97.75280898876404,"C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain
uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at
arbitrary positions, which is essential for the subsequent RL policy learning.
We sequentially pickT′ chunk indicesl = {lj}T′
j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′
j=1) =
{E1, . . . , EL}, with Ei =e cnk
i if i /∈ {lj}T′
j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′
j=1
16",0.0,"The provided quote is not present in the source text. While the source text mentions 'RL policy learning', the specific description of the policy's function ('...that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice') is entirely absent."
8.0,BM25 RAG,True,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",82.08092485549133,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.85,"The quote accurately conveys the information from the source text regarding k2× acceleration for longer context lengths in both TTFT and throughput. The source states: 'For longer context length, acceleration reaches up tok2× for both metrics.' While the meaning is fully preserved and supported, the quote is not a verbatim or near-verbatim extraction. It rephrases several parts of the original sentence, such as 'For longer context length' to 'With longer context lengths', 'acceleration reaches' to 'we are able to achieve acceleration', and 'for both metrics' to 'in both TTFT and throughput'. These changes go beyond minor grammatical adjustments, making it a rephrasing rather than a direct extraction."
9.0,BM25 RAG,True,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",filename: 2509.01092v2.pdf page 7,"Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",98.96907216494846,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
10.0,BM25 RAG,True,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",98.09523809523807,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The provided quote, 'In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance,' is not present anywhere in the source text. The source text mentions 'compression rate of8' but does not discuss a compression rate of 64 or its effects."
11.0,BM25 RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",100.0,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
12.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted from the source text. It appears as the third advantage listed: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
13.0,BM25 RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",96.33802816901408,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,The quote is nearly verbatim from the source text. The only difference is the rendering of 'tok2×' in the source text as 'k²×' in the quote. This is a minor formatting correction that accurately represents the likely intended mathematical notation (k-squared) and does not alter the meaning of the statement.
14.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...",filename: 2509.01092v2.pdf page 3,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",86.9431643625192,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",0.9,"The first two sentences of the quote are verbatim extractions from the source text. The third sentence is also verbatim up to 'preserving the decoder’s'. However, the phrase 'autoregressive property' at the very end of the quote is not present in the provided source text, which ends abruptly after 'preserving the decoder’s'. This constitutes an unsupported addition to the quote."
15.0,BM25 RAG,True,"In this task, wefreeze the decoder model and only train the encoder and projection layer.","Source Reference: 2509.01092v2.pdf, Page: 3","Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",97.75280898876404,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",1.0,"The quote is an exact, verbatim extraction of a complete sentence found within the 'Source Text'."
16.0,BM25 RAG,True,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3","As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",90.64748201438849,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text. There are no alterations, additions, or omissions."
17.0,BM25 RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.1,"The quote contains multiple significant inaccuracies. It refers to 'Table 13', whereas the source text only mentions 'Table 1' and 'Table 2'. More importantly, the quote claims that 'REFRAG16 ... consistently outperforms REFRAG8', which is not supported by the source text. The source text states that 'REFRAG8 and REFRAG16 consistently outperform other baselines' and that 'both REFRAG8 and REFRAG16 maintain superior performance', but it does not claim that REFRAG16 consistently outperforms REFRAG8. While 'RL-based selective compression' is mentioned in the source, its application in the quote's context is misleading due to the other inaccuracies."
18.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","Source Reference: 2509.01092v2.pdf, Page: 9","CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",99.62406015037594,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the text segment starting with 'However, CEPE is limited...' and ending with '...increased decoding latency.' perfectly."
19.0,BM25 RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.1,"The source text discusses REFRAG and LLaMA performance under 'equivalent latency constraints' and mentions '16 tasks', indicating REFRAG outperforms LLaMA. However, the specific numerical details provided in the quote, such as '(8 passages for REFRAG vs. 1 for LLaMA)' and 'achieves a 1.93% average gain', are not present in the provided source text. The source text refers to Figure 4 for such comparisons, but the quote itself is not a verbatim or near-verbatim extraction from the given text."
20.0,BM25 RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,"By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",46.29213483146067,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact, verbatim sentence extracted from the source text. The only minor difference is the spacing around '4k' ('a4k' in source vs 'a 4k' in quote), which does not alter the content or meaning."
21.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source."
22.0,BM25 RAG,True,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",83.04093567251462,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.88,"The quote accurately conveys the core information from a specific sentence in the source text: 'For longer context length, acceleration reaches up tok2× for both metrics.' However, it is not a verbatim or near-verbatim extraction. Several words and the sentence structure have been rephrased (e.g., 'For' to 'With', 'context length' to 'context lengths', 'acceleration reaches' to 'we are able to achieve', and 'for both metrics' to 'in both TTFT and throughput'). While the meaning is perfectly preserved and fully supported by the source, the wording is a paraphrase rather than a direct extraction."
23.0,BM25 RAG,True,"REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1","Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",98.83720930232558,"the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on
computationally intensive token embeddings, condensing most chunks for the query in RAG settings.
We provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training
and many real word long-context applications including RAG, multi-turn conversation with RAG and long
document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity",1.0,"The quote is a verbatim extraction of two consecutive sentences from the source text. There are no alterations, additions, or omissions within the quoted content."
24.0,BM25 RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",100.0,"foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu
et al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into
L := s
k number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then
processes all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding
is then projected with a projection layerϕ to match the size of the token embedding of the decoder model,
ecnk
i = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token
embeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk
1 , . . . ,ecnk
L })wheree i is the
2",0.0,"The provided quote is not present in the source text. The source text describes a procedural process of chunking context, processing chunks with an encoder, projecting chunk embeddings, and feeding them to a decoder. The quote, however, discusses 'objectives' related to aligning the encoder and projection layer for specific functions (compressing tokens, mapping embeddings), which is a conceptual explanation not found in the given text."
25.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3","Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches perfectly without any alterations."
26.0,BM25 RAG,True,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...",filename: 2509.01092v2.pdf page 2,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",98.59943977591035,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.9,"The quote accurately extracts most of the key information, including the context length (16384, mid-to-long), the system (REFRAG), and the acceleration figures (16.53x TTFT with cache, 8.59x without cache), and references Figure 2. There are minor rephrasing differences (e.g., 'Empirically, as shown in figure 2' vs 'Figure 2 shows the empirical measurement', 'with a context length of' vs 'Under the context length of', 'TTFT acceleration' vs 'acceleration in TTFT'). However, the phrase 'with k = 16' is an addition that is not explicitly stated in the provided source text in relation to the 16384 context length and the specific acceleration values. The source mentions 'k' as a general acceleration factor but does not specify 'k=16' for this particular measurement, making this part not verbatim or directly supported by the provided text."
27.0,BM25 RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",95.6896551724138,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote contains specific numerical claims and comparisons (e.g., '8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement', 'across 16 RAG tasks') that are not present anywhere in the provided source text. While the source text discusses REFRAG and LLaMA performance relative to latency, it does not provide the specific data or phrasing found in the quote."
28.0,BM25 RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim sentence extracted from the source text. The only differences are minor whitespace variations (e.g., space after comma before 'LLaMA256', space before parenthesis for 'REFRAG8') which do not alter the content or meaning."
29.0,BM25 RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",100.0,"chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the
effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression
rate without compromising performance.
REFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG
under different compression rates in the continual pre-training task. We observe a performance regression as
the compression rate increases; however, even at a compression rate of32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.
Different combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B",1.0,"The quote is an exact, verbatim extraction from the source text. The only minor difference is the spacing around '64' (source has 'of64appears' while quote has 'of 64 appears'), which is a negligible formatting variation and does not alter the content or meaning."
30.0,BM25 RAG,True,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",99.1150442477876,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
31.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
32.0,BM25 RAG,True,"Table 6 shows that with short context length s we are able to achieve k× acceleration in TTFT and up to k× acceleration in throughput. With longer context lengths, we are able to achieve up to k2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",81.89655172413794,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is almost verbatim from the source text. The only differences are minor spacing issues where the source text has 'achievek×', 'up tok×', and 'up tok2×' without a space, while the quote includes a space: 'achieve k×', 'up to k×', and 'up to k2×'. These are negligible formatting differences and do not alter the meaning or content."
33.0,BM25 RAG,True,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1...",filename: 2509.01092v2.pdf page 2,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",85.44303797468355,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.8,"The quote extracts core numerical data accurately from the source text regarding REFRAG's acceleration. However, it is not a verbatim or near-verbatim extraction due to several modifications:
1.  **Rephrasing and Omission**: The opening phrase 'Empirically, as shown in figure 2,' is a rephrasing and significant summarization of the source's 'Figure 2 shows the empirical measurement of the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration in inference (Yen et al., 2024).' It omits the comparison with CEPE.
2.  **Interpretive Addition**: The quote adds 'with k = 16' before 'achieves'. While the source mentions 'k× acceleration' generally and the measured acceleration is '16.53×', the source text does not explicitly state 'k=16' in relation to this specific measurement. This is an interpretation or an assertion not directly present in the provided text.
3.  **Minor Stylistic Changes**: 'acceleration in TTFT' is reordered to 'TTFT acceleration'. 'i.e.,' is changed to a comma.
4.  **Added Elements**: The quote includes a footnote indicator '1' and an ellipsis '...' at the end, which are not present in the source text."
34.0,BM25 RAG,True,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","2509.01092v2.pdf, Page: 3","In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",56.57370517928286,"on empirical evaluation is in section A.
3 Methodology
To align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction
tasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number
of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.
To further enhance performance, we introduce selective compression via RL. After aligning the encoder and
decoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream
tasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.
During CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in
predicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for
next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It appears in the paragraph starting with 'During CPT, we input the firsts tokens...'"
35.0,BM25 RAG,True,"For the reconstruction task, training begins with reconstructing a single chunk... Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",89.36170212765957,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",0.95,"The quote accurately extracts two verbatim phrases from the source text, separated by an ellipsis (...) which correctly indicates omitted text between the two phrases. The extracted portions themselves are exact."
36.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3","A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It appears in the source text as: 'A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.'"
37.0,BM25 RAG,True,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4","With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",97.79735682819384,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact, verbatim sentence extracted from the source text. The only minor difference is the addition of spaces around the numbers '16' and '9.3%' which are implicitly present in the source text's phrasing (e.g., 'rate of16' vs 'rate of 16'). This does not alter the content or meaning."
38.0,BM25 RAG,False, ,,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,0.0,,0.0,"The source text is an empty string, containing no characters. The quote, however, contains a single space character. Therefore, the quote is not present in the source text and cannot be considered a verbatim or near-verbatim extraction."
39.0,BM25 RAG,True,We follow the work of Linetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,93.5064935064935,"accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in
table 7.
Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl
dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We
use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.
(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.
Result analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,
varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.
This is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is
the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We",0.95,The quote is a near-verbatim extraction from the source text. The only minor deviation is the addition of a space between 'We' and 'follow' at the beginning of the first sentence ('Wefollow' in the source vs 'We follow' in the quote). The quote correctly uses an ellipsis to indicate that the last sentence was truncated from the original source text.
40.0,BM25 RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",94.82758620689656,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote makes several specific claims: '8 passages for REFRAG vs. 1 for LLaMA' at equal latency, '1.22% average improvement', and 'across 16 RAG tasks'. None of these specific numerical details or conditions are present in the provided source text. While the source text discusses REFRAG and LLaMA performance, and mentions REFRAG outperforming LLaMA under the same latency, it does not quantify this with the numbers given in the quote. Therefore, the quote is not supported by the source text and contains hallucinated specific information."
41.0,BM25 RAG,True,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9","REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",48.52941176470589,"the KV cache to reduce memory requirements for long-context applications. However, this approach only
decreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the
model from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,
generating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive
nature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding
latency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings
for prediction, similar to our method. However, their sequential compression process results in high latency
when the summary vector is not cached, and their approach only supports applications where the summary
token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
42.0,BM25 RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",99.54954954954955,"foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu
et al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into
L := s
k number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then
processes all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding
is then projected with a projection layerϕ to match the size of the token embedding of the decoder model,
ecnk
i = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token
embeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk
1 , . . . ,ecnk
L })wheree i is the
2",0.0,"The provided quote is completely absent from the source text. The source text describes a technical process involving models, chunking, and embeddings, but it does not contain any statement about 'main objectives' or aligning an encoder and projection layer in the manner described in the quote."
43.0,BM25 RAG,True,We employ RL to train a policy that optimally determines which segments to compress.,"Source Reference: 2509.01092v2.pdf, Page: 15","Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",51.19047619047619,"C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain
uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at
arbitrary positions, which is essential for the subsequent RL policy learning.
We sequentially pickT′ chunk indicesl = {lj}T′
j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′
j=1) =
{E1, . . . , EL}, with Ei =e cnk
i if i /∈ {lj}T′
j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′
j=1
16",0.15,"The source text mentions 'subsequent RL policy learning' in the context of compression, which provides a thematic connection to the quote. However, the quote states a specific objective for this RL policy ('optimally determines which segments to compress') which is not explicitly detailed or supported in the provided text snippet. The text only states that RL policy learning is 'essential' and 'subsequent,' but does not define its function as described in the quote. Therefore, it is not a verbatim or near-verbatim extraction, nor is the specific claim fully supported by the provided text."
44.0,BM25 RAG,False,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9","In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",47.77777777777777,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a sentence found within the provided source text."
45.0,BM25 RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.1,"The quote contains multiple significant inaccuracies and fabrications not present in the source text. 
1. The source mentions 'Table 1' and 'Table 2', but not 'Table 13'.
2. The source states 'REFRAG8 and REFRAG16 consistently outperform other baselines', but it does NOT state that 'REFRAG16 consistently outperforms REFRAG8'. In fact, the source often groups them together as performing well.
3. The phrase 'across different datasets' is not explicitly mentioned in the source. While 'across nearly all settings' and 'at longer contexts' are present, the specific claim about datasets is not.

Due to these major discrepancies, the quote is largely unsupported and contains fabricated information."
46.0,BM25 RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0","Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",50.12919896640827,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches character for character."
47.0,BM25 RAG,True,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",100.0,"The data transfer latency to high-bandwidth memory is2n+4dlb(s+o)
m .
Throughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given
by:
Throughput= bo
TTFT+DL
where DL is the data latency.
Before After
KV cache memory4dlb(s+o) 4dlb
",0.0,"The quote is not present anywhere in the provided source text. The source text discusses TTFT and TTIT in the context of formulas and a comparison table, but it does not contain the descriptive sentence about their quadratic or linear growth with prompt length, nor does it mention '(Liu et al., 2025)'."
48.0,BM25 RAG,True,Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,filename: 2509.01092v2.pdf page 21,"To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",66.12903225806451,"Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the
success of reconstruction task.
Ablation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and
without continuing from reconstruction task.
22",1.0,"The quote is an exact, verbatim extraction of a sentence found within the source text."
49.0,BM25 RAG,True,"under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","Source Reference: 2509.01092v2.pdf, Page: 7","The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.95,"The quote is a near-verbatim extraction from the source text. It accurately reproduces the content from 'under the same number of retrieved passages...' to '...retrieved passages are less relevant.' However, it omits the introductory phrase 'The result demonstrates that ' which precedes the quoted text in the source."
50.0,BM25 RAG,False,"This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence.","2509.01092v2.pdf, Page: 2","It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",44.628099173553714,"input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k . This
architectural design leads to significant reductions in both latency and memory usage, primarily due to the
shortened input sequence. Additionally, an RL policy is trained to do selective compression to further improve
the performance which we will defer the discussion to section 2. Next, we analyze the system performance
gains achieved with a compression rate ofk.
103 104
# Input T okens
0
10
20
30
Acceleration
TTFT Acceleration
103 104
# Input T okens
1.0
1.5
2.0
2.5
3.0
Acceleration
TTIT Acceleration
103 104
# Input T okens
2
4
6Acceleration
Throughput Acceleration
REFRAG (Cached) REFRAG (Not Cached) CEPE
Figure 2Empirical verification of inference acceleration ofREFRAGwithk= 16.
Latency and throughput improvement.We evaluate three metrics: TTFT, the latency to generate the first
token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated",1.0,"The quote is an exact, verbatim sentence extracted directly from the provided source text. It appears in the first paragraph of the source text."
51.0,BM25 RAG,True,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",filename: 2509.01092v2.pdf page 1,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",100.0,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches the sentence starting with 'Due to diversity...' under point '3)Unusually Structured and Sparse Attention'."
52.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
53.0,BM25 RAG,True,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk
i = ϕ(ci).","2509.01092v2.pdf, Page: 1","The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",98.6013986013986,"foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu
et al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into
L := s
k number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then
processes all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding
is then projected with a projection layerϕ to match the size of the token embedding of the decoder model,
ecnk
i = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token
embeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk
1 , . . . ,ecnk
L })wheree i is the
2",1.0,"The quote is an exact, verbatim extraction from the source text, including the mathematical notation. The only difference is the line break in the source text which is not present in the quote, but this does not affect the content or meaning."
54.0,BM25 RAG,True,"For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",98.66666666666669,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is a very close paraphrase of the sentence 'With longer context lengths, we are able to achieve up to k2× acceleration in both TTFT and throughput.' from the source text. It accurately conveys the same information regarding acceleration for longer context lengths and for both metrics (TTFT and throughput), using slightly different phrasing ('For longer context length' instead of 'With longer context lengths', 'acceleration reaches' instead of 'we are able to achieve acceleration', and 'for both metrics' instead of 'in both TTFT and throughput'). The core meaning and numerical value (k²×) are perfectly preserved."
55.0,BM25 RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",100.0,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
56.0,BM25 RAG,True,The results indicate that curriculum learning is essential for the success of the reconstruction task.,filename: 2509.01092v2.pdf page 5,"Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",70.34482758620689,"Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the
success of reconstruction task.
Ablation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and
without continuing from reconstruction task.
22",0.7,"The quote accurately conveys the core meaning of the first sentence in the source text. However, it is a significant rephrasing rather than a verbatim or near-verbatim extraction. For instance, 'Table 11 shows' is changed to 'The results indicate that', and 'the necessity of' is rephrased as 'is essential for'. While semantically equivalent, the lexical and structural changes are too substantial to be considered near-verbatim."
57.0,BM25 RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. There are no changes, additions, or omissions."
58.0,BM25 RAG,True,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5","This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",98.65771812080536,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'Table 13 demonstrates...' perfectly."
59.0,BM25 RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",94.82758620689656,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote contains specific numerical data (8 passages for REFRAG vs. 1 for LLaMA, 1.22% average improvement, 16 RAG tasks) that are not present anywhere in the provided source text. While the source text discusses REFRAG, LLaMA, and performance vs. latency, it does not provide these exact figures or statements. Therefore, the quote is completely unsupported by the source text."
60.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9","CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text. It starts with 'However, CEPE is limited...' and ends with '...or summarization.', matching the source precisely."
61.0,BM25 RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
62.0,BM25 RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",95.73459715639812,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction of a sentence from the source text."
63.0,BM25 RAG,True,"The effective compression ratek
1−p+kp decreases when","2509.01092v2.pdf, Page: 4","Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",96.22641509433964,"chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection
policy under differentp. The perplexity-based selection is an heuristic based selection which compresses
chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured
by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can
therefore be compressed with minimal information loss. Ideally, this approach should outperform random
selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves
superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while",1.0,"The quote is an exact, verbatim extraction from the source text, including the line break."
64.0,BM25 RAG,False,No quote provided,,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
65.0,BM25 RAG,True,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"2509.01092v2.pdf, Page: 5","Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",51.93370165745856,"Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the
success of reconstruction task.
Ablation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and
without continuing from reconstruction task.
22",0.95,"The quote accurately captures the core meaning of the first sentence in the source text: 'Table 11 shows the necessity of curriculum learning to the success of reconstruction task.' While not verbatim, it uses synonymous phrasing ('essential' for 'necessity') and accurately generalizes the source of the information ('The results indicate that' for 'Table 11 shows'). The semantic content is perfectly preserved, making it a very faithful, near-verbatim extraction."
66.0,BM25 RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence found within the provided source text."
67.0,BM25 RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.1,"The quote is not a verbatim or near-verbatim extraction from the source text. While the source text discusses REFRAG outperforming LLaMA under equal latency constraints and mentions 16 tasks, it does not contain the specific details about '8 passages for REFRAG vs. 1 for LLaMA' or the '1.22% average improvement'. These specific numerical claims are not supported by the provided source text."
68.0,BM25 RAG,False,"REFRAG: Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o.","Source Reference: 2509.01092v2.pdf, Page: 17","Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",44.66666666666666,"feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o. We useREFRAGk to denote
our model with compression rate ofk. We useREFRAGRL to refer to the model with selective compression
using our RL policy.
B.2 Additional Details on Hyperparameters and Experimental Settings for CPT
Hyperparameters.For reconstruction stage, we use a peak learning rate of2e− 4since we only train the
encoder model. For the next paragraph prediction we use a peak learning rate of5e− 5since we train all the
18",0.1,"Only a small portion of the quote, specifically 'feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o.', is present in the source text. The initial parts of the quote, 'REFRAG: Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder,', are completely absent from the provided source text. This indicates significant hallucination or information not supported by the source."
69.0,BM25 RAG,False,No quote provided,,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
70.0,BM25 RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.95,"The quote is nearly verbatim from the source text. The only differences are the addition of spaces after commas or between words where the source text had no space (e.g., 'reference,LLaMA256' in source vs. 'reference, LLaMA256' in quote; 'inREFRAG8' vs. 'in REFRAG8'; 'surpassesLLaMA256' vs. 'surpasses LLaMA256'). These are minor formatting changes that do not alter the meaning or content."
1.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
2.0,Dense RAG,False,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5","The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",61.36363636363637,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
3.0,Dense RAG,False,"Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves 16.53× acceleration in TTFT with cache and 8.59× without cache. Both higher than CEPE (i.e., 2.01× and 1.04× acceleration respectively) while having better model performance (see table 1).","2509.01092v2.pdf, Page: 15","CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",43.30218068535826,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is an almost exact extraction from the source text. The only differences are minor formatting adjustments, such as the removal of extra spaces around numbers (e.g., '16 .53×' in the source becomes '16.53×' in the quote, and '1 .04×' becomes '1.04×') and around '16384' and 'REFRAG'. These changes do not alter the meaning or content, making it a near-verbatim extraction."
4.0,Dense RAG,True,This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.,filename: 2509.01092v2.pdf page 7,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",48.96551724137931,"accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in
table 7.
Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl
dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We
use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.
(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.
Result analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,
varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.
This is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is
the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We",0.0,"The provided quote, 'This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.', is not present anywhere in the source text. The source text discusses evaluation metrics, retriever models, retrieval corpus creation, and result analysis including context length and latency metrics, but the specific sentence quoted is entirely absent."
5.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",46.66666666666666,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'It reduces attention computation complexity...' perfectly."
7.0,Dense RAG,True,"we achieve a compression fraction of 1−p by randomly selecting T′ := pL chunks to remain uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at arbitrary positions, which is essential for the subsequent RL policy learning.","2509.01092v2.pdf, Page: 15","This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",51.570247933884296,"C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain
uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at
arbitrary positions, which is essential for the subsequent RL policy learning.
We sequentially pickT′ chunk indicesl = {lj}T′
j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′
j=1) =
{E1, . . . , EL}, with Ei =e cnk
i if i /∈ {lj}T′
j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′
j=1
16",0.95,"The quote is nearly verbatim. It omits the initial phrase 'C1, . . . , CL,' and has minor spacing differences ('of1−p' vs 'of 1−p', 'selectingT′' vs 'selecting T′') compared to the source text. The core content of the two sentences is accurately extracted."
8.0,Dense RAG,True,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",83.04093567251462,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.9,"The quote accurately conveys the core information that for longer context lengths, k²× acceleration is achieved in both TTFT and throughput. However, it is not a verbatim extraction. It rephrases 'For longer context length' to 'With longer context lengths', 'acceleration reaches' to 'we are able to achieve', and expands 'for both metrics' to 'in both TTFT and throughput'. The change from 'acceleration reaches' to 'we are able to achieve' introduces an agent ('we') not explicitly present in that specific sentence of the source, making it more than a minor stylistic change. The formatting of 'k2' to 'k²' is an accurate interpretation."
9.0,Dense RAG,True,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7","Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",98.96907216494846,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
10.0,Dense RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",100.0,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The provided quote, 'In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance,' is not found anywhere in the source text. The source text mentions 'compression' in the context of 'Llama REFRAG 8 × compression' and 'REFRAG16 achieves a compression rate of 8', but it does not discuss a compression rate of 64 or the concept of it being 'overly aggressive' or leading to 'diminished performance'."
11.0,Dense RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0","These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",100.0,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
12.0,Dense RAG,False,"attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements.",filename: 2509.01092v2.pdf page 9,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",52.8,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction from the beginning of the source text."
13.0,Dense RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",96.33802816901408,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.98,"The quote is nearly an exact sentence from the source text. The only difference is the representation of 'k²×' in the quote versus 'k2×' in the source. This is a minor formatting difference (superscript vs. plain text) that does not change the meaning or content, making it a near-verbatim extraction."
14.0,Dense RAG,True,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","Source Reference: 2509.01092v2.pdf, Page: 3","Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",83.61204013377926,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s
autoregressive property and enabling flexible placement of compression. Further discussion on sequential
selection is provided in section A.1.
4",1.0,"The quote is an exact, verbatim extraction of the first sentence from the provided source text. No words are missing, added, or altered."
15.0,Dense RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer. [...] The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",98.5645933014354,"minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.
The reconstruction task was specifically chosen to encourage the model to rely on context memory rather
than its parametric memory during training. Once the encoder is aligned with the decoder through this
reconstruction task, we initiate CPT byunfreezing the decoder.
Curriculum learning.The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk lengthk increases, the number of possible token combinations expands
exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity
within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.",0.6,"The second sentence of the quote, 'The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training,' is a verbatim extraction from the source text. However, the first sentence, 'In this task, we freeze the decoder model and only train the encoder and projection layer,' is not present as a verbatim or near-verbatim extraction in the provided source text. While the source text implies this state (by mentioning 'unfreezing the decoder' after the reconstruction task), the specific statement itself is not an extraction."
16.0,Dense RAG,True,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",90.64748201438849,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. There are no alterations, additions, or omissions of content."
17.0,Dense RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5","Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.1,"The quote contains several significant inaccuracies and misrepresentations of the source text. 
1. The source text mentions 'Table 1' and 'Table 2', but 'Table 13' is not present, indicating a hallucination of the table number.
2. The source states 'REFRAG8 and REFRAG16 consistently outperform other baselines'. It does NOT state that 'REFRAG16 consistently outperforms REFRAG8'. In fact, it groups them together as high-performing models against *other* baselines.
3. While 'RL-based selective compression' is mentioned in relation to REFRAGRL, and context lengths are discussed, the specific claim that REFRAG16 outperforms REFRAG8 across different datasets and context lengths is not supported by the provided text. The text states 'both REFRAG8 and REFRAG16 maintain superior performance at longer contexts' (against other models, not against each other).

Due to these major discrepancies and factual errors, the quote is largely unsupported and misleading."
18.0,Dense RAG,True,"our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,"CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",44.14414414414415,"the KV cache to reduce memory requirements for long-context applications. However, this approach only
decreases KV cache memory usage, does not improve time-to-first-token latency, and requires training the
model from scratch. Yoshida et al. (2021) extended this idea by employing recursive context compression,
generating a summary hidden state for each chunk to inform the next chunk’s computation. The recursive
nature, however, prevents pre-computation and reuse of chunk embeddings, and does not reduce decoding
latency. Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings
for prediction, similar to our method. However, their sequential compression process results in high latency
when the summary vector is not cached, and their approach only supports applications where the summary
token is restricted to the prefix of the language model (e.g., RAG), limiting applicability. In contrast, our",0.0,"The provided quote, 'our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.', is not present in the 'Source Text'. The 'Source Text' ends with 'In contrast, our', suggesting the quote might be a continuation of that sentence, but the actual content of the quote is entirely missing from the given source."
19.0,Dense RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.0,"The quote is not present in the source text. While the source text discusses REFRAG outperforming LLaMA under equal latency constraints and mentions Figure 4 comparing their performance under these conditions, it does not provide the specific details about '8 passages for REFRAG vs. 1 for LLaMA', '1.93% average gain', or '16 RAG tasks'. These specific numerical claims are entirely absent from the provided source text."
20.0,Dense RAG,True,"However, due to the compression, we are able to have more context information and hence achieve better performance.","2509.01092v2.pdf, Page: 7","By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",46.08695652173913,"the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We
compare the performance under both the short context and the long context scenarios. For the short context,
we use 1 passage forLLaMAFT and use 8 passages for all our models. The baseline ofREFRAG8 will
have the same latency as theLLaMAFT model. However, due to the compression, we are able to have more
context information and hence achieve better performance. Surprisingly,REFRAG16 and REFRAG32 both
outperform theLLaMAFT model despite having2× and4 × fewer tokens in the decoder (i.e., lower latency).
The same result occurs in long context scenarios. Our model has even higher performance gains in multi-choice
tasks. Table 15 shows the performance of our model under different numbers of passages. The result suggests
that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
21.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source."
22.0,Dense RAG,True,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",97.2972972972973,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote accurately conveys the information from the source text regarding acceleration for longer context lengths. The source states: 'With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.' The quote rephrases 'With longer context lengths, we are able to achieve' to 'For longer context length, acceleration reaches' and generalizes 'in both TTFT and throughput' to 'for both metrics'. Despite these minor rephrasing and generalization, the core meaning and numerical value are preserved, making it a near-verbatim extraction."
23.0,Dense RAG,True,The RL-based selective compression policy consistently achieves superior performance across varying compression rates p.,"2509.01092v2.pdf, Page: 4","Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",45.833333333333336,"chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the
effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression
rate without compromising performance.
REFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG
under different compression rates in the continual pre-training task. We observe a performance regression as
the compression rate increases; however, even at a compression rate of32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.
Different combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B",0.05,"The quote is not present in the source text. While the source text mentions an 'RL-trained policy' and discusses 'compression rates', the specific statement 'The RL-based selective compression policy consistently achieves superior performance across varying compression rates p' is not found. Furthermore, the source text explicitly states 'We observe a performance regression as the compression rate increases' and 'a compression rate of64appears to be overly aggressive, resulting in diminished performance', which directly contradicts the quote's assertion of 'consistently achieves superior performance across varying compression rates p'."
24.0,Dense RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",99.8158379373849,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",0.6,"The initial part of the quote, up to 'into the', is an accurate, verbatim extraction from the source text. However, the source text ends abruptly at 'into the', while the quote continues with 'decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.' This latter portion of the quote is not present in the provided source text, meaning a significant part of the quote is not supported and constitutes an addition rather than a verbatim or near-verbatim extraction."
25.0,Dense RAG,True,Reward = - Log(Perplexity),"Source Reference: 2509.01092v2.pdf, Page: 17","Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",57.692307692307686,"embeddings {ci}L
i=1 as transformer input and do not recompute logitssi after each selection, as state changes
have minimal impact and this improves training speed.
We use GRPO (Shao et al., 2024) style baseline to use grouped reward as baseline to reduce variance and to
minimize contamination across different segment prediction task. Specifically, for eachx we randomly select
Gnumber of lengthT ′ action sequences{l (i)}G
i=1 . We have the following objective:
Jθ = 1
G
PG
i=1 E x∼P(X),
{l(i)}G
i=1∼πθ([L]|x)
1
T′
PT′
t=1 min

πθ(l(i)
t |x,{l(i)
j }t−1
j=1)
πθold(l(i)
t |x,{l(i)
j }t−1
j=1)A(i)
t ,clip

πθ(l(i)
t |x,{l(i)
j }t−1
j=1)
πθold(l(i)
t |x,{l(i)
j }t−1
j=1),1−ϵ,1 +ϵ

A(i)
t

(1)
where ϵ is the clipping hyperparameter in PPO (Schulman et al., 2017) for stable training,θ is the current
policy andθold is the policy fro the previous iteration,At is the advantage function. We define our advantage
function using the negative log-perplexity on theotokens xs+1:s+o:
ri =r
",0.2,"The source text states: 'We define our advantage function using the negative log-perplexity on theotokens xs+1:s+o:'. This means the advantage function is defined using negative log-perplexity. The quote, however, states 'Reward = - Log(Perplexity)'. The source text does not define 'Reward' as negative log-perplexity; it defines the 'advantage function' as such. While 'negative log-perplexity' is present in the source, its attribution to 'Reward' in the quote is a misrepresentation and not supported by the text."
26.0,Dense RAG,True,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).",filename: 2509.01092v2.pdf page 2,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",99.33554817275748,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,"The quote is nearly verbatim. There are minor formatting differences in spacing compared to the source text, such as 'of16384' vs 'of 16384', 'context),REFRAG' vs 'context), REFRAG', '16achieves' vs '16 achieves', '16 .53×' vs '16.53×', 'and8.59×' vs 'and 8.59×', and 'and1 .04×' vs 'and 1.04×'. However, all words, numbers, and punctuation are present and in the correct order, making it a highly faithful extraction."
27.0,Dense RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",95.6896551724138,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.15,"The source text generally states that 'REFRAG perform similarly to LLaMA model under the same retrieved passages (slightly better in a weaker retriever case) while outperform significantly under the same latency.' This supports the general idea that REFRAG performs better at equal latency. However, the quote introduces specific numerical details such as '(8 passages for REFRAG vs. 1 for LLaMA)', 'a 1.22% average improvement', and 'across 16 RAG tasks'. These specific figures and conditions are not present or supported anywhere in the provided source text. The core numerical claims of the quote are entirely unsupported, making it largely a hallucination of specific data points."
28.0,Dense RAG,True,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4","For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",45.51282051282052,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It appears in the third sentence of the provided source."
29.0,Dense RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",100.0,"chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the
effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression
rate without compromising performance.
REFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG
under different compression rates in the continual pre-training task. We observe a performance regression as
the compression rate increases; however, even at a compression rate of32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.
Different combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B",1.0,"The quote is an exact, verbatim extraction from the source text. The only minor difference is the spacing around '64' (source has 'of64' while quote has 'of 64'), which is a negligible formatting detail and does not affect the content or meaning."
30.0,Dense RAG,True,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency...",filename: 2509.01092v2.pdf page 1,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",48.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is a verbatim extraction of a continuous segment from the source text. The only minor difference is the handling of 'represen-tations' in the source text due to a line break, which is correctly rendered as 'representations' in the quote. The '...' at the end of the quote accurately indicates that the sentence continues in the source text."
31.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","Source Reference: 2509.01092v2.pdf, Page: 1","This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
32.0,Dense RAG,True,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",75.37993920972644,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",1.0,"The quote is an exact, verbatim extraction from the source text. The only minor difference is the handling of a line break in the source text ('short context\nlength s') which is presented as a continuous phrase in the quote ('short context length s'), but this does not alter the content or meaning."
33.0,Dense RAG,True,"Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves 16.53× acceleration in TTFT with cache and 8.59× without cache.",filename: 2509.01092v2.pdf page 15,"With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",84.80565371024736,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote is a near-verbatim extraction from the source text. The only differences are minor formatting variations such as spacing around numbers and abbreviations (e.g., 'of16384(' vs 'of 16384 (', 'REFRAGachieves' vs 'REFRAG achieves', '16 .53×' vs '16.53×', 'and8.59×' vs 'and 8.59×') and the absence of a line break present in the source. The core content, numbers, and meaning are identical."
34.0,Dense RAG,False,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5","In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",61.36363636363637,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
35.0,Dense RAG,True,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",93.02325581395348,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the text precisely without any alterations, additions, or omissions."
36.0,Dense RAG,True,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2","A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",46.97986577181208,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",0.0,"The provided quote, 'Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.', is not present in the source text. While the source text discusses 'RL-based selective compression', the specific sentence and its content (e.g., 'defer the discussion to section 2') are entirely absent."
37.0,Dense RAG,True,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",97.79735682819384,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text. The only differences are minor spacing variations around numbers (e.g., 'rate of16' in source vs 'rate of 16' in quote, 'a9.3%' vs 'a 9.3%'), which do not alter the content or meaning and are considered verbatim for this evaluation."
38.0,Dense RAG,False,No quote provided,,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
39.0,Dense RAG,True,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,95.54140127388536,"accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in
table 7.
Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl
dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We
use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.
(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.
Result analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,
varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.
This is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is
the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We",0.95,"The quote is a verbatim extraction from the source text. All words in the quote appear in the exact order as in the source. However, the quote omits the initial phrase 'Retrieverandretrievalcorpus.' that precedes 'We follow...' in the source, and it truncates the sentence after 'as our retriever', not including the subsequent phrase 'and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.' Therefore, it is a verbatim segment but not an exact full sentence as it appears in the source."
40.0,Dense RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.15,"The source text discusses REFRAG outperforming LLaMA under equivalent latency constraints and mentions performance averaged over 16 tasks. However, the specific numerical details provided in the quote, such as '8 passages for REFRAG vs. 1 for LLaMA' and '1.22% average improvement', are not present in the provided source text. While the general context is related, the quote is not a verbatim or near-verbatim extraction due to the inclusion of unverified specific data points."
41.0,Dense RAG,True,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1","REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",98.989898989899,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
42.0,Dense RAG,True,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5","Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",63.41463414634146,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
43.0,Dense RAG,True,REFRAGRL uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4","Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",71.11111111111111,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim extraction of the first sentence from the provided source text."
44.0,Dense RAG,False,"Notably, REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4","In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",46.103896103896105,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact sentence from the source text, with only a minor difference in spacing after 'Notably,' (no space in source, one space in quote). This is considered verbatim for practical purposes as it does not alter the content or meaning."
45.0,Dense RAG,True,"This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the performance of REFRAG8.",filename: 2509.01092v2.pdf page 6,This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,52.151898734177216,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.0,"The quote is entirely unsupported by the source text. The source text does not mention 'This finding is particularly surprising', nor does it state that 'REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings', or that 'REFRAG16 surpasses the performance of REFRAG8'. In fact, the source text often groups REFRAG8 and REFRAG16 together as performing well (e.g., 'REFRAG8 and REFRAG16 consistently outperform other baselines' and 'both REFRAG8 and REFRAG16 maintain superior performance')."
46.0,Dense RAG,False,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0","Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",45.58823529411765,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",0.0,"The provided quote, 'In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query,' does not appear anywhere in the source text. The source text discusses issues such as wasteful memory/computation, discarded encoding information, and sparse attention in RAG, and then introduces REFRAG. While the quote describes a common characteristic of RAG contexts that aligns with the *problems* discussed in the source (e.g., 'uninformative and reused,' 'most context chunks during decoding are unrelated'), the sentence itself is not present as a verbatim or near-verbatim extraction."
47.0,Dense RAG,True,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",100.0,"The data transfer latency to high-bandwidth memory is2n+4dlb(s+o)
m .
Throughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given
by:
Throughput= bo
TTFT+DL
where DL is the data latency.
Before After
KV cache memory4dlb(s+o) 4dlb
",0.0,"The provided quote, 'Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).', is not present anywhere in the 'Source Text'. The source text mentions TTFT and TTIT in the context of formulas and a comparison table, but it does not describe their latency growth characteristics (quadratically, linearly) with prompt length, nor does it contain the citation '(Liu et al., 2025)'. The quote is entirely unsupported by the provided source."
48.0,Dense RAG,True,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19","To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",46.59090909090909,"Stage 1 Stage 3 Stage 5 Stage 7 Stage 9
Training Stage
0
50
100
Percentage
Context
1 × k
2 × k
4 × k
8 × k
16 × k
32 × k
64 × k
128 × k
256 × k
Figure 6The data mixture in curriculum learning during the training.
Factor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation
1×81333 445 148 49 16 6 2 1 0 2000
2×8333 298 267 238 213 191 171 153 137 2000
4×883 102 126 156 193 238 293 362 447 2000
8×820 35 61 106 185 324 565 985 1719 4000
16×85 11 23 48 103 220 468 997 2125 4000
32×81 3 7 19 50 133 353 939 2496 4000
64×81 3 9 25 73 212 618 1802 5259 8000
128×81 3 9 25 73 212 618 1802 5259 8000
256×81 3 9 25 73 212 618 1802 5259 8000
Table 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we
have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).",1.0,"The quote is an exact, verbatim extraction from the source text. It appears as the last sentence in the provided source text, following 'Table 8The geometry curriculum learning scheduling.' The line break 'we
have' in the source text is correctly normalized in the quote without altering the content."
49.0,Dense RAG,True,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","2509.01092v2.pdf, Page: 7","The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim extraction of two consecutive sentences from the source text."
50.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with '3) It reduces attention computation complexity...' perfectly."
51.0,Dense RAG,True,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",filename: 2509.01092v2.pdf page 1,"3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",100.0,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches the sentence starting with 'Due to diversity...' under point '3)Unusually Structured and Sparse Attention.'"
52.0,Dense RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with '3) It reduces attention computation complexity...' perfectly."
53.0,Dense RAG,False,No quote provided,,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
54.0,Dense RAG,True,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",83.04093567251462,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,"The quote accurately extracts the core information regarding k²× acceleration for longer context lengths in both TTFT and throughput. However, it is not a verbatim extraction. The source text states: 'For longer context length, acceleration reaches up tok2× for both metrics.' The quote rephrases 'For longer context length' to 'With longer context lengths', changes 'acceleration reaches' to 'we are able to achieve', and clarifies 'for both metrics' to 'in both TTFT and throughput' (which is accurate given the preceding sentence in the source). The factual content is fully supported, but the phrasing is slightly altered."
55.0,Dense RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",100.0,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",0.95,The quote is nearly an exact sentence from the source text. The only difference is a missing space between 'we' and 'freeze' in the source text ('wefreeze') compared to the quote ('we freeze'). This is a minor formatting difference that does not alter the meaning or content.
56.0,Dense RAG,True,"Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported
as average of Arxiv and Book domain.
...
REFRAGw/o curriculum 3.719 3.098 2.272 1.599
REFRAGwith curriculum 0.669 0.451 0.230 0.135","2509.01092v2.pdf, Page: 25","Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",41.05691056910569,"Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported
as average of Arxiv and Book domain.
P16 P32 P128 P2048↓
LLaMA-Full Context1.397 0.734 0.203 0.021
LLaMA-No Context3.483 2.981 2.249 1.590
REFRAGw/o curriculum 3.719 3.098 2.272 1.599
REFRAGwith curriculum 0.669 0.451 0.230 0.135
Table 12Performance comparison on continual pre-training task with and w/o continued from reconstruction task.
Log-Perplexity is reported as average of Arxiv and Book domain.
P16 P32 P128 P2048↓
LLaMA-Full Context1.448 1.458 1.464 1.449
LLaMA-No Context3.483 2.981 2.249 1.590
REFRAGw/o reconstruction 3.272 2.789 2.119 1.544
REFRAGwith reconstruction 2.017 1.837 1.632 1.453
Table 13The performance ofREFRAGunder the same compression rate with full compression (i.e.,REFRAG8) and
selective compression (i.e.,REFRAG16+RL).
Arxiv Book PG19 ProofPile
Compression Rate P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048 P512 P1024 P2048↓
Context Length=2048",0.95,"The quote accurately reproduces the initial and final lines of the specified section from the source text. An ellipsis (...) is correctly used to indicate the omission of the intermediate lines ('P16 P32 P128 P2048↓', 'LLaMA-Full Context...', and 'LLaMA-No Context...'). The extracted parts are verbatim."
57.0,Dense RAG,False,We compare the perplexity of xs+1:s+o using different selection policy under different p.,"2509.01092v2.pdf, Page: 4","Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",47.19101123595506,"selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate
1.02
1.03
1.04
1.05
1.06
1.07
Log-perplexity
Arxiv
4 8 16
Compression rate
1.82
1.83
1.84
1.85
1.86
Log-perplexity
Book
4 8 16
Compression rate
1.91
1.92
1.93
1.94
1.95
Log-perplexity
PG19
4 8 16
Compression rate
0.87
0.88
0.89
0.90
0.91
0.92
Log-perplexity
ProofPile
RL Perplexity-desc Perplexity-asc Random
Figure 3Log-Perplexity on xs+1:s+o under varying compression rates by selectively compressing different percentages
of chunks. We compare three selection methods:RL(trained policy),Perplexity-desc(heuristic: lower perplexity),
Perplexity-asc(heuristic: higher perplexity), andRandom(random selection).
6",0.65,"The quote accurately summarizes the experimental setup described in the figure caption, specifically mentioning the comparison of perplexity for 'xs+1:s+o' using different selection policies under varying conditions. The phrase 'We compare' is directly present. However, it is not a verbatim or near-verbatim extraction. It rephrases several key phrases (e.g., 'Log-Perplexity on' to 'the perplexity of', 'three selection methods' to 'different selection policy') and significantly condenses 'under varying compression rates by selectively compressing different percentages of chunks' to 'under different p.', which is an abstraction not explicitly stated as 'p' in the source. While the information is fully supported, the quote is a paraphrase/summary rather than a direct extraction."
58.0,Dense RAG,True,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",98.65771812080536,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'Table 13 demonstrates...' perfectly."
59.0,Dense RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",94.82758620689656,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote contains specific numerical details such as '8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement', and '16 RAG tasks'. None of these specific figures or claims are present in the provided source text. While the source text discusses REFRAG, LLaMA, and performance vs. latency, it does not support the verbatim content of the quote."
60.0,Dense RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text. It starts with 'However, CEPE is limited...' and ends with '...or summarization.', matching the source precisely."
61.0,Dense RAG,True,the overall input to thedecoder will be reduced by a factor of≃k .,"2509.01092v2.pdf, Page: 2","This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",43.939393939393945,"10000 20000 30000 40000 50000 60000
Training Steps
1.44
1.46
1.48
1.50
Loss
Llama-2-7B
Llama-2-13B
10000 20000 30000 40000 50000 60000
Training Steps
1.46
1.47
1.48
1.49
1.50
1.51
Loss
Roberta-Base
Roberta-Large
Figure 11Training trajectory for different encoder and decoder combinations. On the left, we have two different decoder
the Roberta-Base encoder. On the right we have two different encoder for LLaMA-2-7B decoder model.
10000 20000 30000 40000 50000 60000
Training Steps
1.44
1.45
1.46
1.47
1.48
1.49
1.50
Loss
Roberta-Base
Roberta-Large
Figure 12Training trajectory for different encoder paired with LLaMA-2-13B decoder.
outperforms others. Table 15 shows the performance ofREFRAGunder different number of context for
strong retriever setting.
Demonstration of generated summary for Arxiv and Pubmed articles.Table 20 and table 19 shows the ground
true abstract for different articles and the generated summary fromREFRAG. These results complement the",0.0,"The provided quote, 'the overall input to thedecoder will be reduced by a factor of≃k .', is not present anywhere in the source text. The source text discusses training steps, loss, specific model names (Llama-2, Roberta), and figures/tables related to training trajectories and performance, but it does not contain any of the content from the quote."
62.0,Dense RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",95.73459715639812,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the source text perfectly without any alterations or omissions."
63.0,Dense RAG,True,"The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases).","Source Reference: 2509.01092v2.pdf, Page: 4","Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",97.02970297029702,"chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection
policy under differentp. The perplexity-based selection is an heuristic based selection which compresses
chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured
by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can
therefore be compressed with minimal information loss. Ideally, this approach should outperform random
selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves
superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while",1.0,"The quote is an exact, verbatim extraction from the source text, including the line break and mathematical notation."
64.0,Dense RAG,False, ,,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",0.0,,0.0,"The source text is an empty string, containing no characters. The quote, however, contains a single space character. Therefore, the quote is not present in the source text and cannot be considered a verbatim or near-verbatim extraction."
65.0,Dense RAG,True,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,"Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",47.53363228699552,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted from the source text. The only minor difference is the handling of a hyphenated word ('represen-tations') due to a line break in the source, which is correctly rendered as a single word in the quote."
66.0,Dense RAG,True,"CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",0.95,"The quote is nearly verbatim from the source text. It is an exact sentence from the source, with the omission of the word 'However,' at the very beginning of the sentence. The core content and phrasing are identical."
67.0,Dense RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7","Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.0,"The quote is not present in the provided source text. While the source text discusses 'equal latency', 'REFRAG', 'LLaMA', and '16 tasks', it does not contain the specific details about '8 passages for REFRAG vs. 1 for LLaMA' or the '1.22% average improvement'."
68.0,Dense RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4","Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",100.0,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",0.95,The quote is nearly an exact sentence from the source text. The only difference is a minor formatting change: 'a4k' in the source text is presented as 'a 4k' in the quote. This is a negligible difference that does not alter the meaning or content.
69.0,Dense RAG,False,No quote provided,,"Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
70.0,Dense RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim sentence extracted from the source text. The only differences are minor whitespace variations (e.g., space after comma before 'LLaMA256', space before parenthesis for 'REFRAG8'), which do not alter the content or meaning."
1.0,Hybrid RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
2.0,Hybrid RAG,False,Reconstruction task is essential for the model to learn the continual pre-training task.,filename: 2509.01092v2.pdf page 5,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",61.36363636363637,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
3.0,Hybrid RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,"CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text. Every word and punctuation mark matches perfectly."
4.0,Hybrid RAG,True,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.",filename: 2509.01092v2.pdf page 7,"This improvement is attributable to the limited 4k-token context window of LLAMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on the same LLAMA model without extending its effective positional encoding, maintains robust performance even with a large number of passages, owing to the benefits of our compression approach.",100.0,"2022), ORConvQA (Qu et al., 2020), and QReCC (Anantha et al., 2021). For each conversation turn, we
retrieveKpassages using the same retriever and retrieval corpus as described in section 5.1.
Result analysis.Table 4 presents results across varying numbers of conversational turns and retrieved passages.
Our model outperformsLLaMAFT on two out of three datasets in the 5-passage setting, and on all three
datasets in the 10-passage setting. This improvement is attributable to the limited 4k-token context window of
LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting
in the loss of crucial information required to answer subsequent questions. In contrast, our model, trained on
the sameLLaMA model without extending its effective positional encoding, maintains robust performance
3Note that the implementation of our exact match is stricter than other works. We follow the work of Lin et al. (2024) to use",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the sentence starting with 'This improvement is attributable...' and ending with '...subsequent questions.' perfectly."
5.0,Hybrid RAG,True,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0","In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance.",100.0,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",0.8,"The quote is not a verbatim or near-verbatim extraction from the source text. The exact sentence does not appear in the provided source. However, the quote accurately summarizes the core problem that the source text elaborates on in points 1, 2, and 3. The source discusses 'uninformative' tokens, 'unnecessarily wasteful' allocation for all tokens, and 'unrelated' context chunks, which collectively support the idea that 'much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.' While not an extraction, the information conveyed is fully supported by the source's content."
6.0,Hybrid RAG,True,"Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.",filename: 2509.01092v2.pdf page 4,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V^k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge. Additionally, reconstructing s = k x L tokens from L chunk embeddings further compounds the difficulty of the task. ... To address the optimization challenge, we propose employing curriculum learning for both tasks.",45.40816326530612,"superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while
intuitive, is particularly challenging when multiple chunks must be reconstructed. Table 11 shows the
2Percentage calculated as LLaMA-No Context−Log-perplexity to inspect
LLaMA-No Context−min(LLaMA-Full Context,LLaMA-32K)
5",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the text starting from 'Curriculum learning is essential...' and ending with '...must be reconstructed.' perfectly."
7.0,Hybrid RAG,True,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .",filename: 2509.01092v2.pdf page 1,"This ""compress anywhere"" capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice. As a result, REFRAG minimizes reliance on computationally intensive token embeddings, condensing most chunks for the query in RAG settings.",98.92933618843684,"C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain
uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at
arbitrary positions, which is essential for the subsequent RL policy learning.
We sequentially pickT′ chunk indicesl = {lj}T′
j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′
j=1) =
{E1, . . . , EL}, with Ei =e cnk
i if i /∈ {lj}T′
j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′
j=1
16",0.2,"The quote is not a verbatim or near-verbatim extraction from the source text. While the quote discusses concepts that are present or implied in the source text (e.g., RL policy, compression, handling mixed inputs, full vs. compressed chunks), the specific wording and sentence structure of the quote are entirely different and do not appear in the provided source text. It functions more as a summary or interpretation rather than an accurate extraction."
8.0,Hybrid RAG,True,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to kx acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²x for both metrics.",82.08092485549133,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.95,"The quote accurately extracts the core information regarding acceleration for longer context lengths. The source text states: 'For longer context length, acceleration reaches up tok2× for both metrics.' The quote rephrases 'For longer context length' to 'With longer context lengths', 'acceleration reaches' to 'we are able to achieve', and explicitly states 'in both TTFT and throughput' instead of 'for both metrics' (which is contextually accurate as TTFT and throughput are the metrics discussed). While not verbatim, it is a very close and accurate near-verbatim extraction."
9.0,Hybrid RAG,True,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7","Under equivalent latency constraints, REFRAG consistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",98.96907216494846,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
10.0,Hybrid RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,"We observe a performance regression as the compression rate increases; however, even at a compression rate of 32, our model remains competitive (as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",100.0,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The provided quote, 'In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.', is not found anywhere in the source text. The source text mentions 'REFRAG 8 × compression' and 'compression rate of 8', but not 'compression rate of 64' or the full sentence from the quote."
11.0,Hybrid RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance.",100.0,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
12.0,Hybrid RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency;
2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces
attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens
in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source."
13.0,Hybrid RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for
both metrics.",96.35854341736696,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction from the source text. It matches word-for-word, including punctuation and spacing."
14.0,Hybrid RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...",filename: 2509.01092v2.pdf page 3,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a
negative reward, determines which chunks to retain in their original form. The policy network leverages chunk
embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.",98.98648648648648,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",0.65,"The quote contains two segments that are verbatim from the source text. However, it omits a full sentence ('The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.') between these two segments without using an ellipsis to indicate the omission. Additionally, the quote adds the phrase 'autoregressive property' at its very end, which is not present in the source text. These unindicated omissions and additions mean the quote is not a verbatim or near-verbatim extraction."
15.0,Hybrid RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer. ... The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task,
we freeze the decoder model and only train the encoder and projection layer. … The reconstruction task was specifically chosen to
encourage the model to rely on context memory rather than its parametric memory during training.",99.03846153846152,"minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.
The reconstruction task was specifically chosen to encourage the model to rely on context memory rather
than its parametric memory during training. Once the encoder is aligned with the decoder through this
reconstruction task, we initiate CPT byunfreezing the decoder.
Curriculum learning.The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk lengthk increases, the number of possible token combinations expands
exponentially, specifically at a rate ofV k, whereV is the vocabulary size. Effectively capturing this diversity
within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.",0.5,"The second sentence of the quote, 'The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.', is an exact, verbatim extraction from the source text. However, the first sentence of the quote, 'In this task, we freeze the decoder model and only train the encoder and projection layer.', is not present verbatim or near-verbatim in the provided source text. While the source text implies that the decoder is frozen during the reconstruction task (before CPT), this specific statement is not a direct extraction."
16.0,Hybrid RAG,True,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge... To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the
vocabulary size. … For the reconstruction task, training begins with reconstructing a single chunk … Subsequently, the model reconstructs x1:2k …
To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks and gradually shifting
towards those dominated by more difficult tasks.",86.27968337730871,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",0.47,"The quote contains two distinct parts. The first part, 'As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge...', is not found anywhere in the provided Source Text. The second part, 'To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).', is an exact, verbatim extraction from the Source Text. Since approximately half of the quote is hallucinated and half is verbatim, the overall faithfulness score is low."
17.0,Hybrid RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,"Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the
performance of REFRAG8.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.05,"The quote contains several significant inaccuracies and unsupported claims. It refers to 'Table 13', which is not mentioned in the source text (only 'Table 1' and 'Table 2' are present). More critically, the quote states that 'REFRAG16 ... consistently outperforms REFRAG8'. The source text, however, states 'REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings' and 'both REFRAG8 and REFRAG16 maintain superior performance at longer contexts', grouping them together as high performers against *other baselines*, not comparing them against each other in this manner. The phrase 'across different datasets' is also not explicitly stated in the source. While 'RL-based selective compression' is mentioned in relation to REFRAGRL, and REFRAG16 is a REFRAG model, the overall claim about REFRAG16 outperforming REFRAG8 and the table number are not supported, making the quote largely hallucinated in its core assertion."
18.0,Hybrid RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9","CEPE … is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG
or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",99.62406015037594,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the text precisely, including punctuation and wording."
19.0,Hybrid RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7","With a weak retriever setting, at 10 passages, REFRAG improves performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA. At equal latency
(8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.1,"The quote introduces specific numerical details, namely '8 passages for REFRAG vs. 1 for LLaMA' and '1.93% average gain', which are not present in the provided source text. While the source text discusses REFRAG outperforming LLaMA under equivalent latency constraints and mentions performance averaged over 16 tasks, it does not provide these specific figures. Therefore, the quote is not an accurate, verbatim, or near-verbatim extraction and contains information not supported by the given source."
20.0,Hybrid RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4","By exploiting this attention sparsity structure, we demonstrate a 30.85× the time-to-first-token acceleration … In addition, our optimization framework for
large context enables REFRAG to extend the context size of LLMs by 16×. We provide rigorous validation of REFRAG across diverse long-context tasks.",46.29213483146067,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text, with only a minor formatting difference (a space added between 'a' and '4k') that does not change the meaning or content."
21.0,Hybrid RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","Instead of using tokens from retrieved
passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted from the source text. It appears as the third advantage listed: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
22.0,Hybrid RAG,True,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics.",97.2972972972973,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.85,"The quote accurately conveys the core information from the source text regarding acceleration for longer context lengths. The source states: 'With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.' The quote rephrases this as: 'For longer context length, acceleration reaches up tok2× for both metrics.' While the meaning is fully preserved, there are several changes in wording: 'With' vs 'For', 'lengths' vs 'length', 'we are able to achieve' vs 'acceleration reaches', and 'in both TTFT and throughput' vs 'for both metrics'. These changes go beyond minor punctuation or capitalization, indicating significant rephrasing and a slight generalization ('metrics' instead of specific names), thus it is not a verbatim or very near-verbatim extraction, but highly faithful."
23.0,Hybrid RAG,True,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .",filename: 2509.01092v2.pdf page 1,"Unlike prior methods (Yen et al., 2024), REFRAG supports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice .",99.77272727272728,"the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at
arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting
multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight
reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary
and when low-cost, approximate chunk embeddings suffice . As a result,REFRAGminimizes reliance on
computationally intensive token embeddings, condensing most chunks for the query in RAG settings.
We provide rigorous experimental validations of the effectiveness ofREFRAGin continual pre-training
and many real word long-context applications including RAG, multi-turn conversation with RAG and long
document summarization. Results show that we achieve30.75× TTFT acceleration without loss in perplexity",1.0,"The quote is an exact, verbatim extraction of two consecutive sentences from the provided source text. There are no alterations, additions, or omissions."
24.0,Hybrid RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",99.8158379373849,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",0.75,"The initial part of the quote, 'The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the', is verbatim from the source text. However, the source text ends at 'into the', while the quote continues with 'decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.', which is not present in the provided source text. Therefore, a significant portion of the quote is not supported by the provided source."
25.0,Hybrid RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,"Selective compression REFRAG introduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. There are no alterations or omissions."
26.0,Hybrid RAG,True,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...",filename: 2509.01092v2.pdf page 2,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",98.59943977591035,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",0.95,"The quote accurately extracts the key numerical results (context length of 16384, 16.53× TTFT acceleration with cache, and 8.59× without cache) and their attribution to REFRAG from the source text. While the introductory phrase 'Empirically, as shown in figure 2,' is a rephrasing of the source's 'Figure 2 shows the empirical measurement...' and 'Empirical verification...', and 'with k = 16' is added (likely an inference from the 'k× acceleration' mentioned generally in the source and the 16.53× value), the core factual information is faithfully represented and largely uses the same terminology, making it a strong near-verbatim extraction of the critical data points."
27.0,Hybrid RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks. With a weak retriever setting, at 10 passages, REFRAG improves
performance by 0.71% and accelerates TTFT by 5.26× compared to LLaMA.",95.6896551724138,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The quote is not present in the provided source text. While the source text discusses 'REFRAG' and 'LLaMA' performance under 'latency', it does not contain the specific numerical details (e.g., '8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement', '16 RAG tasks') mentioned in the quote. Therefore, the quote is unsupported by the given source."
28.0,Hybrid RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",1.0,"The quote is an exact, verbatim sentence extracted from the source text. There are no changes in wording or meaning."
29.0,Hybrid RAG,True,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6","We observe a performance regression as
the compression rate increases; however, even at a compression rate of 32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of 64 appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.",100.0,"chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the
effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression
rate without compromising performance.
REFRAG trained under different compression rates.Figure 10 illustrates the training trajectory ofREFRAG
under different compression rates in the continual pre-training task. We observe a performance regression as
the compression rate increases; however, even at a compression rate of32, our model remains competitive
(as shown in table 1). In contrast, a compression rate of64appears to be overly aggressive, resulting in
diminished performance. These findings suggest a practical limit to the compression rate beyond which the
model’s capability is significantly reduced.
Different combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B",0.95,"The quote is nearly an exact extraction from the source text. The only difference is the presence of a space between 'of' and '64' in the quote ('of 64'), which is absent in the source text ('of64'). This is a minor formatting difference and does not change the meaning or content, making it a near-verbatim extraction."
30.0,Hybrid RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",100.0,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",0.95,The quote is nearly verbatim from the source text. The only difference is the addition of a space between 'a' and '4k' in the quote ('a 4k') compared to the source ('a4k'). This is a minor formatting change and does not alter the meaning or content.
31.0,Hybrid RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It perfectly matches the third advantage listed in the source."
32.0,Hybrid RAG,True,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,"Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",75.37993920972644,"number of tokens that are generated from the system in a unit time. Table 6 shows that with short context
length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With
longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput. The
details on the latency and throughput calculation are in section B.4.
Empirical verification of latency/throughput improvement.Figure 2 shows the empirical measurement of
the acceleration ofREFRAGcompared with CEPE, a previous work that achieves significant acceleration
in inference (Yen et al., 2024). Under the context length of16384(i.e., mid-to-long context),REFRAG
achieves16 .53× acceleration in TTFT with cache and8.59× without cache. Both higher than CEPE (i.e.,
2.01× and1 .04× acceleration respectively) while having better model performance (see table 1). With longer
context, we are able to achieve up to32.99× acceleration in TTFT. The reason why we get such acceleration",1.0,"The quote is a verbatim extraction from the source text. The only difference is the handling of a line break in the source text ('short context\nlength s') which is presented as a continuous phrase in the quote ('short context length s'), but this does not alter the content or meaning."
33.0,Hybrid RAG,True,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).","2509.01092v2.pdf, Page: 2","With a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared to CEPE (table 1).",97.51773049645392,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction of a sentence from the provided source text. There are no alterations, additions, or omissions."
34.0,Hybrid RAG,True,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","Source Reference: 2509.01092v2.pdf, Page: 3","In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",56.57370517928286,"on empirical evaluation is in section A.
3 Methodology
To align the encoder and decoder, we follow the work of Yen et al. (2024) to use the next paragraph prediction
tasks for continual pre-training (CPT). Specifically, for each data data point, it containss + o = T number
of tokens, which we use for CPT to prepare the model for downstream tasks utilizing chunk embeddings.
To further enhance performance, we introduce selective compression via RL. After aligning the encoder and
decoder through CPT, we apply supervised fine-tuning (SFT) to adapt the model to specific downstream
tasks, such as RAG and multi-turn conversation. Additional details are provided in section 5.
During CPT, we input the firsts tokens x1:s into the encoder and use its output to assist the decoder in
predicting the nexto tokens xs+1:s+o. This task encourages the model to leverage contextual information for
next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It appears in the paragraph starting with 'During CPT, we input the firsts tokens...'"
35.0,Hybrid RAG,True,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",filename: 2509.01092v2.pdf page 3,"Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and the decoder reconstructs the k tokens using the projected chunk embedding. Subsequently, the model reconstructs x1:2k from two chunk embeddings, and so forth. To continuously adjust task difficulty, we vary the data mixture over time, starting with easier tasks and gradually shifting toward more difficult tasks.",94.33962264150944,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",1.0,"The quote is an exact, verbatim extraction from the source text. There are no changes, additions, or omissions in the text itself, only a difference in line breaks which does not affect the content."
36.0,Hybrid RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","Source Reference: 2509.01092v2.pdf, Page: 3","A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches word-for-word and punctuation-for-punctuation."
37.0,Hybrid RAG,True,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,"With a compression rate of 16, we achieve a 9.3% average perplexity improvement over CEPE across four datasets. Meanwhile, our method is 16.53× faster than LLaMA in TTFT and 2.01× faster than CEPE.",97.79735682819384,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text. The only difference is the addition of spaces around the numbers '16' and '9.3%', which is a minor formatting adjustment and does not change the content or meaning of the quote."
38.0,Hybrid RAG,False,No quote provided,,We use the SlimPajama dataset… We only use the Book and ArXiv domains from the dataset since these two domains contain long texts. We sampled from this dataset to construct a 20B token training dataset which contains 50% data from Arxiv and 50% data from Book.,0.0,"Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm
Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob
Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and
Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors,Proceedings of the 39th International
Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 2206–2240. PMLR,
17–23 Jul 2022.https://proceedings.mlr.press/v162/borgeaud22a.html.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.",0.0,No quote provided by the LLM.
39.0,Hybrid RAG,True,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model as our retriever and use the implementation of Izacard et al. (2023a) to retrieve the K-nearest neighbors.,92.34135667396062,"accumulation across subsystems. A table summarizing the evaluation metrics for each dataset is included in
table 7.
Retrieverandretrievalcorpus.WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl
dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We
use the DRAGON+ model Lin et al. (2023) as our retriever and use the implementation of Izacard et al.
(2023a) to retrieve the K-nearest neighbors as the retrieved passages for each question.
Result analysis.Table 3 shows the performance of different baselines under short and long contexts (i.e.,
varying number of retrieved passages)3. (1/# tokens) is inverse for the number of tokens in the decoder model.
This is used as a metric to gauge the latency of the model (the higher, the lower latency).LLaMAFT is
the original LLaMA-2-7B model that is fine-tuned on the same RAG dataset used to train our model. We",1.0,"The quote is an exact, verbatim extraction from the source text. It starts with 'WefollowtheworkofLinetal.(2024)...' and ends with '...as our retriever...' and perfectly matches the corresponding sentences in the 'Retrieverandretrievalcorpus' section of the source text."
40.0,Hybrid RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",95.6896551724138,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.1,"The source text states that 'REFRAG perform similarly to LLaMA model under the same retrieved passages... while outperform significantly under the same latency.' This general statement aligns with the theme of the quote. However, the quote provides highly specific numerical details such as '8 passages for REFRAG vs. 1 for LLaMA', '1.22% average improvement', and '16 RAG tasks'. None of these specific numbers or the exact phrasing of the quote are present anywhere in the provided source text. Therefore, the specific claims made in the quote are not supported by the source, indicating a high degree of hallucination for the detailed information."
41.0,Hybrid RAG,True,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,"REFRAG makes several novel modifications to the decoding process: Instead of using tokens from retrieved passages as input, REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder's input length, improving token allocation efficiency; 2) It enables reuse of pre-computed chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",98.989898989899,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
42.0,Hybrid RAG,True,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder's chunk embeddings into the decoder's token space, allowing the decoder to interpret and accurately reconstruct the original information.",99.0950226244344,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the last sentence of the provided source text precisely."
43.0,Hybrid RAG,True,We employ RL to train a policy that optimally determines which segments to compress.,filename: 2509.01092v2.pdf page 15,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",51.19047619047619,"C1, . . . , CL, we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain
uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at
arbitrary positions, which is essential for the subsequent RL policy learning.
We sequentially pickT′ chunk indicesl = {lj}T′
j=1, wherelt ∈ [L]. The input arrangement isE(x,{l j}T′
j=1) =
{E1, . . . , EL}, with Ei =e cnk
i if i /∈ {lj}T′
j=1 (compressed), and Ei = {ek∗i, . . . ,ek∗i+k−1} if i∈ {lj}T′
j=1
16",0.0,"The quote 'We employ RL to train a policy that optimally determines which segments to compress' is not present in the provided source text, either verbatim or near-verbatim. The source text mentions 'RL policy learning' as a subsequent step to pretraining, but it does not state that RL is employed to optimally determine which segments to compress. Instead, it describes a process of *randomly selecting* chunks to remain uncompressed."
44.0,Hybrid RAG,False,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9","In contrast, our work is the first to enable pre-computation of chunk embeddings and their use at arbitrary positions within the prompt, supporting diverse applications such as RAG and multi-turn conversation. Furthermore, our method learns where to apply compression, allowing for adaptive compression rates at inference time without recomputing chunk embeddings.",47.77777777777777,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the provided source text."
45.0,Hybrid RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"Source Reference: 2509.01092v2.pdf, Page: 5",This raises a natural question: does the former approach outperform the latter? Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.1,"The quote contains several significant inaccuracies and is not a verbatim or near-verbatim extraction. Firstly, it refers to 'Table 13', which is not mentioned in the source text (only 'Table 1' and 'Table 2' are present). Secondly, the quote claims 'REFRAG16 ... consistently outperforms REFRAG8'. The source text, however, states 'REFRAG8 and REFRAG16 consistently outperform other baselines', grouping them together as strong performers against *other* baselines, not against each other. Thirdly, the phrase 'across different datasets' is not explicitly stated in the source, although 'across nearly all settings' and 'at longer contexts' are mentioned. While some keywords like 'REFRAG16', 'REFRAG8', 'RL-based selective compression', and 'context lengths' are present in the source, the central assertion of the quote is factually incorrect and unsupported by the provided text."
46.0,Hybrid RAG,True,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",50.12919896640827,"require specialized consideration. In RAG, much of the LLM context consists of concatenated
passages from retrieval, with only a small subset directly relevant to the query. These passages
often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to
block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based
on this observation, we argue that most computations over the RAG context during decoding are
unnecessary and can be eliminated with minimal impact on performance. To this end, we propose
REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency
in RAG applications. By exploiting this attention sparsity structure, we demonstrate a30.85× the
time-to-first-token acceleration (3.75× improvement to previous work) without loss in perplexity. In
addition, our optimization framework for large context enablesREFRAGto extend the context size",1.0,"The quote is an exact, verbatim sentence extracted directly from the provided source text."
47.0,Hybrid RAG,True,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025). As a result, LLM inference throughput degrades with larger contexts, limiting their applicability in scenarios demanding high throughput and low latency, such as web-scale discovery.",100.0,"The data transfer latency to high-bandwidth memory is2n+4dlb(s+o)
m .
Throughput CalculationThe throughput, defined as the number of tokens generated per unit time, is given
by:
Throughput= bo
TTFT+DL
where DL is the data latency.
Before After
KV cache memory4dlb(s+o) 4dlb
",0.0,"The quote is not present in the provided source text. The source text discusses TTFT and TTIT with formulas and a comparison table, but it does not state that TTFT latency increases quadratically or that TTIT latency grows linearly with prompt length. Furthermore, the citation '(Liu et al., 2025)' is completely absent from the source text."
48.0,Hybrid RAG,True,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19","To address the optimization challenge, we propose employing curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embedding c1 for x1:k and and the decoder reconstructs the k tokens using the projected chunk embedding e_ink.",46.59090909090909,"Stage 1 Stage 3 Stage 5 Stage 7 Stage 9
Training Stage
0
50
100
Percentage
Context
1 × k
2 × k
4 × k
8 × k
16 × k
32 × k
64 × k
128 × k
256 × k
Figure 6The data mixture in curriculum learning during the training.
Factor Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Stage 7 Stage 8 Stage 9 Summation
1×81333 445 148 49 16 6 2 1 0 2000
2×8333 298 267 238 213 191 171 153 137 2000
4×883 102 126 156 193 238 293 362 447 2000
8×820 35 61 106 185 324 565 985 1719 4000
16×85 11 23 48 103 220 468 997 2125 4000
32×81 3 7 19 50 133 353 939 2496 4000
64×81 3 9 25 73 212 618 1802 5259 8000
128×81 3 9 25 73 212 618 1802 5259 8000
256×81 3 9 25 73 212 618 1802 5259 8000
Table 8The geometry curriculum learning scheduling. The whole training is split into 9 stages. In each stage, we
have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).",1.0,"The quote is an exact, verbatim extraction from the source text. It appears as the second sentence in the description of 'Table 8', with the only change being the removal of a line break between 'we' and 'have', which is a standard text normalization and does not alter the content."
49.0,Hybrid RAG,True,"under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",filename: 2509.01092v2.pdf page 7,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.95,"The quote is a near-verbatim extraction from the source text. It accurately reproduces the content, starting mid-sentence and omitting the introductory phrase 'The result demonstrates that' from the original sentence in the source. The rest of the text is identical."
50.0,Hybrid RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
51.0,Hybrid RAG,True,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1","3) Unusually Structured and Sparse Attention. Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated,
resulting in predominantly zero cross-attention between chunks (see figure 7).",100.0,"being uninformative and reused across multiple inferences. Allocating memory/computation for all the
tokens, as we show in this paper, is unnecessarily wasteful. 2)Wasteful Encoding and Other Information.The
retrieval process in RAG has already pre-processed the chunks of the contexts, and their encodings and
other correlations with the query are already available due to the use of vectorizations and re-rankings. This
information is discarded during decoding. 3)Unusually Structured and Sparse Attention.Due to diversity
and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in
predominantly zero cross-attention between chunks (see figure 7).
1.1 Our Contributions
We proposeREFRAG(REpresentation For RAG), a novel mechanism for efficient decoding of contexts
in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. It matches the sentence starting with 'Due to diversity...' under point '3)Unusually Structured and Sparse Attention'."
52.0,Hybrid RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.
This approach offers three main advantages: 1) It shortens the decoder’s input length… 2) It enables reuse of pre-computed chunk embeddings… and
3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
53.0,Hybrid RAG,True,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk i = ϕ(ci).",filename: 2509.01092v2.pdf page 1,"The encoder model then processes all the chunks to obtain a chunk embedding for each chunk ci = Menc(Ci).
This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model,
ecnk_i = ϕ(ci).",98.6013986013986,"foundation model (e.g., LLaMA (Touvron et al., 2023)) and a lightweight encoder model (e.g., Roberta (Liu
et al., 2019)). When given a questionx1, . . . , xq and contextxq+1, . . . , xT and , the context is chunked into
L := s
k number ofk-sized chunks{C1, . . . , CL} where Ci = {xq+k∗i, . . . , xq+k∗i+k−1}. The encoder model then
processes all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci). This chunk embedding
is then projected with a projection layerϕ to match the size of the token embedding of the decoder model,
ecnk
i = ϕ(ci). These projected chunk embeddings are then fed to the decoder model along with the token
embeddings for the question to generate the answery∼ Mdec({e1, . . . ,eq, ecnk
1 , . . . ,ecnk
L })wheree i is the
2",1.0,"The quote is an exact, verbatim sentence extracted from the source text. The minor difference in spacing around 'ϕ' and the line break for 'ecnk i' in the source are formatting artifacts and do not change the content or meaning of the quote."
54.0,Hybrid RAG,True,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput.
For longer context length, acceleration reaches up to k2× for both metrics.",82.08092485549133,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",0.9,"The quote accurately conveys the core information from the source text regarding k2× acceleration for longer context lengths in TTFT and throughput. However, it is not a verbatim extraction. There are several minor changes in phrasing and structure: 'For longer context length' becomes 'With longer context lengths', 'acceleration reaches' is rephrased to 'we are able to achieve up to k2× acceleration', and 'for both metrics' is clarified to 'in both TTFT and throughput'. While the meaning is fully preserved and accurate, these changes prevent it from being considered verbatim or near-verbatim (e.g., 0.95+)."
55.0,Hybrid RAG,True,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,"Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in the decoder.
In this task, we freeze the decoder model and only train the encoder and projection layer.
The main objectives are to align the encoder and projection layer…",100.0,"next-paragraph prediction, thereby equipping it for downstream applications. The objective is to align any
encoder–decoder combination so that the generations produced withcompressed contextclosely resemble
those generated by the original decoder with access to the full context.
3.1 Continual Pre-training Recipe
To ensure the success of the CPT phase, we propose a training recipe that incorporates a reconstruction task
and a curriculum learning approach. Ablation studies in section 4 demonstrate thatthis recipeiscrucialfor
achieving strong CPT performance.
Reconstruction task.We input the firsts tokens x1:s to the encoder and learn to reconstruct tokensx1:s in
the decoder. In this task, wefreeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",0.95,The quote is nearly an exact sentence from the source text. The only difference is a minor spacing issue: the source text has 'wefreeze' while the quote has 'we freeze'. This is a near-verbatim extraction with identical meaning.
56.0,Hybrid RAG,True,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"Source Reference: 2509.01092v2.pdf, Page: 5","Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce perplexity, even for the reconstruction task.
To address the optimization challenge, we propose employing curriculum learning for both tasks.
Curriculum learning incrementally increases task difficulty, enabling the model to gradually and effectively acquire complex skills.",70.34482758620689,"Ablation study result for curriculum learning.Table 11 shows the necessity of curriculum learning to the
success of reconstruction task.
Ablation study result for reconstruction task.Table 12 shows the performance comparison in CPT with and
without continuing from reconstruction task.
22",0.65,"The quote accurately conveys the core meaning of the first sentence in the source text ('Table 11 shows the necessity of curriculum learning to the success of reconstruction task.'). However, it is a paraphrase rather than a verbatim or near-verbatim extraction. The phrase 'Table 11 shows the necessity of' is rephrased as 'The results indicate that ... is essential for', using different words and a different sentence structure."
57.0,Hybrid RAG,True,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,"Selective compression REFRAG introduces selective token compression, expanding important context chunks uncompressed…
A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.
The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks.",100.0,"1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over
time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting
towards those dominated by more difficult tasks (i.e.,L chunk embeddings). A visualization of the data
mixture during curriculum learning is provided in figure 6, and the detailed scheduling is presented in table 8.
Selective compressionREFRAGintroduces selective token compression, expanding important context chunks
uncompressed to improve answer prediction. A RL policy, guided by next-paragraph prediction perplexity as
a negative reward, determines which chunks to retain in their original form. The encoder and decoder are
fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages
chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text. There are no changes, additions, or omissions."
58.0,Hybrid RAG,True,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,"This raises a natural question: does the former approach outperform the latter?
Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.
These results further highlight the effectiveness of the RL-trained policy…",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.0,"The quote contains multiple significant inaccuracies and fabrications not supported by the source text. 
1. The source mentions 'Table 1' and 'Table 2', but there is no 'Table 13'.
2. The source states that 'REFRAG8 and REFRAG16 consistently outperform other baselines', but it does NOT state that 'REFRAG16 consistently outperforms REFRAG8'. In fact, it groups them together as performing well.
3. While 'RL-based selective compression' is mentioned for 'REFRAGRL', the specific phrasing 'REFRAG16 with RL-based selective compression' is not verbatim, and the claim it supports is false.
4. The phrase 'across different datasets' is not explicitly mentioned in relation to REFRAG16 outperforming REFRAG8. 'Context lengths' is mentioned, but again, not in the context of REFRAG16 outperforming REFRAG8.

Due to these major discrepancies and the fabrication of key information, the quote is considered hallucinated and unrelated to the actual content of the source text."
59.0,Hybrid RAG,True,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"With a strong retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26× speedup in TTFT.
At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",95.6896551724138,"0 100 101 102
Number of Passages
50
52
54
56
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
50
52
54
56
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
0 100 101 102
Number of Passages
52
54
Performance
Performance vs. Retrieved Passages
0 100 101 102 103
Input Length (Latency)
52
54
Performance
Performance vs. Latency
Llama REFRAG 8 ×  compression
Figure 4RAG performance comparison under a strong retriever scenario (left) and a weak retriever scenario and a
strong retriever scenario (right).REFRAGperform similarly to LLaMA model under the same retrieved passages
(slightly better in a weaker retriever case) while outperform significantly under the same latency.
This finding is particularly surprising, asREFRAG16 achieves a compression rate of8without recomputing
chunk embeddings, yet still surpasses the performance ofREFRAG8. These results further highlight the",0.0,"The provided quote, 'At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.', is not found anywhere in the source text. The source text discusses REFRAG performance relative to LLaMA under different scenarios and latency, but it does not contain the specific numerical claims about '1.22% average improvement' or the '8 passages for REFRAG vs. 1 for LLaMA' comparison for latency."
60.0,Hybrid RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9","CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention computations.
However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context,
making it unsuitable for tasks such as multi-turn RAG or summarization.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence from the source text. It starts with 'However, CEPE is limited...' and ends with '...or summarization.', matching the source precisely."
61.0,Hybrid RAG,True,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1","This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context.",100.0,"in RAG.REFRAGsignificantly reduces latency, TTFT, and memory usage during decoding, allwithout
requiring modificationsto the LLM architecture or introducing new decoder parameters.
REFRAGmakes several novel modifications to the decoding process: Instead of using tokens from retrieved
passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate represen-
tations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It
shortens the decoder’s input length, improving token allocation efficiency; 2) It enables reuse of pre-computed
chunk embeddings from retrieval, eliminating redundant computation; and 3) It reduces attention computation
complexity, which now scales quadratically with the number of chunks rather than the number of tokens in
the context. Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the third advantage listed in the source text: '3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.'"
62.0,Hybrid RAG,True,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2","Theoretical analysis (appendix A) shows that for short context lengths, our method achieves up to k×
acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k2× for both
metrics. Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG
with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing
CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by perplexity) compared
to CEPE (table 1).",95.73459715639812,"token; TTIT, the time to generate each subsequent token; and Throughput, the number of tokens generated
per unit time. Theoretical analysis (section A) shows that for short context lengths, our method achieves up to
k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both
metrics. Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG
with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE
(2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared
to CEPE (table 1). We achieve up to6.78× throughput acceleration compared to LLaMA, significantly
outperforming CEPE. Withk = 32, TTFT acceleration reaches32.99× compared to LLaMA (3.75× compared
to CEPE) while maintaining similar performance to CEPE (see figure 8 and table 2). More detailed discussion",1.0,"The quote is an exact, verbatim extraction from the source text. It matches the source text word for word, including punctuation and spacing."
63.0,Hybrid RAG,True,"The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases).",filename: 2509.01092v2.pdf page 4,"Figure 3 presents the performance of various methods for selective compression. We expand p fraction of the
chunks in the original token space using the RL policy. The effective compression rate k

1−p+kp decreases when
fewer chunks are compressed (i.e., p increases).",97.02970297029702,"chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases). We compare the perplexity ofxs+1:s+o using different selection
policy under differentp. The perplexity-based selection is an heuristic based selection which compresses
chunks with low perplexity (Perplexity-desc) or high perplexity (Perplexity-asc). The perplexity is measured
by the LLaMA-2-7B model. Intuitively, a chunk with lower perplexity contains less information and can
therefore be compressed with minimal information loss. Ideally, this approach should outperform random
selection, which is indeed observed in figure 3. The RL-based selective compression policy consistently achieves
superior performance across varying compression ratesp.
4.1 Ablation Study
Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while",1.0,"The quote is an exact, verbatim extraction from the source text, including the line break and mathematical notation."
64.0,Hybrid RAG,True,The results indicate that pre-training on the reconstruction task is important for the success of continual pre-training.,"2509.01092v2.pdf, Page: 5","Reconstruction task. We input the first s tokens x1:s to the encoder and learn to reconstruct tokens x1:s in
the decoder. In this task, we freeze the decoder model and only train the encoder and projection layer. The
main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with
minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the
decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",50.413223140495866,"the reconstruction task.
Reconstruction task is essential for the model to learn the continual pre-training task.Table 12 shows the
performance of the continual pre-training task with and without initialization from the reconstruction task.
The results indicate that pre-training on the reconstruction task is important for the success of continual
pre-training.
Advantages of RL-based selective compression.Figure 3 under various compression rates, achieved by varying
the number of chunks to compress (i.e., adjustingp). Notably, a compression rate of8can be obtained either
by configuringREFRAG16 to compress the appropriate number of chunks, or by employingREFRAG8
with full compression, which is natively trained at a compression rate of8. This raises a natural question:
does the former approach outperform the latter? Table 13 demonstrates thatREFRAG16 with RL-based
selective compression consistently outperformsREFRAG8 across different datasets and context lengths.
4 8 16
Compression rate",1.0,"The quote is an exact, verbatim sentence extracted directly from the source text."
65.0,Hybrid RAG,True,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task.","2509.01092v2.pdf, Page: 5","Curriculum learning. The training tasks described in the previous section may seem straightforward, but they
are inherently complex. As the chunk length k increases, the number of possible token combinations expands
exponentially, specifically at a rate of V k, where V is the vocabulary size. Additionally, reconstructing s = k×L tokens
from L chunk embeddings further compounds the difficulty of the task.",50.642201834862384,"within a fixed-length embedding presents a significant challenge. Additionally, reconstructings = k×L tokens
fromLchunk embeddings further compounds the difficulty of the task.
Counterintuitively, directly continuing pre-training of the decoder to utilize encoder outputs did not reduce
perplexity, even for the reconstruction task.To address the optimization challenge, we propose employing
curriculum learning for both tasks. Curriculum learning incrementally increases task difficulty, enabling the
model to gradually and effectively acquire complex skills. For the reconstruction task, training begins with
reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder
reconstructs thek tokens using the projected chunk embeddingecnk
1 . Subsequently, the model reconstructs
x1:2k frome cnk
1 , ecnk
2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",0.0,"The provided quote is not present in the source text. The source text discusses the proposal and implementation of curriculum learning for the reconstruction task, but it does not mention 'performance of the reconstruction task with and without curriculum learning' or state that 'curriculum learning is essential for the success of the reconstruction task' as a result. This quote appears to be a conclusion or result that is not supported by the given excerpt."
66.0,Hybrid RAG,True,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9","CEPE (Yen et al., 2024) employs cross-attention to token embeddings from context tokens, reducing both KV cache
memory and attention computations. However, CEPE is limited to prefix context applications, as it disrupts the causal
structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency.",100.0,"attention, reducing attention complexity from quadratic to linear; however, this method does not address
memory requirements. It is complementary to our approach and can be integrated to further improve latency.
StreamingLLM(Xiao et al., 2024) proposed attention sinks to decrease KV cache memory for long-context
generation, though this does not reduce latency during the pre-filling stage. CEPE (Yen et al., 2024) employs
cross-attention to token embeddings from context tokens, reducing both KV cache memory and attention
computations. However, CEPE is limited to prefix context applications, as it disrupts the causal structure of
the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE
does not utilize token compression, resulting in similar or even increased decoding latency. Concurrently with
our work, Dai et al. (2025) proposed PCC, an embedding-based memory mechanism that summarizes past",1.0,"The quote is an exact, verbatim extraction of a complete sentence found within the provided source text."
67.0,Hybrid RAG,True,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,"Figure 4 compares the performance of REFRAG and the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings. With a strong
retriever and a maximum of 10 passages, REFRAG matches LLaMA’s performance while achieving a 5.26×
speedup in TTFT. At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22%
average improvement across 16 RAG tasks.",100.0,"that most tasks still benefit from more passages in our model. Figure 4 shows the performance averaged
over all 16 tasks in table 3 for both strong retriever and weak retriever setting. The result demonstrates that
under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong
retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model
enables larger context and hence enables extract more useful information when the retrieved passages are less
relevant. Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings
as the saved context can be reinvested to include additional information within the same latency budget.
Figure 4 compares the performance ofREFRAGand the LLaMA model under two conditions: 1) an equal
number of retrieved passages, and 2) equal latency, for both strong and weak retriever settings.With a strong",0.3,"The source text mentions that REFRAG outperforms LLaMA under equivalent latency constraints and that performance is averaged over 16 tasks (referencing Figure 4). This supports the general claim of the quote. However, the specific numerical details regarding the number of passages (8 for REFRAG vs. 1 for LLaMA) and the exact percentage of improvement (1.22%) are not present anywhere in the provided source text. Therefore, the quote is not a verbatim or near-verbatim extraction, as key quantitative information is missing from the source."
68.0,Hybrid RAG,True,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,"Table 2 evaluates o = 2048 with extended context lengths s ∈ {4096, 8192, 16384}. Although our model is
trained on s+ o = 6144, both REFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.",100.0,"Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.
The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via
chunk embeddings, extending context and supporting broader applications.
With a compression rate of16, we achieve a9.3%average log-perplexity improvement over CEPE across
four datasets2. Meanwhile, our method is16.53× faster than LLaMA in TTFT and2.01× faster than CEPE
(section B.4). At a compression rate of32, our log-perplexity matches CEPE, while TTFT acceleration
increases to30.85×over LLaMA and3.75×over CEPE.
Figure 3 presents the performance of various methods for selective compression. We expandp fraction of the
chunks in the original token space using the RL policy. The effective compression ratek
1−p+kp decreases when",1.0,"The quote is an exact sentence from the source text, with only a minor difference in spacing ('a4k' vs 'a 4k') which does not alter the meaning or content. It is considered verbatim."
69.0,Hybrid RAG,True,"To further validate our training approach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14 reports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the Arxiv domain.","2509.01092v2.pdf, Page: 6","Training datasets. We use the Slimpajama dataset (Soboleva et al., 2023), an open source dataset for LLM pre-
training. … We only use the Book and ArXiv domains from the dataset since these two domains contain long
texts (Yen et al., 2024). We sampled from this dataset to construct a 20B token training dataset which
contains 50% data from Arxiv and 50% data from Book.",46.7479674796748,"This observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in
multi-modal models can negatively impact performance when data is scarce. To further validate our training
approach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14
reports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the
Arxiv domain. Models trained with our recipe achieve performance comparable to the Full Context setting
(i.e., without context compression). Moreover, increasing the context length continues to benefit our model,
as evidenced by lower perplexity for a context length of4096compared to2048.
5 Contextual Learning Applications
In this section, we investigate fine-tuning the model obtained from the pre-training stage to address various
downstream tasks, including RAG, long document summarization, and multi-turn conversation with RAG.",1.0,"The quote is an exact, verbatim extraction of a sentence from the source text."
70.0,Hybrid RAG,True,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4","For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.",100.0,"REFRAGRL uses RL-based selective compression.LLaMAK: LLaMA-2-7B evaluated onxs+1:s+o with the
truncated sequencex s−K:T as input to match the token count ofREFRAG.
Table 1 reports performance fors = 2048and o∈ {512, 1024, 2048}, where, e.g., P512 denoteso = 512.
Bolded results compare baselines, excludingLLaMA-Full Contextand LLaMA-32K, which use full
context without compression and are expected to perform best. Notably,REFRAG8 and REFRAG16
consistently outperform other baselines across nearly all settings, while also achieving lower latency than
CEPE (figure 2). For reference,LLaMA256 uses only the last 256 tokens, matching the number of chunk
embeddings inREFRAG8 (s/k = 256), yetREFRAG8 consistently surpassesLLaMA256, demonstrating
the effectiveness of compressed chunk embeddings.
Table 2 evaluateso = 2048with extended context lengthss∈ {4096, 8192, 16384}. Although our model is
trained ons + o = 6144, bothREFRAG8 and REFRAG16 maintain superior performance at longer contexts.",0.99,"The quote is an almost verbatim extraction from the source text. The only difference is an extra space after the comma following 'For reference,' in the quote compared to the source text (Source: 'For reference,LLaMA256' vs. Quote: 'For reference, LLaMA256'). This is a minor formatting difference that does not alter the content or meaning."
