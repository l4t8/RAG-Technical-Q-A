{'input': 'What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?', 'question': 'What is the primary mechanism through which the REFRAG framework achieves a reduction in computational complexity for attention?', 'option_a': 'It eliminates the need for a key-value cache, thus reducing memory requirements.', 'option_b': 'It modifies the core LLM architecture to use a linear attention mechanism instead of a quadratic one.', 'option_c': 'It changes the scaling factor of attention computation from the number of tokens to the number of chunks.', 'option_d': 'It offloads all attention computations to a specialized lightweight encoder model.', 'context': [Document(metadata={'source': '2509.01092v2.pdf', 'page': 23}, page_content='with compressed representation or less information without compression, since correct summarization requires\ncomplete information from the whole document.\nResult analysis.Table 21 shows the performance of different baselines under the same number of tokens in the\ndecoder. REPLUGFT means that we adopt theREPLUG framework usingLLaMAFT, andREPLUGChat\nmeans that we adopt the LLaMA-2-7B-Chat model forREPLUG. We did not report some of our methods for\ncertain decoder token counts since there were not enough input tokens for those compression rates. Our model\nachieves the best performance under the same number of decoder tokens (i.e., same latency). Additionally,\nREFRAG16 performs better thanREFRAG8 at a decoder token count of 128, since the former model is\nable to incorporate more information from the document with a higher compression rate.\n24'), Document(metadata={'source': '2509.01092v2.pdf', 'page': 6}, page_content='model’s capability is significantly reduced.\nDifferent combinations of encoder and decoder models for REFRAG.We employ LLaMA-2-7B and LLaMA-2-13B\nas decoders, and RoBERTa-Base and RoBERTa-Large as encoders, to investigate how model performance\nvaries with different encoder and decoder sizes. Figure 11 presents results for various encoder-decoder\ncombinations. We observe that increasing the number of parameters in the decoder leads to a substantial\nreduction in loss, whereas enlarging the encoder yields only a modest improvement. This discrepancy may\nbe attributed to the relatively minor increase in size from RoBERTa-Base to RoBERTa-Large compared to\nthe substantial jump from 7B to 13B in the decoder. Additional results in figure 12 indicate that a larger\nencoder may not always be advantageous when training with limited data in the continual pre-training setting.\nThis observation aligns with previous findings by Li et al. (2024), which demonstrate that larger encoders in'), Document(metadata={'source': '2509.01092v2.pdf', 'page': 10}, page_content='7 Conclusion\nIn this work, we introducedREFRAG, a novel and efficient decoding framework tailored for RAG applications.\nBy leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts,REFRAG\ncompresses, senses, and expands context representations to significantly reduce both memory usage and\ninference latency, particularly the TTFT. Extensive experiments across a range of long-context applications,\nincluding RAG, multi-turn conversations, and long document summarization, demonstrate thatREFRAG\nachieves up to30.85× TTFT acceleration (3.75× over previous state-of-the-art methods) without any loss in\nperplexity or downstream accuracy. Our results highlight the importance of specialized treatment for RAG-\nbased systems and open new directions for efficient large-context LLM inference. We believe thatREFRAG\nprovides a practical and scalable solution for deploying LLMs in latency-sensitive, knowledge-intensive\napplications.\n8 Acknowledgements'), Document(metadata={'source': '2509.01092v2.pdf', 'page': 1}, page_content='ranging from modifying the attention mechanism’s complexity (Beltagy et al., 2020) to sparsifying attention\nand context (Child et al., 2019; Xiao et al., 2024; Jiang et al., 2024), and altering context feeding strategies (Yen\net al., 2024). However, most existing methods target generic LLM tasks with long context and are largely\northogonal to our work. This paper focuses on RAG-based applications, such as web-scale search, with the\ngoal of improving inference latency, specifically, the TTFT. We argue that specialized techniques exploiting the\nunique structure and sparsity inherent in RAG contexts can substantially reduce memory and computational\noverhead. Treating RAG TTFT as a generic LLM inference problem overlooks several key aspects: 1)\nInefficient Token Allocation.RAG contexts often contain sparse information, with many retrieved passages\nbeing uninformative and reused across multiple inferences. Allocating memory/computation for all the')], 'answer': '```json\n{\n    "answer": "C",\n    "reasoning": "The REFRAG framework achieves a reduction in computational complexity by \'leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts\'. This means that instead of computing attention over all tokens in a dense manner, it exploits the structure where attention is primarily focused within specific blocks or \'chunks\' of information. This effectively changes the scaling factor of the attention computation from the total number of tokens to a function of the number of relevant blocks or chunks, thereby reducing computational overhead and inference latency.",\n    "source": "filename: 7 Conclusion"\n}\n```'}
