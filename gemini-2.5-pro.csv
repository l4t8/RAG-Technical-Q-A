question_number,pipeline,correct,predicted,ground_truth,quote,source,latency,run_timestamp
1.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.179107904434204,2025-12-02 17:03:05
2.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.50856065750122,2025-12-02 17:03:05
3.0,Baseline (Zero-shot),False,A,B,No quote provided,,19.660865783691406,2025-12-02 17:03:05
4.0,Baseline (Zero-shot),True,D,D,No quote provided,,15.66737151145935,2025-12-02 17:03:05
5.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.58238434791565,2025-12-02 17:03:05
6.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.817792177200317,2025-12-02 17:03:05
7.0,Baseline (Zero-shot),False,A,C,No quote provided,,15.192317008972168,2025-12-02 17:03:05
8.0,Baseline (Zero-shot),True,B,B,No quote provided,,22.90101432800293,2025-12-02 17:03:05
9.0,Baseline (Zero-shot),True,C,C,No quote provided,,41.16489768028259,2025-12-02 17:03:05
10.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.974069118499756,2025-12-02 17:03:05
11.0,Baseline (Zero-shot),True,C,C,No quote provided,,22.018901348114014,2025-12-02 17:03:05
12.0,Baseline (Zero-shot),True,C,C,No quote provided,,28.055509567260746,2025-12-02 17:03:05
13.0,Baseline (Zero-shot),False,A,C,No quote provided,,76.80008554458618,2025-12-02 17:03:05
14.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.41288924217224,2025-12-02 17:03:05
15.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.68061375617981,2025-12-02 17:03:05
16.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.969125509262085,2025-12-02 17:03:05
17.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.587204217910767,2025-12-02 17:03:05
18.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.196716785430908,2025-12-02 17:03:05
19.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.052164554595947,2025-12-02 17:03:05
20.0,Baseline (Zero-shot),False,A,C,No quote provided,,22.578856229782104,2025-12-02 17:03:05
21.0,Baseline (Zero-shot),True,C,C,No quote provided,,45.7730438709259,2025-12-02 17:03:05
22.0,Baseline (Zero-shot),False,A,C,No quote provided,,70.6562008857727,2025-12-02 17:03:05
23.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.3522629737854,2025-12-02 17:03:05
24.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.802765607833862,2025-12-02 17:03:05
25.0,Baseline (Zero-shot),False,A,C,No quote provided,,23.211753129959103,2025-12-02 17:03:05
26.0,Baseline (Zero-shot),False,A,C,No quote provided,,14.588090419769289,2025-12-02 17:03:05
27.0,Baseline (Zero-shot),False,A,C,No quote provided,,16.445321083068848,2025-12-02 17:03:05
28.0,Baseline (Zero-shot),False,A,C,No quote provided,,20.57609605789185,2025-12-02 17:03:05
29.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.996042728424072,2025-12-02 17:03:05
30.0,Baseline (Zero-shot),False,A,C,No quote provided,,21.096905946731567,2025-12-02 17:03:05
31.0,Baseline (Zero-shot),False,A,C,No quote provided,,25.46550154685974,2025-12-02 17:03:05
32.0,Baseline (Zero-shot),True,C,C,No quote provided,,28.297963619232178,2025-12-02 17:03:05
33.0,Baseline (Zero-shot),True,C,C,No quote provided,,22.73003387451172,2025-12-02 17:03:05
34.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.04793167114257,2025-12-02 17:03:05
35.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.587315559387207,2025-12-02 17:03:05
36.0,Baseline (Zero-shot),True,C,C,No quote provided,,22.120845079422,2025-12-02 17:03:05
37.0,Baseline (Zero-shot),False,B,C,No quote provided,,16.791579723358154,2025-12-02 17:03:05
38.0,Baseline (Zero-shot),True,C,C,No quote provided,,60.62325549125672,2025-12-02 17:03:05
39.0,Baseline (Zero-shot),True,C,C,No quote provided,,46.32962989807129,2025-12-02 17:03:05
40.0,Baseline (Zero-shot),True,C,C,No quote provided,,13.572023868560793,2025-12-02 17:03:05
41.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.590946197509766,2025-12-02 17:03:05
42.0,Baseline (Zero-shot),True,B,B,No quote provided,,16.5876522064209,2025-12-02 17:03:05
43.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.898157358169556,2025-12-02 17:03:05
44.0,Baseline (Zero-shot),False,A,C,No quote provided,,25.18939518928528,2025-12-02 17:03:05
45.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.060976028442383,2025-12-02 17:03:05
46.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.109087228775024,2025-12-02 17:03:05
47.0,Baseline (Zero-shot),False,A,B,No quote provided,,22.42534875869751,2025-12-02 17:03:05
48.0,Baseline (Zero-shot),True,B,B,No quote provided,,14.643606662750244,2025-12-02 17:03:05
49.0,Baseline (Zero-shot),True,C,C,No quote provided,,32.66589117050171,2025-12-02 17:03:05
50.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.589017629623413,2025-12-02 17:03:05
51.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.19639086723328,2025-12-02 17:03:05
52.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.4319851398468,2025-12-02 17:03:05
53.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.055500268936155,2025-12-02 17:03:05
54.0,Baseline (Zero-shot),False,A,C,No quote provided,,48.53509449958801,2025-12-02 17:03:05
55.0,Baseline (Zero-shot),False,A,C,No quote provided,,24.576022148132324,2025-12-02 17:03:05
56.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.78759980201721,2025-12-02 17:03:05
57.0,Baseline (Zero-shot),False,A,C,No quote provided,,26.21433019638061,2025-12-02 17:03:05
58.0,Baseline (Zero-shot),False,A,C,No quote provided,,22.118804216384888,2025-12-02 17:03:05
59.0,Baseline (Zero-shot),True,C,C,No quote provided,,52.22575664520264,2025-12-02 17:03:05
60.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.04490685462952,2025-12-02 17:03:05
61.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.124847888946533,2025-12-02 17:03:05
62.0,Baseline (Zero-shot),True,C,C,No quote provided,,36.60494947433472,2025-12-02 17:03:05
63.0,Baseline (Zero-shot),True,C,C,No quote provided,,24.83092832565308,2025-12-02 17:03:05
64.0,Baseline (Zero-shot),False,A,C,No quote provided,,36.563539028167725,2025-12-02 17:03:05
65.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.379910945892334,2025-12-02 17:03:05
66.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.3986554145813,2025-12-02 17:03:05
67.0,Baseline (Zero-shot),True,C,C,No quote provided,,32.86777067184448,2025-12-02 17:03:05
68.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.814502477645874,2025-12-02 17:03:05
69.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.73862719535828,2025-12-02 17:03:05
70.0,Baseline (Zero-shot),True,C,C,No quote provided,,31.868873357772827,2025-12-02 17:03:05
1.0,BM25 RAG,True,C,C,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts, REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.","2509.01092v2.pdf, Page: 10",21.574841499328613,2025-12-02 17:24:04
2.0,BM25 RAG,False,A,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly).","2509.01092v2.pdf, Page: 5",16.59390950202942,2025-12-02 17:24:04
3.0,BM25 RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,16.888609647750854,2025-12-02 17:24:04
4.0,BM25 RAG,True,D,D,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.",filename: 2509.01092v2.pdf page 7,15.97303295135498,2025-12-02 17:24:04
5.0,BM25 RAG,True,C,C,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.",filename: 2509.01092v2.pdf page 1,31.64019799232483,2025-12-02 17:24:04
6.0,BM25 RAG,True,C,C,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.",filename: 2509.01092v2.pdf page 4,12.28768277168274,2025-12-02 17:24:04
7.0,BM25 RAG,True,C,C,"...a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",12.901953220367432,2025-12-02 17:24:04
8.0,BM25 RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",15.343577146530151,2025-12-02 17:24:04
9.0,BM25 RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",filename: 2509.01092v2.pdf page 7,12.918832063674929,2025-12-02 17:24:04
10.0,BM25 RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,14.129066944122314,2025-12-02 17:24:04
11.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,15.052400588989258,2025-12-02 17:24:04
12.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.13232707977295,2025-12-02 17:24:04
13.0,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",13.78185749053955,2025-12-02 17:24:04
14.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form... The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...",filename: 2509.01092v2.pdf page 3,18.06386590003968,2025-12-02 17:24:04
15.0,BM25 RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,13.30970025062561,2025-12-02 17:24:04
16.0,BM25 RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3",19.35283207893372,2025-12-02 17:24:04
17.0,BM25 RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"Source Reference: 2509.01092v2.pdf, Page: 5",14.438718795776367,2025-12-02 17:24:04
18.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","Source Reference: 2509.01092v2.pdf, Page: 9",20.27389883995056,2025-12-02 17:24:04
19.0,BM25 RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",16.281574487686157,2025-12-02 17:24:04
20.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,11.906551361083984,2025-12-02 17:24:04
21.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.37177300453186,2025-12-02 17:24:04
22.0,BM25 RAG,True,C,C,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,12.427669525146484,2025-12-02 17:24:04
23.0,BM25 RAG,True,C,C,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications.",filename: 2509.01092v2.pdf page 1,14.745120286941528,2025-12-02 17:24:04
24.0,BM25 RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,13.210476160049438,2025-12-02 17:24:04
25.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",13.576685190200806,2025-12-02 17:24:04
26.0,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache...",filename: 2509.01092v2.pdf page 2,18.989648580551147,2025-12-02 17:24:04
27.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",16.534199476242065,2025-12-02 17:24:04
28.0,BM25 RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,12.367456197738647,2025-12-02 17:24:04
29.0,BM25 RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,17.7964928150177,2025-12-02 17:24:04
30.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",11.982027530670166,2025-12-02 17:24:04
31.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.059861660003662,2025-12-02 17:24:04
32.0,BM25 RAG,True,C,C,"Table 6 shows that with short context length s we are able to achieve k× acceleration in TTFT and up to k× acceleration in throughput. With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",14.132452726364136,2025-12-02 17:24:04
33.0,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).","2509.01092v2.pdf, Page: 2",13.825227737426758,2025-12-02 17:24:04
34.0,BM25 RAG,True,C,C,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","2509.01092v2.pdf, Page: 3",37.35052108764648,2025-12-02 17:24:04
35.0,BM25 RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk... Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,20.37289762496948,2025-12-02 17:24:04
36.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","Source Reference: 2509.01092v2.pdf, Page: 3",28.864025592803955,2025-12-02 17:24:04
37.0,BM25 RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",11.41406774520874,2025-12-02 17:24:04
38.0,BM25 RAG,False,ERROR,C,No quote provided,,47.92555379867554,2025-12-02 17:24:04
39.0,BM25 RAG,True,C,C,We follow the work of Linetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,15.977759599685667,2025-12-02 17:24:04
40.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,15.971615552902222,2025-12-02 17:24:04
41.0,BM25 RAG,True,C,C,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9",21.45688223838806,2025-12-02 17:24:04
42.0,BM25 RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,16.330153465270996,2025-12-02 17:24:04
43.0,BM25 RAG,True,C,C,We employ RL to train a policy that optimally determines which segments to compress.,"2509.01092v2.pdf, Page: 15",21.621522665023804,2025-12-02 17:24:04
44.0,BM25 RAG,False,B,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",22.00243139266968,2025-12-02 17:24:04
45.0,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,14.437816858291626,2025-12-02 17:24:04
46.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",17.63939332962036,2025-12-02 17:24:04
47.0,BM25 RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,10.976104736328123,2025-12-02 17:24:04
48.0,BM25 RAG,True,B,B,Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,filename: 2509.01092v2.pdf page 21,15.0076322555542,2025-12-02 17:24:04
49.0,BM25 RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","Source Reference: 2509.01092v2.pdf, Page: 7",20.58275532722473,2025-12-02 17:24:04
50.0,BM25 RAG,False,A,C,"attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements. It is complementary to our approach and can be integrated to further improve latency.","2509.01092v2.pdf, Page: 9",19.4294958114624,2025-12-02 17:24:04
51.0,BM25 RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1",15.287999868392944,2025-12-02 17:24:04
52.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,13.513782262802124,2025-12-02 17:24:04
53.0,BM25 RAG,True,C,C,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk
i = ϕ(ci).","2509.01092v2.pdf, Page: 1",12.287761449813845,2025-12-02 17:24:04
54.0,BM25 RAG,True,C,C,"For longer context length, acceleration reaches up to k²× for both metrics.","2509.01092v2.pdf, Page: 2",11.059167861938477,2025-12-02 17:24:04
55.0,BM25 RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,11.878539323806764,2025-12-02 17:24:04
56.0,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,filename: 2509.01092v2.pdf page 5,15.974553346633911,2025-12-02 17:24:04
57.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,17.04495120048523,2025-12-02 17:24:04
58.0,BM25 RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",14.391748189926147,2025-12-02 17:24:04
59.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",13.824200868606567,2025-12-02 17:24:04
60.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,13.824052095413208,2025-12-02 17:24:04
61.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.748756170272827,2025-12-02 17:24:04
62.0,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,12.167585611343384,2025-12-02 17:24:04
63.0,BM25 RAG,True,C,C,"The effective compression rate k
1−p+kp decreases when",filename: 2509.01092v2.pdf page 4,11.790778636932371,2025-12-02 17:24:04
,,False,,,,,,2025-12-02 17:24:04
65.0,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"2509.01092v2.pdf, Page: 5",28.24225878715515,2025-12-02 17:24:04
66.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",12.378402709960938,2025-12-02 17:24:04
67.0,BM25 RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",26.41956615447998,2025-12-02 17:24:04
68.0,BM25 RAG,False,B,C,"REFRAG: Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o.","Source Reference: 2509.01092v2.pdf, Page: 17",20.88770031929016,2025-12-02 17:24:04
69.0,BM25 RAG,False,ERROR,C,No quote provided,,48.84510326385498,2025-12-02 17:24:04
70.0,BM25 RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,15.009690761566162,2025-12-02 17:24:04
1.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.016928672790527,2025-12-02 17:48:14
2.0,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",21.81468796730041,2025-12-02 17:48:14
3.0,Dense RAG,False,C,B,"Both higher than CEPE (i.e., 2.01× and 1.04× acceleration respectively) while having better model performance (see table 1).","2509.01092v2.pdf, Page: 15",48.42552757263184,2025-12-02 17:48:14
4.0,Dense RAG,True,D,D,This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.,"2509.01092v2.pdf, Page: 7",21.61267924308777,2025-12-02 17:48:14
5.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",27.957314491271973,2025-12-02 17:48:14
,,False,,,,,,2025-12-02 17:48:14
7.0,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",15.666388034820557,2025-12-02 17:48:14
8.0,Dense RAG,True,B,B,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",15.973538637161257,2025-12-02 17:48:14
9.0,Dense RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",17.509703397750854,2025-12-02 17:48:14
10.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,11.98017120361328,2025-12-02 17:48:14
11.0,Dense RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",12.594558715820312,2025-12-02 17:48:14
12.0,Dense RAG,False,A,C,"attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements.",filename: 2509.01092v2.pdf page 9,21.19603943824768,2025-12-02 17:48:14
13.0,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,14.1303071975708,2025-12-02 17:48:14
14.0,Dense RAG,True,C,C,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, Page: 3",15.258558511734009,2025-12-02 17:48:14
15.0,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. [...] The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,20.68261480331421,2025-12-02 17:48:14
16.0,Dense RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,21.502132177352905,2025-12-02 17:48:14
17.0,Dense RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,19.97042441368103,2025-12-02 17:48:14
18.0,Dense RAG,True,C,C,"our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",65.4276876449585,2025-12-02 17:48:14
19.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",13.826541900634766,2025-12-02 17:48:14
20.0,Dense RAG,True,C,C,The input context is chunked and processed by the light-weight encoder to produce chunk embeddings... These chunk embeddings along with the token embeddings of the question input are fed to the decoder.,"2509.01092v2.pdf, Page: 2",25.800743579864506,2025-12-02 17:48:14
21.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,10.977477073669434,2025-12-02 17:48:14
22.0,Dense RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",12.983641386032104,2025-12-02 17:48:14
23.0,Dense RAG,True,C,C,These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.,"2509.01092v2.pdf, Page: 6",22.42428040504456,2025-12-02 17:48:14
24.0,Dense RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,20.243279695510864,2025-12-02 17:48:14
25.0,Dense RAG,True,C,C,Reward = - Log(Perplexity),"2509.01092v2.pdf, Page: 17",13.24046540260315,2025-12-02 17:48:14
26.0,Dense RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...","2509.01092v2.pdf, Page: 2",14.727595090866089,2025-12-02 17:48:14
27.0,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,14.455180168151855,2025-12-02 17:48:14
28.0,Dense RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",32.25513029098511,2025-12-02 17:48:14
29.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,15.305373668670654,2025-12-02 17:48:14
30.0,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",19.71769499778748,2025-12-02 17:48:14
31.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,16.380649089813232,2025-12-02 17:48:14
32.0,Dense RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",16.076308250427246,2025-12-02 17:48:14
33.0,Dense RAG,True,C,C,"Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves 16.53× acceleration in TTFT with cache and 8.59× without cache.",filename: 2509.01092v2.pdf page 15,14.539921045303345,2025-12-02 17:48:14
34.0,Dense RAG,False,ERROR,C,No quote provided,,76.96307158470154,2025-12-02 17:48:14
35.0,Dense RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,21.045095443725582,2025-12-02 17:48:14
36.0,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",29.48262119293213,2025-12-02 17:48:14
37.0,Dense RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,12.148813009262083,2025-12-02 17:48:14
38.0,Dense RAG,False,,C,"Not applicable, as no supporting evidence was found in the context.",Not applicable.,26.24618029594421,2025-12-02 17:48:14
39.0,Dense RAG,True,C,C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,17.227094173431396,2025-12-02 17:48:14
40.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.108783960342407,2025-12-02 17:48:14
41.0,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",12.181846618652344,2025-12-02 17:48:14
42.0,Dense RAG,True,B,B,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",22.52753496170044,2025-12-02 17:48:14
43.0,Dense RAG,True,C,C,REFRAGRL uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4",14.658833026885986,2025-12-02 17:48:14
44.0,Dense RAG,False,D,C,"Notably, REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4",16.932452917099,2025-12-02 17:48:14
45.0,Dense RAG,True,C,C,"This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the performance of REFRAG8.",filename: 2509.01092v2.pdf page 6,15.101384162902832,2025-12-02 17:48:14
46.0,Dense RAG,False,D,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",23.05578064918518,2025-12-02 17:48:14
47.0,Dense RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,14.425376176834106,2025-12-02 17:48:14
48.0,Dense RAG,True,B,B,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",15.717730522155762,2025-12-02 17:48:14
49.0,Dense RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",filename: 2509.01092v2.pdf page 7,27.29338502883911,2025-12-02 17:48:14
50.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",10.74449872970581,2025-12-02 17:48:14
51.0,Dense RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",filename: 2509.01092v2.pdf page 1,13.649616479873655,2025-12-02 17:48:14
52.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.771034240722656,2025-12-02 17:48:14
53.0,Dense RAG,True,C,C,"REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: ... 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",46.95270562171936,2025-12-02 17:48:14
54.0,Dense RAG,True,C,C,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,13.235216617584229,2025-12-02 17:48:14
55.0,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,12.8122239112854,2025-12-02 17:48:14
56.0,Dense RAG,True,C,C,"Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported
as average of Arxiv and Book domain.
...
REFRAGw/o curriculum 3.719 3.098 2.272 1.599
REFRAGwith curriculum 0.669 0.451 0.230 0.135","2509.01092v2.pdf, Page: 25",20.38652968406677,2025-12-02 17:48:14
57.0,Dense RAG,False,A,C,We compare the perplexity of xs+1:s+o using different selection policy under different p.,"2509.01092v2.pdf, Page: 4",21.502865076065063,2025-12-02 17:48:14
58.0,Dense RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,13.2085862159729,2025-12-02 17:48:14
59.0,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,16.283151865005493,2025-12-02 17:48:14
60.0,Dense RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,11.36487102508545,2025-12-02 17:48:14
61.0,Dense RAG,True,C,C,"the context is the dominating part of the input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k .","2509.01092v2.pdf, Page: 2",16.280967950820923,2025-12-02 17:48:14
62.0,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",15.360061407089232,2025-12-02 17:48:14
63.0,Dense RAG,True,C,C,"The effective compression rate k / (1−p+kp) decreases when fewer chunks are compressed (i.e.,p increases).",filename: 2509.01092v2.pdf page 4,10.751355409622192,2025-12-02 17:48:14
64.0,Dense RAG,False,INFORMATION NOT AVAILABLE IN CONTEXT,C,,,34.40594935417175,2025-12-02 17:48:14
65.0,Dense RAG,True,C,C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",52.83854007720947,2025-12-02 17:48:14
66.0,Dense RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,12.59441089630127,2025-12-02 17:48:14
67.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",20.27465295791626,2025-12-02 17:48:14
68.0,Dense RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,13.21183705329895,2025-12-02 17:48:14
69.0,Dense RAG,False,ERROR,C,No quote provided,,44.54114890098572,2025-12-02 17:48:14
70.0,Dense RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,14.130982637405396,2025-12-02 17:48:14
1.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.237319707870483,2025-12-02 20:39:50
2.0,Baseline (Zero-shot),False,A,C,No quote provided,,15.374397993087769,2025-12-02 20:39:50
3.0,Baseline (Zero-shot),False,C,B,No quote provided,,24.532078981399536,2025-12-02 20:39:50
4.0,Baseline (Zero-shot),True,D,D,No quote provided,,15.110836505889893,2025-12-02 20:39:50
5.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.06275081634521,2025-12-02 20:39:50
6.0,Baseline (Zero-shot),False,A,C,No quote provided,,16.89988660812378,2025-12-02 20:39:50
7.0,Baseline (Zero-shot),False,A,C,No quote provided,,18.431761980056763,2025-12-02 20:39:50
8.0,Baseline (Zero-shot),True,B,B,No quote provided,,21.51600289344788,2025-12-02 20:39:50
9.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.498174905776978,2025-12-02 20:39:50
10.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.052483797073364,2025-12-02 20:39:50
11.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.27530217170716,2025-12-02 20:39:50
12.0,Baseline (Zero-shot),True,C,C,No quote provided,,38.29514813423157,2025-12-02 20:39:50
13.0,Baseline (Zero-shot),False,A,C,No quote provided,,123.33400774002077,2025-12-02 20:39:50
14.0,Baseline (Zero-shot),True,C,C,No quote provided,,22.384976148605347,2025-12-02 20:39:50
15.0,Baseline (Zero-shot),False,A,C,No quote provided,,31.64015388488769,2025-12-02 20:39:50
16.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.1247193813324,2025-12-02 20:39:50
17.0,Baseline (Zero-shot),False,A,C,No quote provided,,24.67987585067749,2025-12-02 20:39:50
18.0,Baseline (Zero-shot),True,C,C,No quote provided,,26.622366666793823,2025-12-02 20:39:50
19.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.205604076385498,2025-12-02 20:39:50
20.0,Baseline (Zero-shot),False,A,C,No quote provided,,22.422532081604004,2025-12-02 20:39:50
21.0,Baseline (Zero-shot),True,C,C,No quote provided,,43.56861233711243,2025-12-02 20:39:50
22.0,Baseline (Zero-shot),False,A,C,No quote provided,,58.42149996757507,2025-12-02 20:39:50
23.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.203542947769165,2025-12-02 20:39:50
24.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.361141204833984,2025-12-02 20:39:50
25.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.53419256210327,2025-12-02 20:39:50
26.0,Baseline (Zero-shot),False,A,C,No quote provided,,14.719824314117432,2025-12-02 20:39:50
27.0,Baseline (Zero-shot),False,A,C,No quote provided,,15.66734743118286,2025-12-02 20:39:50
28.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.66080355644226,2025-12-02 20:39:50
29.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.295620441436768,2025-12-02 20:39:50
30.0,Baseline (Zero-shot),False,A,C,No quote provided,,22.10567808151245,2025-12-02 20:39:50
31.0,Baseline (Zero-shot),False,A,C,No quote provided,,23.9599769115448,2025-12-02 20:39:50
32.0,Baseline (Zero-shot),False,A,C,No quote provided,,21.810955047607425,2025-12-02 20:39:50
33.0,Baseline (Zero-shot),True,C,C,No quote provided,,28.879018306732178,2025-12-02 20:39:50
34.0,Baseline (Zero-shot),False,A,C,No quote provided,,18.73670506477356,2025-12-02 20:39:50
35.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.817639350891113,2025-12-02 20:39:50
36.0,Baseline (Zero-shot),True,C,C,No quote provided,,23.87133550643921,2025-12-02 20:39:50
37.0,Baseline (Zero-shot),False,B,C,No quote provided,,16.00235152244568,2025-12-02 20:39:50
38.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.036304235458374,2025-12-02 20:39:50
39.0,Baseline (Zero-shot),True,C,C,No quote provided,,30.72023034095764,2025-12-02 20:39:50
40.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.43191885948181,2025-12-02 20:39:50
41.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.778002977371216,2025-12-02 20:39:50
42.0,Baseline (Zero-shot),True,B,B,No quote provided,,15.477879285812378,2025-12-02 20:39:50
43.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.510050058364868,2025-12-02 20:39:50
44.0,Baseline (Zero-shot),False,A,C,No quote provided,,27.033312797546387,2025-12-02 20:39:50
45.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.817701816558838,2025-12-02 20:39:50
46.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.76469349861145,2025-12-02 20:39:50
47.0,Baseline (Zero-shot),False,A,B,No quote provided,,20.461433172225952,2025-12-02 20:39:50
48.0,Baseline (Zero-shot),True,B,B,No quote provided,,16.762300968170166,2025-12-02 20:39:50
49.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.155415058135983,2025-12-02 20:39:50
50.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.087785482406616,2025-12-02 20:39:50
51.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.588658809661865,2025-12-02 20:39:50
52.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.967798948287964,2025-12-02 20:39:50
53.0,Baseline (Zero-shot),True,C,C,No quote provided,,14.143659114837646,2025-12-02 20:39:50
54.0,Baseline (Zero-shot),False,A,C,No quote provided,,54.26078057289124,2025-12-02 20:39:50
55.0,Baseline (Zero-shot),False,A,C,No quote provided,,18.53435611724853,2025-12-02 20:39:50
56.0,Baseline (Zero-shot),True,C,C,No quote provided,,14.723275184631348,2025-12-02 20:39:50
57.0,Baseline (Zero-shot),False,A,C,No quote provided,,23.6747465133667,2025-12-02 20:39:50
58.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.842944860458374,2025-12-02 20:39:50
59.0,Baseline (Zero-shot),False,A,C,No quote provided,,18.41314125061035,2025-12-02 20:39:50
60.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.274833917617798,2025-12-02 20:39:50
61.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.20297861099243,2025-12-02 20:39:50
62.0,Baseline (Zero-shot),False,A,C,No quote provided,,42.3942461013794,2025-12-02 20:39:50
63.0,Baseline (Zero-shot),True,C,C,No quote provided,,24.969614028930664,2025-12-02 20:39:50
64.0,Baseline (Zero-shot),False,A,C,No quote provided,,13.084659337997437,2025-12-02 20:39:50
65.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.54805278778076,2025-12-02 20:39:50
66.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.974496603012083,2025-12-02 20:39:50
67.0,Baseline (Zero-shot),True,C,C,No quote provided,,29.82122278213501,2025-12-02 20:39:50
68.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.974115133285522,2025-12-02 20:39:50
69.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.817413568496704,2025-12-02 20:39:50
70.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.11081886291504,2025-12-02 20:39:50
1.0,BM25 RAG,True,C,C,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts, REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.",filename: 2509.01092v2.pdf page 10,21.489235639572144,2025-12-02 20:59:18
2.0,BM25 RAG,False,A,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly).","2509.01092v2.pdf, Page: 5",16.28182578086853,2025-12-02 20:59:18
3.0,BM25 RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,16.282176971435547,2025-12-02 20:59:18
4.0,BM25 RAG,True,D,D,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.",filename: 2509.01092v2.pdf page 7,12.606307983398438,2025-12-02 20:59:18
5.0,BM25 RAG,True,C,C,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.",filename: 2509.01092v2.pdf page 1,22.195746898651123,2025-12-02 20:59:18
6.0,BM25 RAG,True,C,C,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",12.529492139816284,2025-12-02 20:59:18
7.0,BM25 RAG,True,C,C,"...a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",12.90338945388794,2025-12-02 20:59:18
8.0,BM25 RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,13.789143800735474,2025-12-02 20:59:18
9.0,BM25 RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",filename: 2509.01092v2.pdf page 7,12.327297449111938,2025-12-02 20:59:18
10.0,BM25 RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,12.453721284866331,2025-12-02 20:59:18
11.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,14.185211658477783,2025-12-02 20:59:18
12.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",12.993001461029053,2025-12-02 20:59:18
13.0,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,15.975996971130373,2025-12-02 20:59:18
14.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...",filename: 2509.01092v2.pdf page 3,21.505757331848145,2025-12-02 20:59:18
15.0,BM25 RAG,True,C,C,"In this task, wefreeze the decoder model and only train the encoder and projection layer.","Source Reference: 2509.01092v2.pdf, Page: 3",15.671408653259276,2025-12-02 20:59:18
16.0,BM25 RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3",17.50943422317505,2025-12-02 20:59:18
17.0,BM25 RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,13.519834280014038,2025-12-02 20:59:18
18.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","Source Reference: 2509.01092v2.pdf, Page: 9",16.79521632194519,2025-12-02 20:59:18
19.0,BM25 RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,14.539571046829224,2025-12-02 20:59:18
20.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,12.288787603378296,2025-12-02 20:59:18
21.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",16.260430574417114,2025-12-02 20:59:18
22.0,BM25 RAG,True,C,C,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,11.04968523979187,2025-12-02 20:59:18
23.0,BM25 RAG,True,C,C,"REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",22.41762137413025,2025-12-02 20:59:18
24.0,BM25 RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,13.822295188903809,2025-12-02 20:59:18
25.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",12.900790214538574,2025-12-02 20:59:18
26.0,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...",filename: 2509.01092v2.pdf page 2,22.11264991760254,2025-12-02 20:59:18
27.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",15.049911260604858,2025-12-02 20:59:18
28.0,BM25 RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,11.67171573638916,2025-12-02 20:59:18
29.0,BM25 RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,16.287147760391235,2025-12-02 20:59:18
30.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,15.442825317382812,2025-12-02 20:59:18
31.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,10.666122198104858,2025-12-02 20:59:18
32.0,BM25 RAG,True,C,C,"Table 6 shows that with short context length s we are able to achieve k× acceleration in TTFT and up to k× acceleration in throughput. With longer context lengths, we are able to achieve up to k2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,11.568864822387695,2025-12-02 20:59:18
33.0,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1...",filename: 2509.01092v2.pdf page 2,12.392146110534668,2025-12-02 20:59:18
34.0,BM25 RAG,True,C,C,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","2509.01092v2.pdf, Page: 3",54.98484611511231,2025-12-02 20:59:18
35.0,BM25 RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk... Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,18.769466400146484,2025-12-02 20:59:18
36.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",16.864969730377197,2025-12-02 20:59:18
37.0,BM25 RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",10.446492195129396,2025-12-02 20:59:18
38.0,BM25 RAG,False,,C,,,19.08850240707397,2025-12-02 20:59:18
39.0,BM25 RAG,True,C,C,We follow the work of Linetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,19.02084708213806,2025-12-02 20:59:18
40.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,13.827250003814695,2025-12-02 20:59:18
41.0,BM25 RAG,True,C,C,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9",15.361876726150513,2025-12-02 20:59:18
42.0,BM25 RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,14.977553129196169,2025-12-02 20:59:18
43.0,BM25 RAG,True,C,C,We employ RL to train a policy that optimally determines which segments to compress.,"Source Reference: 2509.01092v2.pdf, Page: 15",16.976608991622925,2025-12-02 20:59:18
44.0,BM25 RAG,False,B,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",19.6604597568512,2025-12-02 20:59:18
45.0,BM25 RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,20.85120701789856,2025-12-02 20:59:18
46.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",18.113757133483887,2025-12-02 20:59:18
47.0,BM25 RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,12.249036312103271,2025-12-02 20:59:18
48.0,BM25 RAG,True,B,B,Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,filename: 2509.01092v2.pdf page 21,13.86498475074768,2025-12-02 20:59:18
49.0,BM25 RAG,True,C,C,"under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","Source Reference: 2509.01092v2.pdf, Page: 7",15.297377347946169,2025-12-02 20:59:18
50.0,BM25 RAG,False,A,C,"This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence.","2509.01092v2.pdf, Page: 2",16.512475967407227,2025-12-02 20:59:18
51.0,BM25 RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",filename: 2509.01092v2.pdf page 1,14.599984884262083,2025-12-02 20:59:18
52.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",13.406025171279907,2025-12-02 20:59:18
53.0,BM25 RAG,True,C,C,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk
i = ϕ(ci).","2509.01092v2.pdf, Page: 1",11.984473943710327,2025-12-02 20:59:18
54.0,BM25 RAG,True,C,C,"For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,10.56665849685669,2025-12-02 20:59:18
55.0,BM25 RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,12.547553539276125,2025-12-02 20:59:18
56.0,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,filename: 2509.01092v2.pdf page 5,15.699917316436768,2025-12-02 20:59:18
57.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,12.607956409454346,2025-12-02 20:59:18
58.0,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",14.144090175628662,2025-12-02 20:59:18
59.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",13.833006143569946,2025-12-02 20:59:18
60.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",10.142968654632568,2025-12-02 20:59:18
61.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",11.329942464828491,2025-12-02 20:59:18
62.0,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,13.665298223495483,2025-12-02 20:59:18
63.0,BM25 RAG,True,C,C,"The effective compression ratek
1−p+kp decreases when","2509.01092v2.pdf, Page: 4",16.481469869613647,2025-12-02 20:59:18
64.0,BM25 RAG,False,ERROR,C,No quote provided,,54.01665186882019,2025-12-02 20:59:18
65.0,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"2509.01092v2.pdf, Page: 5",22.110530138015747,2025-12-02 20:59:18
66.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,10.749718189239502,2025-12-02 20:59:18
67.0,BM25 RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",23.6505982875824,2025-12-02 20:59:18
68.0,BM25 RAG,False,B,C,"REFRAG: Our approach is illustrated in figure 1. We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o.","Source Reference: 2509.01092v2.pdf, Page: 17",18.74458408355713,2025-12-02 20:59:18
69.0,BM25 RAG,False,ERROR,C,No quote provided,,35.9617018699646,2025-12-02 20:59:18
70.0,BM25 RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,15.365936040878296,2025-12-02 20:59:18
1.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.899203538894652,2025-12-02 21:22:19
2.0,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",17.517815351486206,2025-12-02 21:22:19
3.0,Dense RAG,False,C,B,"Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves 16.53× acceleration in TTFT with cache and 8.59× without cache. Both higher than CEPE (i.e., 2.01× and 1.04× acceleration respectively) while having better model performance (see table 1).","2509.01092v2.pdf, Page: 15",63.90648698806763,2025-12-02 21:22:19
4.0,Dense RAG,True,D,D,This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.,filename: 2509.01092v2.pdf page 7,19.663006067276,2025-12-02 21:22:19
5.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,24.271275997161865,2025-12-02 21:22:19
,,False,,,,,,2025-12-02 21:22:19
7.0,Dense RAG,True,C,C,"we achieve a compression fraction of 1−p by randomly selecting T′ := pL chunks to remain uncompressed for the decoder. This pretraining allows the model to effectively handle mixed inputs at arbitrary positions, which is essential for the subsequent RL policy learning.","2509.01092v2.pdf, Page: 15",21.19820809364319,2025-12-02 21:22:19
8.0,Dense RAG,True,B,B,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",18.12564468383789,2025-12-02 21:22:19
9.0,Dense RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",15.354417085647585,2025-12-02 21:22:19
10.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,12.58212924003601,2025-12-02 21:22:19
11.0,Dense RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",12.895880460739136,2025-12-02 21:22:19
12.0,Dense RAG,False,A,C,"attention, reducing attention complexity from quadratic to linear; however, this method does not address memory requirements.",filename: 2509.01092v2.pdf page 9,17.805492162704468,2025-12-02 21:22:19
13.0,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,12.36427116394043,2025-12-02 21:22:19
14.0,Dense RAG,True,C,C,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","Source Reference: 2509.01092v2.pdf, Page: 3",14.016350030899048,2025-12-02 21:22:19
15.0,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. [...] The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,18.46888422966004,2025-12-02 21:22:19
16.0,Dense RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,19.547465801239014,2025-12-02 21:22:19
17.0,Dense RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",18.82898783683777,2025-12-02 21:22:19
18.0,Dense RAG,True,C,C,"our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,42.38892984390259,2025-12-02 21:22:19
19.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.020013332366943,2025-12-02 21:22:19
20.0,Dense RAG,True,C,C,"However, due to the compression, we are able to have more context information and hence achieve better performance.","2509.01092v2.pdf, Page: 7",13.93006181716919,2025-12-02 21:22:19
21.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.286623001098633,2025-12-02 21:22:19
22.0,Dense RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",16.280264139175415,2025-12-02 21:22:19
23.0,Dense RAG,True,C,C,The RL-based selective compression policy consistently achieves superior performance across varying compression rates p.,"2509.01092v2.pdf, Page: 4",23.64957594871521,2025-12-02 21:22:19
24.0,Dense RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,17.181743621826172,2025-12-02 21:22:19
25.0,Dense RAG,True,C,C,Reward = - Log(Perplexity),"Source Reference: 2509.01092v2.pdf, Page: 17",15.402990341186523,2025-12-02 21:22:19
26.0,Dense RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).",filename: 2509.01092v2.pdf page 2,14.981306552886965,2025-12-02 21:22:19
27.0,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,12.280670404434204,2025-12-02 21:22:19
28.0,Dense RAG,True,C,C,"The original Llama-2-7B supports only a4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",30.700811624526978,2025-12-02 21:22:19
29.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,15.551791191101074,2025-12-02 21:22:19
30.0,Dense RAG,True,C,C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: 1) It shortens the decoder’s input length, improving token allocation efficiency...",filename: 2509.01092v2.pdf page 1,17.304218530654907,2025-12-02 21:22:19
31.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","Source Reference: 2509.01092v2.pdf, Page: 1",11.05497932434082,2025-12-02 21:22:19
32.0,Dense RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",16.89017081260681,2025-12-02 21:22:19
33.0,Dense RAG,True,C,C,"Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves 16.53× acceleration in TTFT with cache and 8.59× without cache.",filename: 2509.01092v2.pdf page 15,14.741350889205933,2025-12-02 21:22:19
34.0,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",39.31010580062866,2025-12-02 21:22:19
35.0,Dense RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,16.2771418094635,2025-12-02 21:22:19
36.0,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",44.93416213989258,2025-12-02 21:22:19
37.0,Dense RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,12.193663358688354,2025-12-02 21:22:19
38.0,Dense RAG,False,ERROR,C,No quote provided,,63.57363033294678,2025-12-02 21:22:19
39.0,Dense RAG,True,C,C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,16.585350513458252,2025-12-02 21:22:19
40.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",13.206645011901855,2025-12-02 21:22:19
41.0,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",12.477934122085571,2025-12-02 21:22:19
42.0,Dense RAG,True,B,B,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",20.90148401260376,2025-12-02 21:22:19
43.0,Dense RAG,True,C,C,REFRAGRL uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4",16.900463581085205,2025-12-02 21:22:19
44.0,Dense RAG,False,D,C,"Notably, REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4",16.600996494293213,2025-12-02 21:22:19
45.0,Dense RAG,True,C,C,"This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the performance of REFRAG8.",filename: 2509.01092v2.pdf page 6,13.50326943397522,2025-12-02 21:22:19
46.0,Dense RAG,False,D,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",17.5152747631073,2025-12-02 21:22:19
47.0,Dense RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,11.35859441757202,2025-12-02 21:22:19
48.0,Dense RAG,True,B,B,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",14.990962505340576,2025-12-02 21:22:19
49.0,Dense RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","2509.01092v2.pdf, Page: 7",15.638525247573853,2025-12-02 21:22:19
50.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,10.127240419387816,2025-12-02 21:22:19
51.0,Dense RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",filename: 2509.01092v2.pdf page 1,12.604994535446169,2025-12-02 21:22:19
52.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",13.514634132385254,2025-12-02 21:22:19
53.0,Dense RAG,False,ERROR,C,No quote provided,,59.67113780975342,2025-12-02 21:22:19
54.0,Dense RAG,True,C,C,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,11.662668466567991,2025-12-02 21:22:19
55.0,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,11.576289176940918,2025-12-02 21:22:19
56.0,Dense RAG,True,C,C,"Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported
as average of Arxiv and Book domain.
...
REFRAGw/o curriculum 3.719 3.098 2.272 1.599
REFRAGwith curriculum 0.669 0.451 0.230 0.135","2509.01092v2.pdf, Page: 25",19.743836402893063,2025-12-02 21:22:19
57.0,Dense RAG,False,A,C,We compare the perplexity of xs+1:s+o using different selection policy under different p.,"2509.01092v2.pdf, Page: 4",30.40986037254333,2025-12-02 21:22:19
58.0,Dense RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,14.133213996887209,2025-12-02 21:22:19
59.0,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.742825508117676,2025-12-02 21:22:19
60.0,Dense RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,10.781734466552734,2025-12-02 21:22:19
61.0,Dense RAG,True,C,C,the overall input to thedecoder will be reduced by a factor of≃k .,"2509.01092v2.pdf, Page: 2",15.596940755844116,2025-12-02 21:22:19
62.0,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",12.34127116203308,2025-12-02 21:22:19
63.0,Dense RAG,True,C,C,"The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases).","Source Reference: 2509.01092v2.pdf, Page: 4",17.214544534683228,2025-12-02 21:22:19
64.0,Dense RAG,False,INFORMATION NOT AVAILABLE IN CONTEXT,C,,,34.117104053497314,2025-12-02 21:22:19
65.0,Dense RAG,True,C,C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,23.356518745422363,2025-12-02 21:22:19
66.0,Dense RAG,True,C,C,"CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,10.44833254814148,2025-12-02 21:22:19
67.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",20.588653802871704,2025-12-02 21:22:19
68.0,Dense RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",10.755516052246094,2025-12-02 21:22:19
69.0,Dense RAG,False,ERROR,C,No quote provided,,43.01837778091431,2025-12-02 21:22:19
70.0,Dense RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,13.519158840179443,2025-12-02 21:22:19
1.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,14.126380681991575,2025-12-02 21:42:00
2.0,Hybrid RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,filename: 2509.01092v2.pdf page 5,26.42400813102722,2025-12-02 21:42:00
3.0,Hybrid RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,11.677167654037476,2025-12-02 21:42:00
4.0,Hybrid RAG,True,D,D,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.",filename: 2509.01092v2.pdf page 7,13.517845153808594,2025-12-02 21:42:00
5.0,Hybrid RAG,True,C,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",18.11412215232849,2025-12-02 21:42:00
6.0,Hybrid RAG,True,C,C,"Curriculum learning is essential for effective training in the reconstruction task.The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.",filename: 2509.01092v2.pdf page 4,17.80543828010559,2025-12-02 21:42:00
7.0,Hybrid RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .",filename: 2509.01092v2.pdf page 1,13.001795053482056,2025-12-02 21:42:00
8.0,Hybrid RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",15.861589670181274,2025-12-02 21:42:00
9.0,Hybrid RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",12.28407859802246,2025-12-02 21:42:00
10.0,Hybrid RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,12.592164039611816,2025-12-02 21:42:00
11.0,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,14.7422776222229,2025-12-02 21:42:00
12.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",13.51479959487915,2025-12-02 21:42:00
13.0,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,12.595333814620972,2025-12-02 21:42:00
14.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...",filename: 2509.01092v2.pdf page 3,21.8105571269989,2025-12-02 21:42:00
15.0,Hybrid RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. ... The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,23.344804286956787,2025-12-02 21:42:00
16.0,Hybrid RAG,True,C,C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge... To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,21.19510602951049,2025-12-02 21:42:00
17.0,Hybrid RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,14.674805164337158,2025-12-02 21:42:00
18.0,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",14.203844547271729,2025-12-02 21:42:00
19.0,Hybrid RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",13.822751522064207,2025-12-02 21:42:00
20.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",16.896385192871094,2025-12-02 21:42:00
21.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",11.366596698760986,2025-12-02 21:42:00
22.0,Hybrid RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",16.900106191635132,2025-12-02 21:42:00
23.0,Hybrid RAG,True,C,C,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .",filename: 2509.01092v2.pdf page 1,15.977063655853271,2025-12-02 21:42:00
24.0,Hybrid RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,22.117719650268555,2025-12-02 21:42:00
25.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,10.753211736679075,2025-12-02 21:42:00
26.0,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...",filename: 2509.01092v2.pdf page 2,15.33541989326477,2025-12-02 21:42:00
27.0,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,13.474462270736694,2025-12-02 21:42:00
28.0,Hybrid RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,13.587300539016724,2025-12-02 21:42:00
29.0,Hybrid RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",15.188589572906494,2025-12-02 21:42:00
30.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,11.542072534561155,2025-12-02 21:42:00
31.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.593105554580688,2025-12-02 21:42:00
32.0,Hybrid RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,12.906654834747314,2025-12-02 21:42:00
33.0,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).","2509.01092v2.pdf, Page: 2",13.82149863243103,2025-12-02 21:42:00
34.0,Hybrid RAG,True,C,C,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","Source Reference: 2509.01092v2.pdf, Page: 3",57.75805330276489,2025-12-02 21:42:00
35.0,Hybrid RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",filename: 2509.01092v2.pdf page 3,14.136090517044067,2025-12-02 21:42:00
36.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","Source Reference: 2509.01092v2.pdf, Page: 3",15.357886552810667,2025-12-02 21:42:00
37.0,Hybrid RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,9.727561712265016,2025-12-02 21:42:00
38.0,Hybrid RAG,False,ERROR,C,No quote provided,,36.9684784412384,2025-12-02 21:42:00
39.0,Hybrid RAG,True,C,C,WefollowtheworkofLinetal.(2024)touseWikipediadumpsandCommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,16.404857397079468,2025-12-02 21:42:00
40.0,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,13.864046812057495,2025-12-02 21:42:00
41.0,Hybrid RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,11.874537229537964,2025-12-02 21:42:00
42.0,Hybrid RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,13.76975154876709,2025-12-02 21:42:00
43.0,Hybrid RAG,True,C,C,We employ RL to train a policy that optimally determines which segments to compress.,filename: 2509.01092v2.pdf page 15,17.512571811676025,2025-12-02 21:42:00
44.0,Hybrid RAG,False,B,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",20.28004193305969,2025-12-02 21:42:00
45.0,Hybrid RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"Source Reference: 2509.01092v2.pdf, Page: 5",12.619500875473022,2025-12-02 21:42:00
46.0,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,15.1336030960083,2025-12-02 21:42:00
47.0,Hybrid RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,15.41559886932373,2025-12-02 21:42:00
48.0,Hybrid RAG,True,B,B,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",14.58296251296997,2025-12-02 21:42:00
49.0,Hybrid RAG,True,C,C,"under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",filename: 2509.01092v2.pdf page 7,10.88824725151062,2025-12-02 21:42:00
50.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,14.00572109222412,2025-12-02 21:42:00
51.0,Hybrid RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1",13.507246971130373,2025-12-02 21:42:00
52.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.978767395019531,2025-12-02 21:42:00
53.0,Hybrid RAG,True,C,C,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk i = ϕ(ci).",filename: 2509.01092v2.pdf page 1,12.892916440963743,2025-12-02 21:42:00
54.0,Hybrid RAG,True,C,C,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",13.200737237930298,2025-12-02 21:42:00
55.0,Hybrid RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,11.17952823638916,2025-12-02 21:42:00
56.0,Hybrid RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"Source Reference: 2509.01092v2.pdf, Page: 5",15.534285306930542,2025-12-02 21:42:00
57.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,11.971351623535156,2025-12-02 21:42:00
58.0,Hybrid RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,15.959396839141846,2025-12-02 21:42:00
59.0,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,10.065622806549072,2025-12-02 21:42:00
60.0,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",12.230860471725464,2025-12-02 21:42:00
61.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.27164888381958,2025-12-02 21:42:00
62.0,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",13.18713903427124,2025-12-02 21:42:00
63.0,Hybrid RAG,True,C,C,"The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases).",filename: 2509.01092v2.pdf page 4,12.94187831878662,2025-12-02 21:42:00
64.0,Hybrid RAG,True,C,C,The results indicate that pre-training on the reconstruction task is important for the success of continual pre-training.,"2509.01092v2.pdf, Page: 5",61.38263416290283,2025-12-02 21:42:00
65.0,Hybrid RAG,True,C,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task.","2509.01092v2.pdf, Page: 5",24.000487089157104,2025-12-02 21:42:00
66.0,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",12.19923734664917,2025-12-02 21:42:00
67.0,Hybrid RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,27.07609224319458,2025-12-02 21:42:00
68.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,12.59116005897522,2025-12-02 21:42:00
69.0,Hybrid RAG,True,C,C,"To further validate our training approach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14 reports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the Arxiv domain.","2509.01092v2.pdf, Page: 6",42.81636261940002,2025-12-02 21:42:00
70.0,Hybrid RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.","2509.01092v2.pdf, Page: 4",14.62334156036377,2025-12-02 21:42:00
1.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.398279666900637,2025-12-04 15:28:27
2.0,Hybrid RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",21.811705827713013,2025-12-04 15:28:27
3.0,Hybrid RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,13.415400505065918,2025-12-04 15:28:27
4.0,Hybrid RAG,True,D,D,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.","2509.01092v2.pdf, Page: 7",15.872589826583862,2025-12-04 15:28:27
5.0,Hybrid RAG,True,C,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",34.612488985061646,2025-12-04 15:28:27
6.0,Hybrid RAG,True,C,C,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",13.823617219924929,2025-12-04 15:28:27
7.0,Hybrid RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",14.439291954040527,2025-12-04 15:28:27
8.0,Hybrid RAG,True,B,B,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,16.89668893814087,2025-12-04 15:28:27
9.0,Hybrid RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",12.902987718582152,2025-12-04 15:28:27
10.0,Hybrid RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.","2509.01092v2.pdf, Page: 6",13.211032629013062,2025-12-04 15:28:27
11.0,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","Source Reference: 2509.01092v2.pdf, Page: 0",15.359120845794678,2025-12-04 15:28:27
12.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.680875301361084,2025-12-04 15:28:27
13.0,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,13.88954734802246,2025-12-04 15:28:27
14.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","2509.01092v2.pdf, Page: 3",16.06284499168396,2025-12-04 15:28:27
15.0,Hybrid RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,23.876809120178223,2025-12-04 15:28:27
16.0,Hybrid RAG,True,C,C,"As the chunk length k increases, the number of possible token combinations expands exponentially, specifically at a rate of V k, where V is the vocabulary size. Effectively capturing this diversity within a fixed-length embedding presents a significant challenge... To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,21.50512337684632,2025-12-04 15:28:27
17.0,Hybrid RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,17.201023817062378,2025-12-04 15:28:27
18.0,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",13.82456088066101,2025-12-04 15:28:27
19.0,Hybrid RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",15.010486125946043,2025-12-04 15:28:27
20.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",17.860549449920654,2025-12-04 15:28:27
21.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",13.824909687042236,2025-12-04 15:28:27
22.0,Hybrid RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",17.511619091033936,2025-12-04 15:28:27
23.0,Hybrid RAG,True,C,C,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications.","2509.01092v2.pdf, Page: 1",16.84735655784607,2025-12-04 15:28:27
24.0,Hybrid RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,18.48038935661316,2025-12-04 15:28:27
25.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",13.209688425064089,2025-12-04 15:28:27
26.0,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache...",filename: 2509.01092v2.pdf page 2,16.58916997909546,2025-12-04 15:28:27
27.0,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,13.824129581451416,2025-12-04 15:28:27
28.0,Hybrid RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,13.209847450256348,2025-12-04 15:28:27
29.0,Hybrid RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",16.589159727096558,2025-12-04 15:28:27
30.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,12.800153017044067,2025-12-04 15:28:27
31.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",11.776137828826904,2025-12-04 15:28:27
32.0,Hybrid RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,15.974706649780272,2025-12-04 15:28:27
33.0,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).",filename: 2509.01092v2.pdf page 2,15.053107261657717,2025-12-04 15:28:27
34.0,Hybrid RAG,True,C,C,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","Source Reference: 2509.01092v2.pdf, Page: 3",59.90543150901794,2025-12-04 15:28:27
35.0,Hybrid RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",filename: 2509.01092v2.pdf page 3,15.36008644104004,2025-12-04 15:28:27
36.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. ... The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","2509.01092v2.pdf, Page: 3",24.576648235321045,2025-12-04 15:28:27
37.0,Hybrid RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",12.288323163986206,2025-12-04 15:28:27
38.0,Hybrid RAG,False,ERROR,C,No quote provided,,32.56350088119507,2025-12-04 15:28:27
39.0,Hybrid RAG,True,C,C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,"2509.01092v2.pdf, Page: 7",13.891623735427856,2025-12-04 15:28:27
40.0,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,14.985790729522703,2025-12-04 15:28:27
41.0,Hybrid RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,12.288289785385132,2025-12-04 15:28:27
42.0,Hybrid RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,16.06150794029236,2025-12-04 15:28:27
43.0,Hybrid RAG,True,C,C,We employ RL to train a policy that optimally determines which segments to compress.,filename: 2509.01092v2.pdf page 15,19.57393527030945,2025-12-04 15:28:27
44.0,Hybrid RAG,False,B,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",25.191469192504883,2025-12-04 15:28:27
45.0,Hybrid RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,15.360037565231323,2025-12-04 15:28:27
46.0,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,16.281607151031494,2025-12-04 15:28:27
47.0,Hybrid RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,12.595322370529177,2025-12-04 15:28:27
48.0,Hybrid RAG,True,B,B,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).",filename: 2509.01092v2.pdf page 19,14.746113061904907,2025-12-04 15:28:27
49.0,Hybrid RAG,True,C,C,"under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","Source Reference: 2509.01092v2.pdf, Page: 7",15.359930038452148,2025-12-04 15:28:27
50.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,17.203337907791138,2025-12-04 15:28:27
51.0,Hybrid RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1",14.74642252922058,2025-12-04 15:28:27
52.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.902843236923218,2025-12-04 15:28:27
53.0,Hybrid RAG,True,C,C,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk i = ϕ(ci).",filename: 2509.01092v2.pdf page 1,14.437666416168211,2025-12-04 15:28:27
54.0,Hybrid RAG,True,C,C,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",12.595542430877686,2025-12-04 15:28:27
55.0,Hybrid RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.","Source Reference: 2509.01092v2.pdf, Page: 3",14.233795166015623,2025-12-04 15:28:27
56.0,Hybrid RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,filename: 2509.01092v2.pdf page 5,19.920742750167847,2025-12-04 15:28:27
57.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,12.131350755691528,2025-12-04 15:28:27
58.0,Hybrid RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,16.69130539894104,2025-12-04 15:28:27
59.0,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,11.059142589569092,2025-12-04 15:28:27
60.0,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",12.595239400863647,2025-12-04 15:28:27
61.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,15.053789615631104,2025-12-04 15:28:27
62.0,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",13.21101427078247,2025-12-04 15:28:27
63.0,Hybrid RAG,True,C,C,"The effective compression ratek
1−p+kp decreases when
fewer chunks are compressed (i.e.,p increases).","2509.01092v2.pdf, Page: 4",20.24687647819519,2025-12-04 15:28:27
64.0,Hybrid RAG,False,ERROR,C,No quote provided,,82.97346472740173,2025-12-04 15:28:27
65.0,Hybrid RAG,True,C,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly). The results indicate that curriculum learning is essential for the success of the reconstruction task.","2509.01092v2.pdf, Page: 5",23.65440273284912,2025-12-04 15:28:27
66.0,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,12.902938604354858,2025-12-04 15:28:27
67.0,Hybrid RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",27.46376061439514,2025-12-04 15:28:27
68.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","Source Reference: 2509.01092v2.pdf, Page: 4",16.16051697731018,2025-12-04 15:28:27
69.0,Hybrid RAG,True,C,C,"To further validate our training approach on other decoder models, we conduct experiments with LLaMA-3.1-8B and LLaMA-3.2-3B. Table 14 reports the performance of these models paired with RoBERTa-Base and RoBERTa-Large encoders on the Arxiv domain.","2509.01092v2.pdf, Page: 6",55.60278367996216,2025-12-04 15:28:27
70.0,Hybrid RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,9.525395154953005,2025-12-04 15:28:27
1.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.683484315872192,2025-12-04 15:57:49
2.0,Baseline (Zero-shot),False,A,C,No quote provided,,15.997472763061523,2025-12-04 15:57:49
3.0,Baseline (Zero-shot),False,A,B,No quote provided,,19.83620977401733,2025-12-04 15:57:49
4.0,Baseline (Zero-shot),True,D,D,No quote provided,,16.898077249526978,2025-12-04 15:57:49
5.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.73762989044189,2025-12-04 15:57:49
6.0,Baseline (Zero-shot),True,C,C,No quote provided,,24.8836727142334,2025-12-04 15:57:49
7.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.6939914226532,2025-12-04 15:57:49
8.0,Baseline (Zero-shot),True,B,B,No quote provided,,21.31842470169068,2025-12-04 15:57:49
9.0,Baseline (Zero-shot),True,C,C,No quote provided,,27.338623762130737,2025-12-04 15:57:49
10.0,Baseline (Zero-shot),True,C,C,No quote provided,,13.41330337524414,2025-12-04 15:57:49
11.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.14740300178528,2025-12-04 15:57:49
12.0,Baseline (Zero-shot),True,C,C,No quote provided,,34.74713921546936,2025-12-04 15:57:49
13.0,Baseline (Zero-shot),False,A,C,No quote provided,,123.45446372032166,2025-12-04 15:57:49
14.0,Baseline (Zero-shot),True,C,C,No quote provided,,22.52940773963928,2025-12-04 15:57:49
15.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.348656177520752,2025-12-04 15:57:49
16.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.33311438560486,2025-12-04 15:57:49
17.0,Baseline (Zero-shot),False,A,C,No quote provided,,25.80226159095764,2025-12-04 15:57:49
18.0,Baseline (Zero-shot),True,C,C,No quote provided,,35.63461685180664,2025-12-04 15:57:49
19.0,Baseline (Zero-shot),True,C,C,No quote provided,,22.92958188056945,2025-12-04 15:57:49
20.0,Baseline (Zero-shot),False,A,C,No quote provided,,22.53508400917053,2025-12-04 15:57:49
21.0,Baseline (Zero-shot),True,C,C,No quote provided,,45.160624265670776,2025-12-04 15:57:49
22.0,Baseline (Zero-shot),False,A,C,No quote provided,,69.73333048820496,2025-12-04 15:57:49
23.0,Baseline (Zero-shot),False,A,C,No quote provided,,22.731459140777588,2025-12-04 15:57:49
24.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.362056732177734,2025-12-04 15:57:49
25.0,Baseline (Zero-shot),False,A,C,No quote provided,,17.50955033302307,2025-12-04 15:57:49
26.0,Baseline (Zero-shot),False,B,C,No quote provided,,15.665448665618896,2025-12-04 15:57:49
27.0,Baseline (Zero-shot),False,A,C,No quote provided,,26.41955590248108,2025-12-04 15:57:49
28.0,Baseline (Zero-shot),True,C,C,No quote provided,,22.42725896835327,2025-12-04 15:57:49
29.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.50874638557434,2025-12-04 15:57:49
30.0,Baseline (Zero-shot),False,A,C,No quote provided,,21.81150770187378,2025-12-04 15:57:49
31.0,Baseline (Zero-shot),False,A,C,No quote provided,,25.810815811157227,2025-12-04 15:57:49
32.0,Baseline (Zero-shot),False,A,C,No quote provided,,22.883846282958984,2025-12-04 15:57:49
33.0,Baseline (Zero-shot),True,C,C,No quote provided,,26.40658593177796,2025-12-04 15:57:49
34.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.02958369255066,2025-12-04 15:57:49
35.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.99711036682129,2025-12-04 15:57:49
36.0,Baseline (Zero-shot),True,C,C,No quote provided,,25.367751121521,2025-12-04 15:57:49
37.0,Baseline (Zero-shot),False,B,C,No quote provided,,17.02622938156128,2025-12-04 15:57:49
38.0,Baseline (Zero-shot),True,C,C,No quote provided,,64.21282362937927,2025-12-04 15:57:49
39.0,Baseline (Zero-shot),True,C,C,No quote provided,,47.916075229644775,2025-12-04 15:57:49
40.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.046377897262573,2025-12-04 15:57:49
41.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.8176908493042,2025-12-04 15:57:49
42.0,Baseline (Zero-shot),True,B,B,No quote provided,,16.58941078186035,2025-12-04 15:57:49
43.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.12457227706909,2025-12-04 15:57:49
44.0,Baseline (Zero-shot),False,A,C,No quote provided,,24.270670175552368,2025-12-04 15:57:49
45.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.20160698890686,2025-12-04 15:57:49
46.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.66083788871765,2025-12-04 15:57:49
47.0,Baseline (Zero-shot),False,A,B,No quote provided,,23.042064905166622,2025-12-04 15:57:49
48.0,Baseline (Zero-shot),True,B,B,No quote provided,,16.79185700416565,2025-12-04 15:57:49
49.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.170664072036743,2025-12-04 15:57:49
50.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.49189734458924,2025-12-04 15:57:49
51.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.99183964729309,2025-12-04 15:57:49
52.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.77881050109864,2025-12-04 15:57:49
53.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.360446214675903,2025-12-04 15:57:49
54.0,Baseline (Zero-shot),True,C,C,No quote provided,,36.80327391624451,2025-12-04 15:57:49
55.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.185219526290894,2025-12-04 15:57:49
56.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.669498682022097,2025-12-04 15:57:49
57.0,Baseline (Zero-shot),False,A,C,No quote provided,,23.32933259010315,2025-12-04 15:57:49
58.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.36966586112976,2025-12-04 15:57:49
59.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.66192054748535,2025-12-04 15:57:49
60.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.509841442108154,2025-12-04 15:57:49
61.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.723879098892212,2025-12-04 15:57:49
62.0,Baseline (Zero-shot),False,A,C,No quote provided,,43.79455852508545,2025-12-04 15:57:49
63.0,Baseline (Zero-shot),True,C,C,No quote provided,,25.10961437225341,2025-12-04 15:57:49
64.0,Baseline (Zero-shot),False,A,C,No quote provided,,24.09166145324707,2025-12-04 15:57:49
65.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.00373888015747,2025-12-04 15:57:49
66.0,Baseline (Zero-shot),True,C,C,No quote provided,,20.576823234558105,2025-12-04 15:57:49
67.0,Baseline (Zero-shot),True,C,C,No quote provided,,36.86604857444763,2025-12-04 15:57:49
68.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.41300368309021,2025-12-04 15:57:49
69.0,Baseline (Zero-shot),True,C,C,No quote provided,,13.30329942703247,2025-12-04 15:57:49
70.0,Baseline (Zero-shot),True,C,C,No quote provided,,45.46025085449219,2025-12-04 15:57:49
1.0,BM25 RAG,True,C,C,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts, REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.","2509.01092v2.pdf, Page: 10",19.91420841217041,2025-12-04 16:26:48
2.0,BM25 RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",35.374417543411255,2025-12-04 16:26:48
3.0,BM25 RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,16.316707372665405,2025-12-04 16:26:48
4.0,BM25 RAG,True,D,D,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.",filename: 2509.01092v2.pdf page 7,15.733718872070312,2025-12-04 16:26:48
5.0,BM25 RAG,True,C,C,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.",filename: 2509.01092v2.pdf page 1,30.745715618133545,2025-12-04 16:26:48
6.0,BM25 RAG,True,C,C,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",13.288666009902954,2025-12-04 16:26:48
7.0,BM25 RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",12.69439697265625,2025-12-04 16:26:48
8.0,BM25 RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,13.824092388153076,2025-12-04 16:26:48
9.0,BM25 RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",filename: 2509.01092v2.pdf page 7,12.594537734985352,2025-12-04 16:26:48
10.0,BM25 RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,13.824039459228516,2025-12-04 16:26:48
11.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,14.86277174949646,2025-12-04 16:26:48
12.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.935286045074465,2025-12-04 16:26:48
13.0,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",14.744669914245604,2025-12-04 16:26:48
14.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...",filename: 2509.01092v2.pdf page 3,24.575751543045044,2025-12-04 16:26:48
15.0,BM25 RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,13.22371244430542,2025-12-04 16:26:48
16.0,BM25 RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3",17.803424835205078,2025-12-04 16:26:48
17.0,BM25 RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,13.209007501602173,2025-12-04 16:26:48
18.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",14.244888067245483,2025-12-04 16:26:48
19.0,BM25 RAG,True,C,C,"At equal latency (8 passages for REFRAGvs. 1 for LLaMA),REFRAGachieves a1.93%average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.938783407211304,2025-12-04 16:26:48
20.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,12.902178049087524,2025-12-04 16:26:48
21.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.1313738822937,2025-12-04 16:26:48
22.0,BM25 RAG,True,C,C,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,11.980205059051514,2025-12-04 16:26:48
23.0,BM25 RAG,True,C,C,"Unlike prior methods (Yen et al., 2024),REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .",filename: 2509.01092v2.pdf page 1,17.20367169380188,2025-12-04 16:26:48
24.0,BM25 RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the","Source Reference: 2509.01092v2.pdf, Page: 3",13.824036121368408,2025-12-04 16:26:48
25.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",13.209075689315796,2025-12-04 16:26:48
26.0,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache...",filename: 2509.01092v2.pdf page 2,16.281373500823975,2025-12-04 16:26:48
27.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.873629093170166,2025-12-04 16:26:48
28.0,BM25 RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,12.77509379386902,2025-12-04 16:26:48
29.0,BM25 RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,16.895015478134155,2025-12-04 16:26:48
30.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",13.209873676300049,2025-12-04 16:26:48
31.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.979918003082275,2025-12-04 16:26:48
32.0,BM25 RAG,True,C,C,"Table 6 shows that with short context length s we are able to achieve k× acceleration in TTFT and up to k× acceleration in throughput. With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",15.05352258682251,2025-12-04 16:26:48
33.0,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).","2509.01092v2.pdf, Page: 2",15.00051498413086,2025-12-04 16:26:48
34.0,BM25 RAG,True,C,C,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.","2509.01092v2.pdf, Page: 3",37.12123847007752,2025-12-04 16:26:48
35.0,BM25 RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.","2509.01092v2.pdf, Page: 3",58.77920484542847,2025-12-04 16:26:48
36.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property.","Source Reference: 2509.01092v2.pdf, Page: 3",27.238889932632446,2025-12-04 16:26:48
37.0,BM25 RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,11.772912502288818,2025-12-04 16:26:48
38.0,BM25 RAG,False,ERROR,C,No quote provided,,47.61608505249024,2025-12-04 16:26:48
39.0,BM25 RAG,False,ERROR,C,No quote provided,,446.6617586612701,2025-12-04 16:26:48
40.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,12.59692883491516,2025-12-04 16:26:48
41.0,BM25 RAG,True,C,C,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9",14.743398904800417,2025-12-04 16:26:48
42.0,BM25 RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,13.516135454177856,2025-12-04 16:26:48
43.0,BM25 RAG,True,C,C,We employ RL to train a policy that optimally determines which segments to compress.,"2509.01092v2.pdf, Page: 15",51.607990026474,2025-12-04 16:26:48
44.0,BM25 RAG,False,B,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",20.88816857337952,2025-12-04 16:26:48
45.0,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,14.131274461746216,2025-12-04 16:26:48
46.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,15.052438497543337,2025-12-04 16:26:48
47.0,BM25 RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,11.323941230773926,2025-12-04 16:26:48
48.0,BM25 RAG,True,B,B,Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,"2509.01092v2.pdf, Page: 21",15.708375215530396,2025-12-04 16:26:48
49.0,BM25 RAG,True,C,C,"under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","Source Reference: 2509.01092v2.pdf, Page: 7",15.974520683288574,2025-12-04 16:26:48
50.0,BM25 RAG,False,A,C,"This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence.","2509.01092v2.pdf, Page: 2",19.35281205177307,2025-12-04 16:26:48
51.0,BM25 RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).","2509.01092v2.pdf, Page: 1",13.82364535331726,2025-12-04 16:26:48
52.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,15.666071653366089,2025-12-04 16:26:48
53.0,BM25 RAG,True,C,C,"This chunk embedding is then projected with a projection layer ϕ to match the size of the token embedding of the decoder model, ecnk i = ϕ(ci).",filename: 2509.01092v2.pdf page 1,11.367347717285156,2025-12-04 16:26:48
54.0,BM25 RAG,True,C,C,"For longer context length, acceleration reaches up to k²× for both metrics.",filename: 2509.01092v2.pdf page 2,11.08825659751892,2025-12-04 16:26:48
55.0,BM25 RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,11.949841260910034,2025-12-04 16:26:48
56.0,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,filename: 2509.01092v2.pdf page 5,17.817590713500977,2025-12-04 16:26:48
57.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,10.177705526351929,2025-12-04 16:26:48
58.0,BM25 RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",13.47524642944336,2025-12-04 16:26:48
59.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.74589705467224,2025-12-04 16:26:48
60.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",11.98014760017395,2025-12-04 16:26:48
61.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,15.359934091567991,2025-12-04 16:26:48
62.0,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,13.20956563949585,2025-12-04 16:26:48
63.0,BM25 RAG,True,C,C,"The effective compression ratek
1−p+kp decreases when","2509.01092v2.pdf, Page: 4",18.123636722564697,2025-12-04 16:26:48
64.0,BM25 RAG,False,ERROR,C,No quote provided,,56.32133746147156,2025-12-04 16:26:48
65.0,BM25 RAG,True,C,C,The results indicate that curriculum learning is essential for the success of the reconstruction task.,"2509.01092v2.pdf, Page: 5",21.706488847732544,2025-12-04 16:26:48
66.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",12.594574689865112,2025-12-04 16:26:48
67.0,BM25 RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",27.34104585647583,2025-12-04 16:26:48
68.0,BM25 RAG,False,B,C,"We use RoBERTa-large (Liu et al., 2019) as the encoder, feeding x1:s tokens and evaluating the perplexity on the output tokensxs+1:s+o.","Source Reference: 2509.01092v2.pdf, Page: 17",18.73853182792664,2025-12-04 16:26:48
69.0,BM25 RAG,False,ERROR,C,No quote provided,,47.92502808570862,2025-12-04 16:26:48
70.0,BM25 RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,15.357314825057983,2025-12-04 16:26:48
1.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.663510799407959,2025-12-04 16:50:55
2.0,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",16.281285762786865,2025-12-04 16:50:55
3.0,Dense RAG,False,C,B,"Both higher than CEPE (i.e., 2.01× and 1.04× acceleration respectively) while having better model performance (see table 1).","2509.01092v2.pdf, Page: 15",49.151280879974365,2025-12-04 16:50:55
4.0,Dense RAG,True,D,D,This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.,"2509.01092v2.pdf, Page: 7",20.890461683273315,2025-12-04 16:50:55
5.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",25.80421781539917,2025-12-04 16:50:55
6.0,Dense RAG,False,ERROR,C,No quote provided,None,41.77824926376343,2025-12-04 16:50:55
7.0,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",14.595024347305298,2025-12-04 16:50:55
8.0,Dense RAG,True,B,B,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",14.895812749862671,2025-12-04 16:50:55
9.0,Dense RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",13.516361951828003,2025-12-04 16:50:55
10.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,12.543474674224854,2025-12-04 16:50:55
11.0,Dense RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",13.875200748443604,2025-12-04 16:50:55
12.0,Dense RAG,True,C,C,reconstructings = k×L tokens from L chunk embeddings further compounds the difficulty of the task.,filename: 2509.01092v2.pdf page 3,20.275085926055908,2025-12-04 16:50:55
13.0,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,14.745105028152466,2025-12-04 16:50:55
14.0,Dense RAG,True,C,C,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","Source Reference: 2509.01092v2.pdf, Page: 3",15.667605638504028,2025-12-04 16:50:55
15.0,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,23.653719186782837,2025-12-04 16:50:55
16.0,Dense RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,20.58199644088745,2025-12-04 16:50:55
17.0,Dense RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",15.359265565872192,2025-12-04 16:50:55
18.0,Dense RAG,True,C,C,"our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,45.77278280258179,2025-12-04 16:50:55
19.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",13.516489267349243,2025-12-04 16:50:55
20.0,Dense RAG,True,C,C,"However, due to the compression, we are able to have more context information and hence achieve better performance.","2509.01092v2.pdf, Page: 7",14.438141584396362,2025-12-04 16:50:55
21.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.204228401184082,2025-12-04 16:50:55
22.0,Dense RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",11.967957258224487,2025-12-04 16:50:55
23.0,Dense RAG,True,C,C,These results further highlight the effectiveness of the RL-trained policy and underscore the practicality of dynamically adjusting the compression rate without compromising performance.,"2509.01092v2.pdf, Page: 6",21.987433910369873,2025-12-04 16:50:55
24.0,Dense RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,14.43726110458374,2025-12-04 16:50:55
25.0,Dense RAG,True,C,C,Reward = - Log(Perplexity),"2509.01092v2.pdf, Page: 17",13.82321548461914,2025-12-04 16:50:55
26.0,Dense RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...","2509.01092v2.pdf, Page: 2",15.667355298995972,2025-12-04 16:50:55
27.0,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.437997817993164,2025-12-04 16:50:55
28.0,Dense RAG,True,C,C,The encoder model then processes all the chunks to obtain a chunk embedding for each chunkci = Menc(Ci).,"2509.01092v2.pdf, Page: 1",50.994646072387695,2025-12-04 16:50:55
29.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,12.595396518707275,2025-12-04 16:50:55
30.0,Dense RAG,True,C,C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,15.974355936050415,2025-12-04 16:50:55
31.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.901535749435425,2025-12-04 16:50:55
32.0,Dense RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",15.974877119064331,2025-12-04 16:50:55
33.0,Dense RAG,True,C,C,"Under the context length of16384(i.e., mid-to-long context),REFRAG achieves16 .53× acceleration in TTFT with cache and8.59× without cache.",filename: 2509.01092v2.pdf page 15,12.595044612884521,2025-12-04 16:50:55
34.0,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",31.64081311225891,2025-12-04 16:50:55
35.0,Dense RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",filename: 2509.01092v2.pdf page 3,18.93527913093567,2025-12-04 16:50:55
36.0,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",34.51792240142822,2025-12-04 16:50:55
37.0,Dense RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,11.058753490447998,2025-12-04 16:50:55
38.0,Dense RAG,False,ERROR,C,No quote provided,None,65.61057543754578,2025-12-04 16:50:55
39.0,Dense RAG,True,C,C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,17.66955852508545,2025-12-04 16:50:55
40.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.409729480743408,2025-12-04 16:50:55
41.0,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,12.5946626663208,2025-12-04 16:50:55
42.0,Dense RAG,True,B,B,The results indicate that pre-training on the reconstruction task is important for the success of continual pre-training.,"2509.01092v2.pdf, Page: 5",22.118045330047607,2025-12-04 16:50:55
43.0,Dense RAG,True,C,C,REFRAGRL uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4",19.045934915542603,2025-12-04 16:50:55
44.0,Dense RAG,False,D,C,"Notably,REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4",21.504439115524292,2025-12-04 16:50:55
45.0,Dense RAG,True,C,C,"This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the performance of REFRAG8.",filename: 2509.01092v2.pdf page 6,15.535208225250244,2025-12-04 16:50:55
46.0,Dense RAG,True,C,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",30.236364603042603,2025-12-04 16:50:55
47.0,Dense RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,16.282612562179565,2025-12-04 16:50:55
48.0,Dense RAG,True,B,B,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",17.4618718624115,2025-12-04 16:50:55
49.0,Dense RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",filename: 2509.01092v2.pdf page 7,13.564298391342163,2025-12-04 16:50:55
50.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",13.209316730499268,2025-12-04 16:50:55
51.0,Dense RAG,True,C,C,"Due to diversity and other operations such as deduplication, most context chunks during decoding are unrelated, resulting in predominantly zero cross-attention between chunks (see figure 7).",filename: 2509.01092v2.pdf page 1,14.135494470596313,2025-12-04 16:50:55
52.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,12.151095151901245,2025-12-04 16:50:55
53.0,Dense RAG,True,C,C,"REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder. This approach offers three main advantages: ... 3) It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",52.60581421852112,2025-12-04 16:50:55
54.0,Dense RAG,True,C,C,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,12.345516443252563,2025-12-04 16:50:55
55.0,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,12.902181148529053,2025-12-04 16:50:55
56.0,Dense RAG,True,C,C,"Table 11Performance comparison on reconstruction task with and w/o curriculum learning. Log-Perplexity is reported
as average of Arxiv and Book domain.
...
REFRAGw/o curriculum 3.719 3.098 2.272 1.599
REFRAGwith curriculum 0.669 0.451 0.230 0.135","2509.01092v2.pdf, Page: 25",20.889747381210327,2025-12-04 16:50:55
57.0,Dense RAG,False,A,C,We compare the perplexity of xs+1:s+o using different selection policy under different p.,"2509.01092v2.pdf, Page: 4",20.581690549850464,2025-12-04 16:50:55
58.0,Dense RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,14.131335973739624,2025-12-04 16:50:55
59.0,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",14.438358545303345,2025-12-04 16:50:55
60.0,Dense RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,10.444468021392822,2025-12-04 16:50:55
61.0,Dense RAG,True,C,C,"the context is the dominating part of the input (i.e., s≫q ) and hence the overall input to the decoder will be reduced by a factor of≃k .","2509.01092v2.pdf, Page: 2",16.56698703765869,2025-12-04 16:50:55
62.0,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",13.537940502166748,2025-12-04 16:50:55
63.0,Dense RAG,True,C,C,"The effective compression rate k / (1−p+kp) decreases when fewer chunks are compressed (i.e.,p increases).",filename: 2509.01092v2.pdf page 4,11.673256874084473,2025-12-04 16:50:55
64.0,Dense RAG,False,ERROR,C,No quote provided,None,22.736101388931274,2025-12-04 16:50:55
65.0,Dense RAG,True,C,C,"REFRAG leverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",50.035593032836914,2025-12-04 16:50:55
66.0,Dense RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,13.245011329650879,2025-12-04 16:50:55
67.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,23.658507347106934,2025-12-04 16:50:55
68.0,Dense RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",13.81912612915039,2025-12-04 16:50:55
69.0,Dense RAG,False,ERROR,C,No quote provided,None,62.97555613517761,2025-12-04 16:50:55
70.0,Dense RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,15.05294680595398,2025-12-04 16:50:55
