question_number,pipeline,correct,predicted,ground_truth,quote,source,latency,run_timestamp,id,error
1.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.19919204711914,2025-11-30 13:36:03,,
2.0,Baseline (Zero-shot),False,A,C,No quote provided,,16.702476024627686,2025-11-30 13:36:03,,
3.0,Baseline (Zero-shot),False,A,B,No quote provided,,17.560168027877808,2025-11-30 13:36:03,,
4.0,Baseline (Zero-shot),True,D,D,No quote provided,,17.52934718132019,2025-11-30 13:36:03,,
5.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.289870023727417,2025-11-30 13:36:03,,
6.0,Baseline (Zero-shot),True,C,C,No quote provided,,23.66543698310852,2025-11-30 13:36:03,,
7.0,Baseline (Zero-shot),False,A,C,No quote provided,,18.747491359710693,2025-11-30 13:36:03,,
8.0,Baseline (Zero-shot),True,B,B,No quote provided,,21.22592282295227,2025-11-30 13:36:03,,
9.0,Baseline (Zero-shot),True,C,C,No quote provided,,27.47951054573059,2025-11-30 13:36:03,,
10.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.742995738983154,2025-11-30 13:36:03,,
11.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.15061616897583,2025-11-30 13:36:03,,
12.0,Baseline (Zero-shot),True,C,C,No quote provided,,26.96446657180786,2025-11-30 13:36:03,,
13.0,Baseline (Zero-shot),False,A,C,No quote provided,,121.32422304153442,2025-11-30 13:36:03,,
14.0,Baseline (Zero-shot),True,C,C,No quote provided,,24.826889038085938,2025-11-30 13:36:03,,
15.0,Baseline (Zero-shot),False,A,C,No quote provided,,21.493409633636475,2025-11-30 13:36:03,,
16.0,Baseline (Zero-shot),True,C,C,No quote provided,,19.33566689491272,2025-11-30 13:36:03,,
17.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.50408959388733,2025-11-30 13:36:03,,
18.0,Baseline (Zero-shot),True,C,C,No quote provided,,33.96539759635925,2025-11-30 13:36:03,,
19.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.222537755966187,2025-11-30 13:36:03,,
20.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.439875841140747,2025-11-30 13:36:03,,
21.0,Baseline (Zero-shot),True,C,C,No quote provided,,44.56849265098572,2025-11-30 13:36:03,,
22.0,Baseline (Zero-shot),False,A,C,No quote provided,,106.21453309059144,2025-11-30 13:36:03,,
23.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.96406579017639,2025-11-30 13:36:03,,
24.0,Baseline (Zero-shot),True,C,C,No quote provided,,14.810994863510132,2025-11-30 13:36:03,,
25.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.378226041793823,2025-11-30 13:36:03,,
26.0,Baseline (Zero-shot),False,A,C,No quote provided,,13.102569580078123,2025-11-30 13:36:03,,
27.0,Baseline (Zero-shot),False,A,C,No quote provided,,15.35844373703003,2025-11-30 13:36:03,,
28.0,Baseline (Zero-shot),False,A,C,No quote provided,,19.99634838104248,2025-11-30 13:36:03,,
29.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.97860836982727,2025-11-30 13:36:03,,
30.0,Baseline (Zero-shot),False,A,C,No quote provided,,16.754525423049927,2025-11-30 13:36:03,,
31.0,Baseline (Zero-shot),False,A,C,No quote provided,,24.72108769416809,2025-11-30 13:36:03,,
32.0,Baseline (Zero-shot),False,A,C,No quote provided,,20.89293003082276,2025-11-30 13:36:03,,
33.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.8123619556427,2025-11-30 13:36:03,,
34.0,Baseline (Zero-shot),False,A,C,No quote provided,,20.702807903289795,2025-11-30 13:36:03,,
35.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.57866930961609,2025-11-30 13:36:03,,
36.0,Baseline (Zero-shot),True,C,C,No quote provided,,21.23524141311645,2025-11-30 13:36:03,,
37.0,Baseline (Zero-shot),False,A,C,No quote provided,,15.052033185958862,2025-11-30 13:36:03,,
38.0,Baseline (Zero-shot),True,C,C,No quote provided,,30.01565146446228,2025-11-30 13:36:03,,
39.0,Baseline (Zero-shot),True,C,C,No quote provided,,42.96678352355957,2025-11-30 13:36:03,,
40.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.949187994003296,2025-11-30 13:36:03,,
41.0,Baseline (Zero-shot),True,C,C,No quote provided,,15.980977058410645,2025-11-30 13:36:03,,
42.0,Baseline (Zero-shot),True,B,B,No quote provided,,14.749933004379272,2025-11-30 13:36:03,,
43.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.592713594436646,2025-11-30 13:36:03,,
44.0,Baseline (Zero-shot),False,A,C,No quote provided,,26.25874900817871,2025-11-30 13:36:03,,
45.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.520081043243408,2025-11-30 13:36:03,,
46.0,Baseline (Zero-shot),True,C,C,No quote provided,,17.746288776397705,2025-11-30 13:36:03,,
47.0,Baseline (Zero-shot),False,A,B,No quote provided,,18.94143056869507,2025-11-30 13:36:03,,
48.0,Baseline (Zero-shot),True,B,B,No quote provided,,15.972811222076416,2025-11-30 13:36:03,,
49.0,Baseline (Zero-shot),True,C,C,No quote provided,,16.587575912475586,2025-11-30 13:36:03,,
50.0,Baseline (Zero-shot),True,C,C,No quote provided,,18.412781476974487,2025-11-30 13:36:03,,
1.0,BM25 RAG,True,C,C,"By leveraging the inherent sparsity and block-diagonal attention patterns present in RAG contexts, REFRAG compresses, senses, and expands context representations to significantly reduce both memory usage and inference latency, particularly the TTFT.",filename: 2509.01092v2.pdf page 10,21.90574598312378,2025-11-30 13:56:56,,
2.0,BM25 RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",38.27962493896485,2025-11-30 13:56:56,,
3.0,BM25 RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,14.742502689361572,2025-11-30 13:56:56,,
4.0,BM25 RAG,True,D,D,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.","2509.01092v2.pdf, Page: 7",12.551043272018433,2025-11-30 13:56:56,,
5.0,BM25 RAG,True,C,C,"RAG contexts often contain sparse information, with many retrieved passages being uninformative and reused across multiple inferences.",filename: 2509.01092v2.pdf page 1,29.54012608528137,2025-11-30 13:56:56,,
6.0,BM25 RAG,True,C,C,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",11.373889207839966,2025-11-30 13:56:56,,
7.0,BM25 RAG,True,C,C,"...a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",11.59180498123169,2025-11-30 13:56:56,,
8.0,BM25 RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,11.761751413345335,2025-11-30 13:56:56,,
9.0,BM25 RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.",filename: 2509.01092v2.pdf page 7,11.677345514297484,2025-11-30 13:56:56,,
10.0,BM25 RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,12.009150743484495,2025-11-30 13:56:56,,
11.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",14.064963340759276,2025-11-30 13:56:56,,
12.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",13.025819540023804,2025-11-30 13:56:56,,
13.0,BM25 RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",14.036844968795776,2025-11-30 13:56:56,,
14.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. The encoder and decoder are fine-tuned to handle mixed inputs of compressed and uncompressed chunks. The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...","Source Reference: 2509.01092v2.pdf, Page: 3",24.558956146240234,2025-11-30 13:56:56,,
15.0,BM25 RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.","Source Reference: 2509.01092v2.pdf, Page: 3",14.43474817276001,2025-11-30 13:56:56,,
16.0,BM25 RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).","2509.01092v2.pdf, Page: 3",16.48289656639099,2025-11-30 13:56:56,,
17.0,BM25 RAG,True,C,C,Table 13 demonstrates that REFRAG16 with RL-based selective compression consistently outperforms REFRAG8 across different datasets and context lengths.,"Source Reference: 2509.01092v2.pdf, Page: 5",13.92500901222229,2025-11-30 13:56:56,,
18.0,BM25 RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.",filename: 2509.01092v2.pdf page 9,13.514994382858276,2025-11-30 13:56:56,,
19.0,BM25 RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,13.215579986572266,2025-11-30 13:56:56,,
20.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.",filename: 2509.01092v2.pdf page 4,11.225929975509644,2025-11-30 13:56:56,,
21.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",12.129814386367798,2025-11-30 13:56:56,,
22.0,BM25 RAG,True,C,C,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,10.140521764755247,2025-11-30 13:56:56,,
23.0,BM25 RAG,True,C,C,"REFRAGsupports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .","2509.01092v2.pdf, Page: 1",19.35412883758545,2025-11-30 13:56:56,,
24.0,BM25 RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,12.905127763748167,2025-11-30 13:56:56,,
25.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.",filename: 2509.01092v2.pdf page 3,11.518528938293455,2025-11-30 13:56:56,,
26.0,BM25 RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1, both surpassing CEPE (2.01× and 1.04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).",filename: 2509.01092v2.pdf page 2,13.064291715621948,2025-11-30 13:56:56,,
27.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,15.228537559509276,2025-11-30 13:56:56,,
28.0,BM25 RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,12.052028894424438,2025-11-30 13:56:56,,
29.0,BM25 RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,14.811632871627808,2025-11-30 13:56:56,,
30.0,BM25 RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",10.138200759887695,2025-11-30 13:56:56,,
31.0,BM25 RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.36741065979004,2025-11-30 13:56:56,,
32.0,BM25 RAG,True,C,C,"Table 6 shows that with short context length s we are able to achieve k× acceleration in TTFT and up to k× acceleration in throughput. With longer context lengths, we are able to achieve up to k2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,11.272023677825928,2025-11-30 13:56:56,,
33.0,BM25 RAG,True,C,C,"REFRAG with k = 16 achieves 16 .53× TTFT acceleration with cache and 8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively)",filename: 2509.01092v2.pdf page 2,11.465353727340698,2025-11-30 13:56:56,,
34.0,BM25 RAG,True,C,C,"performance of the reconstruction task with and without curriculum learning (i.e., reconstruction of x1:s from s/k chunk embedding directly).","Source Reference: 2509.01092v2.pdf, Page: 5",49.15427589416504,2025-11-30 13:56:56,,
35.0,BM25 RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",filename: 2509.01092v2.pdf page 3,19.04717063903809,2025-11-30 13:56:56,,
36.0,BM25 RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",15.6915123462677,2025-11-30 13:56:56,,
37.0,BM25 RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",11.343535900115969,2025-11-30 13:56:56,,
38.0,BM25 RAG,False,,C,,,18.421658039093018,2025-11-30 13:56:56,,
39.0,BM25 RAG,False,ERROR,C,No quote provided,,425.04929852485657,2025-11-30 13:56:56,,
40.0,BM25 RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,13.002811670303345,2025-11-30 13:56:56,,
41.0,BM25 RAG,True,C,C,"Chevalier et al. (2023) proposed recursive compression for documents, using compressed embeddings for prediction, similar to our method.","2509.01092v2.pdf, Page: 9",12.887280702590942,2025-11-30 13:56:56,,
42.0,BM25 RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,15.987800359725952,2025-11-30 13:56:56,,
43.0,BM25 RAG,True,C,C,We employ RL to train a policy that optimally determines which segments to compress.,"2509.01092v2.pdf, Page: 15",46.69695782661438,2025-11-30 13:56:56,,
44.0,BM25 RAG,False,B,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,44.54814195632935,2025-11-30 13:56:56,,
45.0,BM25 RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,14.438326120376589,2025-11-30 13:56:56,,
46.0,BM25 RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",15.798884868621826,2025-11-30 13:56:56,,
47.0,BM25 RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,11.543855667114258,2025-11-30 13:56:56,,
48.0,BM25 RAG,True,B,B,Table 11 shows the necessity of curriculum learning to the success of reconstruction task.,"2509.01092v2.pdf, Page: 21",12.288731098175049,2025-11-30 13:56:56,,
49.0,BM25 RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.","Source Reference: 2509.01092v2.pdf, Page: 7",20.276132345199585,2025-11-30 13:56:56,,
50.0,BM25 RAG,False,A,C,"This architectural design leads to significant reductions in both latency and memory usage, primarily due to the shortened input sequence.","2509.01092v2.pdf, Page: 2",17.20518922805786,2025-11-30 13:56:56,,
1.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.737315893173218,2025-11-30 14:13:04,,
2.0,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",18.433237314224243,2025-11-30 14:13:04,,
3.0,Dense RAG,False,C,B,"Under the context length of 16384 (i.e., mid-to-long context), REFRAG achieves 16 .53× acceleration in TTFT with cache and 8.59× without cache. Both higher than CEPE (i.e., 2.01× and 1 .04× acceleration respectively) while having better model performance (see table 1).","2509.01092v2.pdf, Page: 15",35.63846158981323,2025-11-30 14:13:04,,
4.0,Dense RAG,True,D,D,"However, due to the compression, we are able to have more context information and hence achieve better performance. ... The same result occurs in long context scenarios.","2509.01092v2.pdf, Page: 7",20.89223957061768,2025-11-30 14:13:04,,
5.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,24.58038592338562,2025-11-30 14:13:04,,
,,,,,,,,2025-11-30 14:13:04,5.0,object of type 'NoneType' has no len()
7.0,Dense RAG,True,C,C,an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.,"2509.01092v2.pdf, Page: 2",19.662707090377808,2025-11-30 14:13:04,,
8.0,Dense RAG,True,B,B,"With longer context lengths, we are able to achieve up to k²× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",14.132005214691162,2025-11-30 14:13:04,,
9.0,Dense RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",15.054529905319214,2025-11-30 14:13:04,,
10.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,12.289586305618286,2025-11-30 14:13:04,,
11.0,Dense RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",14.439897537231444,2025-11-30 14:13:04,,
12.0,Dense RAG,True,C,C,reconstructings = k×L tokens from L chunk embeddings further compounds the difficulty of the task.,filename: 2509.01092v2.pdf page 3,22.12148118019104,2025-11-30 14:13:04,,
13.0,Dense RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up tok2× for both metrics.",filename: 2509.01092v2.pdf page 2,14.440667152404783,2025-11-30 14:13:04,,
14.0,Dense RAG,True,C,C,"chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property and enabling flexible placement of compression.","2509.01092v2.pdf, Page: 3",18.432088375091556,2025-11-30 14:13:04,,
15.0,Dense RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer.",filename: 2509.01092v2.pdf page 3,22.720837593078613,2025-11-30 14:13:04,,
16.0,Dense RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,18.88525342941284,2025-11-30 14:13:04,,
17.0,Dense RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,"2509.01092v2.pdf, Page: 5",17.35346555709839,2025-11-30 14:13:04,,
18.0,Dense RAG,True,C,C,"our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",62.6461615562439,2025-11-30 14:13:04,,
19.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,12.61747932434082,2025-11-30 14:13:04,,
20.0,Dense RAG,True,C,C,"However, due to the compression, we are able to have more context information and hence achieve better performance.","2509.01092v2.pdf, Page: 7",14.71333122253418,2025-11-30 14:13:04,,
21.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",11.09048342704773,2025-11-30 14:13:04,,
22.0,Dense RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",14.130486965179443,2025-11-30 14:13:04,,
23.0,Dense RAG,True,C,C,The RL-based selective compression policy consistently achieves superior performance across varying compression rates p.,"2509.01092v2.pdf, Page: 4",24.26739740371704,2025-11-30 14:13:04,,
24.0,Dense RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.",filename: 2509.01092v2.pdf page 3,14.13353991508484,2025-11-30 14:13:04,,
25.0,Dense RAG,True,C,C,Reward = - Log(Perplexity),"Source Reference: 2509.01092v2.pdf, Page: 17",16.892288208007812,2025-11-30 14:13:04,,
26.0,Dense RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of 16384 (mid-to-long context), REFRAG with k = 16 achieves 16.53× TTFT acceleration with cache and 8.59× without cache1...","2509.01092v2.pdf, Page: 2",15.360280990600586,2025-11-30 14:13:04,,
27.0,Dense RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,12.901149988174438,2025-11-30 14:13:04,,
28.0,Dense RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",25.79928493499756,2025-11-30 14:13:04,,
29.0,Dense RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.",filename: 2509.01092v2.pdf page 6,13.821470737457275,2025-11-30 14:13:04,,
30.0,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.","2509.01092v2.pdf, Page: 1",18.736374616622925,2025-11-30 14:13:04,,
31.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.023429870605469,2025-11-30 14:13:04,,
32.0,Dense RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.","2509.01092v2.pdf, Page: 15",17.543730974197388,2025-11-30 14:13:04,,
33.0,Dense RAG,True,C,C,"Under the context length of16384(i.e., mid-to-long context),REFRAG achieves16 .53× acceleration in TTFT with cache and8.59× without cache.",filename: 2509.01092v2.pdf page 15,11.363731622695925,2025-11-30 14:13:04,,
34.0,Dense RAG,False,A,C,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",34.71043300628662,2025-11-30 14:13:04,,
35.0,Dense RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth.",filename: 2509.01092v2.pdf page 3,17.50815176963806,2025-11-30 14:13:04,,
36.0,Dense RAG,True,C,C,"Additionally, an RL policy is trained to do selective compression to further improve the performance which we will defer the discussion to section 2.","2509.01092v2.pdf, Page: 2",34.54675507545471,2025-11-30 14:13:04,,
37.0,Dense RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.",filename: 2509.01092v2.pdf page 4,8.948212385177612,2025-11-30 14:13:04,,
38.0,Dense RAG,False,ERROR,C,No quote provided,,60.35451340675354,2025-11-30 14:13:04,,
39.0,Dense RAG,True,C,C,We follow the work of Linetal. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,18.121755361557007,2025-11-30 14:13:04,,
40.0,Dense RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.","2509.01092v2.pdf, Page: 7",12.828473567962646,2025-11-30 14:13:04,,
41.0,Dense RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,13.743019342422484,2025-11-30 14:13:04,,
42.0,Dense RAG,True,B,B,Reconstruction task is essential for the model to learn the continual pre-training task.,"2509.01092v2.pdf, Page: 5",22.341163158416748,2025-11-30 14:13:04,,
43.0,Dense RAG,True,C,C,REFRAG uses RL-based selective compression.,"2509.01092v2.pdf, Page: 4",16.58551049232483,2025-11-30 14:13:04,,
44.0,Dense RAG,False,D,C,"REFRAG8 and REFRAG16 consistently outperform other baselines across nearly all settings, while also achieving lower latency than CEPE (figure 2).","2509.01092v2.pdf, Page: 4",25.699922800064087,2025-11-30 14:13:04,,
45.0,Dense RAG,True,C,C,"This finding is particularly surprising, as REFRAG16 achieves a compression rate of 8 without recomputing chunk embeddings, yet still surpasses the performance of REFRAG8.",filename: 2509.01092v2.pdf page 6,14.388567447662354,2025-11-30 14:13:04,,
46.0,Dense RAG,False,D,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",18.100990056991577,2025-11-30 14:13:04,,
47.0,Dense RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,9.822258710861206,2025-11-30 14:13:04,,
48.0,Dense RAG,True,B,B,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",15.436508893966677,2025-11-30 14:13:04,,
49.0,Dense RAG,True,C,C,"The result demonstrates that under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",filename: 2509.01092v2.pdf page 7,12.900505781173706,2025-11-30 14:13:04,,
50.0,Dense RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.979698181152344,2025-11-30 14:13:04,,
1.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",15.590412616729736,2025-11-30 14:27:32,,
2.0,Hybrid RAG,False,A,C,reconstruction of x1:s from s/k chunk embedding directly,"2509.01092v2.pdf, Page: 5",23.889443397521973,2025-11-30 14:27:32,,
3.0,Hybrid RAG,True,B,B,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.",filename: 2509.01092v2.pdf page 9,13.927541017532349,2025-11-30 14:27:32,,
4.0,Hybrid RAG,True,D,D,"This improvement is attributable to the limited 4k-token context window of LLaMAFT, which necessitates truncating portions of the conversational history in longer contexts, resulting in the loss of crucial information required to answer subsequent questions.","2509.01092v2.pdf, Page: 7",13.856212377548218,2025-11-30 14:27:32,,
5.0,Hybrid RAG,True,C,C,"In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query.","2509.01092v2.pdf, Page: 0",34.277740478515625,2025-11-30 14:27:32,,
6.0,Hybrid RAG,True,C,C,"The reconstruction task, while intuitive, is particularly challenging when multiple chunks must be reconstructed.","2509.01092v2.pdf, Page: 4",14.265195846557617,2025-11-30 14:27:32,,
7.0,Hybrid RAG,True,C,C,"This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .",filename: 2509.01092v2.pdf page 1,14.693482398986816,2025-11-30 14:27:32,,
8.0,Hybrid RAG,True,B,B,"With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,15.712933540344238,2025-11-30 14:27:32,,
9.0,Hybrid RAG,True,C,C,"Under equivalent latency constraints,REFRAGconsistently outperform LLaMA on both settings as the saved context can be reinvested to include additional information within the same latency budget.","2509.01092v2.pdf, Page: 7",16.589126586914062,2025-11-30 14:27:32,,
10.0,Hybrid RAG,True,C,C,"In contrast, a compression rate of 64 appears to be overly aggressive, resulting in diminished performance.",filename: 2509.01092v2.pdf page 6,13.526237964630127,2025-11-30 14:27:32,,
11.0,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.","2509.01092v2.pdf, Page: 0",14.122843265533447,2025-11-30 14:27:32,,
12.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",13.810344934463501,2025-11-30 14:27:32,,
13.0,Hybrid RAG,True,C,C,"Theoretical analysis (section A) shows that for short context lengths, our method achieves up to k× acceleration in TTFT and throughput. For longer context length, acceleration reaches up to k²× for both metrics.","2509.01092v2.pdf, Page: 2",13.084510087966919,2025-11-30 14:27:32,,
14.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form. ... The policy network leverages chunk embeddings and masking to optimize sequential chunk expansion, thereby preserving the decoder’s autoregressive property...","2509.01092v2.pdf, Page: 3",28.097609758377075,2025-11-30 14:27:32,,
15.0,Hybrid RAG,True,C,C,"In this task, we freeze the decoder model and only train the encoder and projection layer. The main objectives are to align the encoder and projection layer so that: 1) encoder can compress k tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information. The reconstruction task was specifically chosen to encourage the model to rely on context memory rather than its parametric memory during training.",filename: 2509.01092v2.pdf page 3,23.33713984489441,2025-11-30 14:27:32,,
16.0,Hybrid RAG,True,C,C,"To continuously adjust task difficulty, we vary the data mixture over time, starting with examples dominated by easier tasks (e.g., single chunk embedding) and gradually shifting towards those dominated by more difficult tasks (i.e.,L chunk embeddings).",filename: 2509.01092v2.pdf page 3,18.430927753448486,2025-11-30 14:27:32,,
17.0,Hybrid RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,17.493717670440674,2025-11-30 14:27:32,,
18.0,Hybrid RAG,True,C,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization. Additionally, CEPE does not utilize token compression, resulting in similar or even increased decoding latency.","2509.01092v2.pdf, Page: 9",15.270968675613403,2025-11-30 14:27:32,,
19.0,Hybrid RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG achieves a 1.93% average gain over 16 RAG tasks.","2509.01092v2.pdf, Page: 7",15.257620096206665,2025-11-30 14:27:32,,
20.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",17.979698181152344,2025-11-30 14:27:32,,
21.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",14.373101234436035,2025-11-30 14:27:32,,
22.0,Hybrid RAG,True,C,C,"For longer context length, acceleration reaches up tok2× for both metrics.","2509.01092v2.pdf, Page: 2",15.208395004272461,2025-11-30 14:27:32,,
23.0,Hybrid RAG,True,C,C,"REFRAG supports compression of token chunks at arbitrary positions (see figure 1) while preserving the autoregressive nature of the decoder, thereby supporting multi-turn and agentic applications. This “compress anywhere” capability is further enhanced by a lightweight reinforcement learning (RL) policy that selectively determines when full chunk token input is necessary and when low-cost, approximate chunk embeddings suffice .",filename: 2509.01092v2.pdf page 1,14.140073537826538,2025-11-30 14:27:32,,
24.0,Hybrid RAG,True,C,C,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the decoder’s token space, allowing the decoder to interpret and accurately reconstruct the original information.","2509.01092v2.pdf, Page: 3",24.630186796188354,2025-11-30 14:27:32,,
25.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","2509.01092v2.pdf, Page: 3",12.184788227081299,2025-11-30 14:27:32,,
26.0,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE...",filename: 2509.01092v2.pdf page 2,14.815276384353638,2025-11-30 14:27:32,,
27.0,Hybrid RAG,True,C,C,"At equal latency (8 passages forREFRAGvs. 1 for LLaMA),REFRAGattains a1.22%
average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,13.77335238456726,2025-11-30 14:27:32,,
28.0,Hybrid RAG,True,C,C,"For reference, LLaMA256 uses only the last 256 tokens, matching the number of chunk embeddings in REFRAG8 (s/k = 256), yet REFRAG8 consistently surpasses LLaMA256, demonstrating the effectiveness of compressed chunk embeddings.",filename: 2509.01092v2.pdf page 4,12.51370644569397,2025-11-30 14:27:32,,
29.0,Hybrid RAG,True,C,C,"In contrast, a compression rate of64appears to be overly aggressive, resulting in diminished performance. These findings suggest a practical limit to the compression rate beyond which the model’s capability is significantly reduced.","2509.01092v2.pdf, Page: 6",16.118730068206787,2025-11-30 14:27:32,,
30.0,Hybrid RAG,True,C,C,"The original Llama-2-7B supports only a 4k context window, whereas our approach enables extrapolation via chunk embeddings, extending context and supporting broader applications.","2509.01092v2.pdf, Page: 4",11.004004716873169,2025-11-30 14:27:32,,
31.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.","2509.01092v2.pdf, Page: 1",10.851292610168457,2025-11-30 14:27:32,,
32.0,Hybrid RAG,True,C,C,"Table 6 shows that with short context length s we are able to achievek× acceleration in TTFT and up tok× acceleration in throughput. With longer context lengths, we are able to achieve up tok2× acceleration in both TTFT and throughput.",filename: 2509.01092v2.pdf page 15,13.476497888565063,2025-11-30 14:27:32,,
33.0,Hybrid RAG,True,C,C,"Empirically, as shown in figure 2, with a context length of16384(mid-to-long context),REFRAG with k = 16achieves16 .53× TTFT acceleration with cache and8.59× without cache1, both surpassing CEPE (2.01× and1 .04×, respectively), while achieving 9.3% performance (measured by log-perplexity) compared to CEPE (table 1).","2509.01092v2.pdf, Page: 2",14.40287160873413,2025-11-30 14:27:32,,
34.0,Hybrid RAG,True,C,C,"This task encourages the model to leverage contextual information for next-paragraph prediction, thereby equipping it for downstream applications.",filename: 2509.01092v2.pdf page 3,39.948413372039795,2025-11-30 14:27:32,,
35.0,Hybrid RAG,True,C,C,"For the reconstruction task, training begins with reconstructing a single chunk: the encoder receives one chunk embeddingc1 for x1:k and and the decoder reconstructs thek tokens using the projected chunk embeddingecnk 1 . Subsequently, the model reconstructs x1:2k frome cnk 1 , ecnk 2 , and so forth. To continuously adjust task difficulty, we vary the data mixture over",filename: 2509.01092v2.pdf page 3,18.956955671310425,2025-11-30 14:27:32,,
36.0,Hybrid RAG,True,C,C,"A RL policy, guided by next-paragraph prediction perplexity as a negative reward, determines which chunks to retain in their original form.","Source Reference: 2509.01092v2.pdf, Page: 3",15.908974409103394,2025-11-30 14:27:32,,
37.0,Hybrid RAG,True,C,C,"With a compression rate of 16, we achieve a 9.3% average log-perplexity improvement over CEPE across four datasets2.","2509.01092v2.pdf, Page: 4",10.215905666351318,2025-11-30 14:27:32,,
38.0,Hybrid RAG,False,ERROR,C,No quote provided,None,53.527527809143066,2025-11-30 14:27:32,,
39.0,Hybrid RAG,True,C,C,We follow the work of Lin et al. (2024) to use Wikipedia dumps and CommonCrawl dumps to create a retrieval corpus with 400 million passages. Each passage contains less than 200 words. We use the DRAGON+ model Lin et al. (2023) as our retriever...,filename: 2509.01092v2.pdf page 7,13.947234392166138,2025-11-30 14:27:32,,
40.0,Hybrid RAG,True,C,C,"At equal latency (8 passages for REFRAG vs. 1 for LLaMA), REFRAG attains a 1.22% average improvement across 16 RAG tasks.",filename: 2509.01092v2.pdf page 7,12.39293098449707,2025-11-30 14:27:32,,
41.0,Hybrid RAG,True,C,C,"Instead of using tokens from retrieved passages as input,REFRAGleverages pre-computed, compressed chunk embeddings as approximate representations, feeding these embeddings directly into the decoder.",filename: 2509.01092v2.pdf page 1,12.5457444190979,2025-11-30 14:27:32,,
42.0,Hybrid RAG,True,B,B,"The main objectives are to align the encoder and projection layer so that: 1) encoder can compressk tokens with minimal information loss, and 2) projection layer can effectively map the encoder’s chunk embeddings into the",filename: 2509.01092v2.pdf page 3,13.975437641143799,2025-11-30 14:27:32,,
43.0,Hybrid RAG,False,A,C,we achieve a compression fraction of1−p by randomly selectingT′ := pL chunks to remain,filename: 2509.01092v2.pdf page 15,27.018200397491455,2025-11-30 14:27:32,,
44.0,Hybrid RAG,False,B,C,"However, CEPE is limited to prefix context applications, as it disrupts the causal structure of the context, making it unsuitable for tasks such as multi-turn RAG or summarization.","2509.01092v2.pdf, Page: 9",23.977450132369995,2025-11-30 14:27:32,,
45.0,Hybrid RAG,True,C,C,Table 13 demonstrates thatREFRAG16 with RL-based selective compression consistently outperformsREFRAG8 across different datasets and context lengths.,filename: 2509.01092v2.pdf page 5,14.657744407653809,2025-11-30 14:27:32,,
46.0,Hybrid RAG,True,C,C,"These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks.",filename: 2509.01092v2.pdf page 0,14.677012920379639,2025-11-30 14:27:32,,
47.0,Hybrid RAG,True,B,B,"Moreover, the time-to-first-token (TTFT) latency increases quadratically, while the time-to-iterative-token (TTIT) latency grows linearly with prompt length (Liu et al., 2025).",filename: 2509.01092v2.pdf page 0,12.947132110595703,2025-11-30 14:27:32,,
48.0,Hybrid RAG,True,B,B,"The whole training is split into 9 stages. In each stage, we have a combination of different data (e.g., 1X8 means reconstructing 8 tokens, 2X8 means reconstructing 16 tokens).","2509.01092v2.pdf, Page: 19",15.16574764251709,2025-11-30 14:27:32,,
49.0,Hybrid RAG,True,C,C,"under the same number of retrieved passages, we are able to match the performance of LLaMA in the strong retriever setting and even outperform LLaMA under the weak retriever setting. This is because our model enables larger context and hence enables extract more useful information when the retrieved passages are less relevant.",filename: 2509.01092v2.pdf page 7,15.974571466445923,2025-11-30 14:27:32,,
50.0,Hybrid RAG,True,C,C,"It reduces attention computation complexity, which now scales quadratically with the number of chunks rather than the number of tokens in the context.",filename: 2509.01092v2.pdf page 1,11.67382526397705,2025-11-30 14:27:32,,
